{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import packages here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import data here\n",
    "# data are tsv files, so slight manipulation to read\n",
    "'''\n",
    "train = pd.read_csv('/home/bsong/Python_Stuff/Data/Kaggle_Mercari/train.tsv',delimiter='\\t')\n",
    "test = pd.read_csv('/home/bsong/Python_Stuff/Data/Kaggle_Mercari/test.tsv',delimiter='\\t') # no response variable\n",
    "'''\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_raw = pd.read_csv('/home/bsong/Python_Stuff/Data/Kaggle_Mercari/train.tsv',delimiter= '\\t')\n",
    "#train_raw = train_raw.iloc[0:10000,] # just a bit\n",
    "# standardize price here because may as well\n",
    "normalized_price = np.log1p(train_raw['price'].values)\n",
    "mean_price_norm = np.mean(normalized_price)\n",
    "std_price_norm = np.std(normalized_price) \n",
    "train_raw['price'] = (normalized_price - mean_price_norm)/std_price_norm \n",
    "\n",
    "end_time = time.time()\n",
    "print('import data took ' + str(end_time - start_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train.drop(['train_id','price'],axis=1)\n",
    "y = train['price']\n",
    "\n",
    "#X_train,X_val,y_train,y_val = train_test_split(X,y,test_size = 0.3) # use this in separate ipynb for modeling\n",
    "\n",
    "# convert to tensorflow \n",
    "#train_tf = tf.estimator.inputs.pandas_input_fn(x = pd.DataFrame(train_X), y = pd.Series(train_y), shuffle = True)\n",
    "#test_tf = tf.estimator.inputs.pandas_input_fn(x = pd.DataFrame(test), shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(list(set(X['brand_name'])))) # 4810 unique brand names (and a lot of NaN)\n",
    "print(X['brand_name'].isnull().sum()) # 632682 NaN out of 1.4mil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# look at price\n",
    "\n",
    "print('mean: ' + str(np.mean(y)))\n",
    "print('standard deviation: ' + str(np.std(y)))\n",
    "plt.hist(y, bins = 300) # skewed right histogram\n",
    "plt.axis([0, 200,0 ,550000])\n",
    "plt.show()\n",
    "\n",
    "log_y = np.log(y.values + 0.000000001) # prevent infinity\n",
    "mean_log_y = np.mean(log_y)\n",
    "std_log_y = np.std(log_y)\n",
    "standardized_y = (log_y - mean_log_y) / std_log_y\n",
    "\n",
    "print(standardized_y)\n",
    "plt.hist(standardized_y,bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(log_y)\n",
    "print(min(log_y))\n",
    "print(max(log_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# investigate the words in train\n",
    "# block ends with appending words into a list\n",
    "\n",
    "pattern = r'''(?x)          # set flag to allow verbose regexps\n",
    "        (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "      | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "      | \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "    '''\n",
    "tokenizer = RegexpTokenizer(pattern)\n",
    "prod_names = train['name']\n",
    "list_of_words = []\n",
    "counter = 0\n",
    "\n",
    "for names in prod_names: # reads one name at a time\n",
    "    tokenize_names = tokenizer.tokenize(names.lower()) # tokenize each name after making all lowercase\n",
    "    for i in tokenize_names:\n",
    "        if (len(i) > 2 ): #ignore words of length 2 or less\n",
    "            list_of_words.append(i) # append all words to one list\n",
    "    counter += 1\n",
    "    if counter % 200000 == 0:\n",
    "        perc = round(counter/len(prod_names)*100)\n",
    "        print((str(perc) + '%  complete'))\n",
    "print('100% complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# investigate word count list\n",
    "#list_of_words_no_stopwords = list(filter(lambda x: x not in stopwords.words('english'),list_of_words)) # takes too long\n",
    "top_words = Counter(list_of_words).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize the top words \n",
    "x_ = []\n",
    "y_ = []\n",
    "for i in top_words:\n",
    "    x_.append(i[0]) # names of word counts as list\n",
    "    y_.append(i[1]) # count of word counts as list (ordered)\n",
    "    \n",
    "plt.plot(range(0,200),y_[:200]) # maybe just make 200 flags for now\n",
    "plt.show()\n",
    "\n",
    "#using top 200 words as flags for potential features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# copy train just in case\n",
    "train_copy = train.copy()\n",
    "train_nrows = train_copy.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add flags as features to train set\n",
    "top200_words = x_[:200]\n",
    "counter = 0\n",
    "\n",
    "for topword in top200_words:\n",
    "    new_col_vals = [] # new column that will be flags for each topword\n",
    "    for rows in prod_names:\n",
    "        if topword in rows.split(): # append 1 if topword is in product name\n",
    "            new_col_vals.append(1)\n",
    "        else: \n",
    "            new_col_vals.append(0)\n",
    "    train_copy[topword] = pd.Series(new_col_vals) #train_copy should now have 200 more columns after this\n",
    "    counter += 1\n",
    "    if counter % 5 == 0:\n",
    "        perc = counter*.5\n",
    "        print((str(perc) + '%  complete'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_copy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Looking into 'category_name'\n",
    "\n",
    "cat_names = train['category_name']\n",
    "spaced_cat = []\n",
    "all_cat_words = []\n",
    "counter = 0\n",
    "\n",
    "for categories in cat_names:\n",
    "    split_cat = str(categories).split('/')\n",
    "    spaced_cat.append(str(categories).replace('/', ' '))\n",
    "    indiv_list_of_cat = [x.strip() for x in split_cat]\n",
    "    all_cat_words.extend(indiv_list_of_cat)\n",
    "    counter += 1\n",
    "    if counter % 200000 == 0:\n",
    "        perc = round(counter*100/len(cat_names))\n",
    "        print((str(perc) + '%  complete'))\n",
    "print('100% complete')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_categories = Counter(all_cat_words).most_common()\n",
    "names_common_cat = []\n",
    "count_common_cat = []\n",
    "\n",
    "for i in count_categories:\n",
    "    names_common_cat.append(i[0]) # names of word counts as list\n",
    "    count_common_cat.append(i[1]) # count of word counts as list (ordered)\n",
    "\n",
    "plt.plot(range(0,200),count_common_cat[:200]) # maybe use 25 top categories (or even 5)\n",
    "plt.show()\n",
    "\n",
    "print(count_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# include the category flags here (or maybe separate the files here)\n",
    "\n",
    "top25_categories = names_common_cat[:25]\n",
    "counter = 0\n",
    "\n",
    "for topcat in top25_categories:\n",
    "    new_col_vals = [] # new column that will be flags for each topword\n",
    "    for rows in cat_names:\n",
    "        if topcat in str(rows).split(): # append 1 if topword is in product name\n",
    "            new_col_vals.append(1)\n",
    "        else: \n",
    "            new_col_vals.append(0)\n",
    "    train_copy[topcat] = pd.Series(new_col_vals) #train_copy should now have 200 more columns after this\n",
    "    counter += 1\n",
    "    if counter % 3 == 0:\n",
    "        perc = counter/25*100\n",
    "        print((str(perc) + '%  complete'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bag of words on category_name\n",
    "\n",
    "array_categories = np.array(spaced_cat)\n",
    "count = CountVectorizer()\n",
    "bag = count.fit_transform(array_categories)\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf.fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test some tfidf prediction here\n",
    "X = train.drop(['train_id','price'],axis=1)\n",
    "y = train['price']\n",
    "X_train,X_val,y_train,y_val = train_test_split(X,y,train_size = 0.5) # use this in separate ipynb for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Looking into 'category_name'\n",
    "\n",
    "cat_names = X_train['category_name']\n",
    "spaced_cat = []\n",
    "counter = 0\n",
    "\n",
    "for categories in cat_names:\n",
    "    spaced_cat.append(str(categories).replace('/', ' '))\n",
    "    counter += 1\n",
    "    if counter % 200000 == 0:\n",
    "        perc = round(counter*100/len(cat_names))\n",
    "        print((str(perc) + '%  complete'))\n",
    "print('100% complete')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Looking into 'category_name'\n",
    "\n",
    "cat_names_val = X_val['category_name']\n",
    "spaced_cat_val = []\n",
    "counter = 0\n",
    "\n",
    "for categories in cat_names_val:\n",
    "    spaced_cat_val.append(str(categories).replace('/', ' '))\n",
    "    counter += 1\n",
    "    if counter % 200000 == 0:\n",
    "        perc = round(counter*100/len(cat_names))\n",
    "        print((str(perc) + '%  complete'))\n",
    "print('100% complete')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False,preprocessor=None)\n",
    "array_categories = np.array(spaced_cat)\n",
    "bag = tfidf.fit_transform(array_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "linreg.fit(bag,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bag_val = tfidf.transform(np.array(spaced_cat_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_price = linreg.predict(bag_val)\n",
    "predicted_price.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linreg.score(bag_val,y_val)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
