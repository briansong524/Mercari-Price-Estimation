{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Made price into bins\n",
    "'''\n",
    "## import packages\n",
    "import time\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import random\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import nltk\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## import data \n",
    "start_time = time.time()\n",
    "\n",
    "train_raw = pd.read_csv('/home/bsong/Python_Stuff/Data/Kaggle_Mercari/train.tsv',delimiter= '\\t')\n",
    "test_raw = pd.read_csv('/home/bsong/Python_Stuff/Data/Kaggle_Mercari/test.tsv', delimiter='\\t')\n",
    "#train_raw = train_raw.iloc[0:10000,] # just a bit\n",
    "# standardize price here because may as well\n",
    "#normalized_price = np.log1p(train_raw['price'].values)\n",
    "#mean_price_norm = np.mean(normalized_price)\n",
    "#std_price_norm = np.std(normalized_price) \n",
    "#train_raw['price'] = (normalized_price - mean_price_norm)/std_price_norm \n",
    "\n",
    "end_time = time.time()\n",
    "print('import data took ' + str(end_time - start_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## bins\n",
    "\n",
    "sort_price = sorted(train_raw.price.values)\n",
    "perc1 = np.percentile(sort_price,25)\n",
    "perc2 = np.percentile(sort_price,80)\n",
    "perc3 = np.percentile(sort_price,90)\n",
    "\n",
    "total_bins = 4\n",
    "inds1 = train_raw.train_id[train_raw.price <= perc1].values\n",
    "inds2 = train_raw.train_id[(train_raw.price > perc1) & (train_raw.price <= perc2)].values\n",
    "inds3 = train_raw.train_id[(train_raw.price > perc2) & (train_raw.price <= perc3)].values\n",
    "inds4 = train_raw.train_id[train_raw.price > perc3].values\n",
    "\n",
    "price_1 = train_raw.price[train_raw.price <= perc1].values\n",
    "price_2 = train_raw.price[(train_raw.price > perc1) & (train_raw.price <= perc2)].values\n",
    "price_3 = train_raw.price[(train_raw.price > perc2) & (train_raw.price <= perc3)].values\n",
    "price_4 = train_raw.price[train_raw.price > perc3].values\n",
    "\n",
    "train_raw['price_bins'] = [0]*train_raw.shape[0]\n",
    "train_raw['price_bins_mean'] = [0]*train_raw.shape[0]\n",
    "\n",
    "train_raw.price_bins.iloc[inds1] = 1\n",
    "train_raw.price_bins.iloc[inds2] = 2\n",
    "train_raw.price_bins.iloc[inds3] = 3\n",
    "train_raw.price_bins.iloc[inds4] = 4\n",
    "\n",
    "train_raw.price_bins_mean.iloc[inds1] = price_1.mean()\n",
    "train_raw.price_bins_mean.iloc[inds2] = price_2.mean()\n",
    "train_raw.price_bins_mean.iloc[inds3] = price_3.mean()\n",
    "train_raw.price_bins_mean.iloc[inds4] = price_4.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_BRANDS = 4004\n",
    "NUM_CATEGORIES = 1001\n",
    "NAME_MIN_DF = 10\n",
    "MAX_FEATURES_ITEM_DESCRIPTION = 39000\n",
    "\n",
    "def handle_missing_inplace(dataset):\n",
    "    dataset['category_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['brand_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['item_description'].fillna(value='missing', inplace=True)\n",
    "\n",
    "\n",
    "def cutting(dataset):\n",
    "    pop_brand = dataset['brand_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n",
    "    dataset.loc[~dataset['brand_name'].isin(pop_brand), 'brand_name'] = 'missing'\n",
    "    pop_category = dataset['category_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n",
    "    dataset.loc[~dataset['category_name'].isin(pop_category), 'category_name'] = 'missing'\n",
    "\n",
    "\n",
    "def to_categorical(dataset):\n",
    "    dataset['category_name'] = dataset['category_name'].astype('category')\n",
    "    dataset['brand_name'] = dataset['brand_name'].astype('category')\n",
    "    dataset['item_condition_id'] = dataset['item_condition_id'].astype('category')\n",
    "\n",
    "## post-prediction functions\n",
    "\n",
    "def rmsle(h, y): \n",
    "    log_h = np.log(h+1) # the +1 is to prevent 0 \n",
    "    log_y = np.log(y+1) # writing these to prevent memoryerror\n",
    "    sq_logs = np.square(log_h - log_y)\n",
    "    score_ = np.sqrt(np.mean(sq_logs))\n",
    "    return score_\n",
    "\n",
    "def rmse(h,y):\n",
    "    sq_logs = np.square(h-y)\n",
    "    score_ = np.sqrt(np.mean(sq_logs))\n",
    "    return score_\n",
    "\n",
    "def unwind(preds, mean_,std_, norm_ = True):\n",
    "    unstandardized = preds*std_ + mean_\n",
    "    if norm_ == True: # norm_ is if the original value (like the label) was normalized with np.logm1\n",
    "        unstandardized = np.expm1(unstandardized)\n",
    "    return unstandardized\n",
    "\n",
    "def optimal_weights_ensemble(Xs, y):\n",
    "    # make sure Xs is all predictions where each column is predictions by each model\n",
    "    # y just has to be a vector of true values\n",
    "    # note: the values of Xs need to be scaled to the values that would go into RMSE\n",
    "    \n",
    "    y = np.reshape(y,(-1,1)) # to make it shape [n,1]\n",
    "    \n",
    "    first = np.matmul(np.transpose(y),Xs)\n",
    "    second = np.matmul(np.transpose(Xs), Xs)\n",
    "    w_ = np.matmul(first, np.linalg.inv(second)) # this should be of size [1,n_weights]\n",
    "    return np.transpose(w_) #returns [n_weight,1] for easier matrix multiplication "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "handle_missing_inplace(train_raw)\n",
    "print('[{}] Finished to handle missing'.format(time.time() - start_time))\n",
    "\n",
    "cutting(train_raw)\n",
    "print('[{}] Finished to cut'.format(time.time() - start_time))\n",
    "\n",
    "to_categorical(train_raw)\n",
    "print('[{}] Finished to convert categorical'.format(time.time() - start_time))\n",
    "\n",
    "cv = CountVectorizer(min_df=NAME_MIN_DF)\n",
    "X_name = cv.fit_transform(train_raw['name'])\n",
    "print('[{}] Finished count vectorize `name`'.format(time.time() - start_time))\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X_category = cv.fit_transform(train_raw['category_name'])\n",
    "print('[{}] Finished count vectorize `category_name`'.format(time.time() - start_time))\n",
    "\n",
    "tv = TfidfVectorizer(max_features=MAX_FEATURES_ITEM_DESCRIPTION,\n",
    "                     ngram_range=(1, 3),\n",
    "                     stop_words='english')\n",
    "X_description = tv.fit_transform(train_raw['item_description'])\n",
    "print('[{}] Finished TFIDF vectorize `item_description`'.format(time.time() - start_time))\n",
    "\n",
    "lb = LabelBinarizer(sparse_output=True)\n",
    "X_brand = lb.fit_transform(train_raw['brand_name'])\n",
    "print('[{}] Finished label binarize `brand_name`'.format(time.time() - start_time))\n",
    "\n",
    "X_dummies = csr_matrix(pd.get_dummies(train_raw[['item_condition_id', 'shipping']],\n",
    "                                      sparse=True).values)\n",
    "print('[{}] Finished to get dummies on `item_condition_id` and `shipping`'.format(time.time() - start_time))\n",
    "\n",
    "sparse_train_raw = hstack((X_dummies, X_description, X_brand, X_category, X_name)).tocsr()\n",
    "print('[{}] Finished to create sparse train_raw'.format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ind = round(0.95*sparse_train_raw.shape[0]) # save 5% as a test set \n",
    "X_not_test = sparse_train_raw[:test_ind,] # these will be split\n",
    "Y_not_test = train_raw['price_bins'][:test_ind,]          # into train and validation set\n",
    "\n",
    "X_test = sparse_train_raw[test_ind:,] # these are the \n",
    "Y_test = train_raw['price_bins'][test_ind:,]          # test sets\n",
    "\n",
    "split_ratio = 0.9\n",
    "ind_split = round(split_ratio*X_not_test.shape[0])\n",
    "X_train = X_not_test[:ind_split,]\n",
    "X_val = X_not_test[ind_split:,]\n",
    "\n",
    "Y_train = Y_not_test[:ind_split,]\n",
    "Y_val = Y_not_test[ind_split:,]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
