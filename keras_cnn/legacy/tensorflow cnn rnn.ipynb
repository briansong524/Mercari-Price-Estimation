{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## import packages\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import random\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## import data \n",
    "train_raw = pd.read_csv('/home/bsong/Python_Stuff/Data/Kaggle_Mercari/train.tsv',delimiter= '\\t')\n",
    "#train_raw = train_raw.iloc[0:10000,] # just a bit\n",
    "# standardize price here because may as well\n",
    "normalized_price = np.log1p(train_raw['price'].values)\n",
    "train_raw['price'] = (normalized_price - np.mean(normalized_price))/ np.std(normalized_price) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## define functions to use\n",
    "\n",
    "######## General functions\n",
    "\n",
    "def rmsle(h, y): \n",
    "    log_h = np.log(h+1) # the +1 is to prevent 0 \n",
    "    log_y = np.log(y+1) # writing these to prevent memoryerror\n",
    "    sq_logs = np.square(log_h - log_y)\n",
    "    score_ = np.sqrt(np.mean(sq_logs))\n",
    "    return score_\n",
    "\n",
    "######## Basic text manipulation functions (some specific to Mercari Kaggle Competition) \n",
    "\n",
    "def split_cat(text): # this one is to reduce the categoriy_name into three subcategories\n",
    "    try: return text.split(\"/\")\n",
    "    except: return (\"No Label\", \"No Label\", \"No Label\")\n",
    "\n",
    "def handle_missing_inplace(dataset):  # this one is to put placeholders in place of missing values (NaN)\n",
    "    dataset['cat1'].fillna(value='No Label', inplace=True)\n",
    "    dataset['cat2'].fillna(value='No Label', inplace=True)\n",
    "    dataset['cat3'].fillna(value='No Label', inplace=True)\n",
    "    dataset['brand_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['item_description'].fillna(value='No description yet', inplace=True)\n",
    "     \n",
    "def build_dictionary(words, n_words): # dictionary that maps words to indices. this function should be modular.\n",
    "    #input is [['a','b','c'],['a','b','c']]\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]] # word indexed as \"unknown\" if not one of the top #n_words (popular/common) words\n",
    "    count.extend(Counter(words).most_common(n_words - 1)) # most_common returns the top (n_words-1) ['word',count]\n",
    "    dictionary = dict()\n",
    "    for word, _ in count: # the 'word, _' is writted because count is a list of list(2), so defining 'word' as the first term per\n",
    "        dictionary[word] = len(dictionary) # {'word': some number incrementing by one. fyi, no repeats because from most_common)}\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys())) # {ind. : 'word'} I guess for looking up if needed?\n",
    "    return dictionary, reversed_dictionary\n",
    "\n",
    "def clean_and_tokenize(dataset_col): # input is a column of strings\n",
    "    pattern = '\\w+' # does this only keep words\n",
    "    list_of_lists = list()\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    for word in dataset_col:\n",
    "        list_of_words = list()\n",
    "        tokenized = tokenizer.tokenize(word)\n",
    "        for i in tokenized:\n",
    "            if (len(i) > 2 ): #ignore words of length 2 or less\n",
    "                list_of_words.append(i.lower()) # append all words to one list\n",
    "        list_of_lists.append(list_of_words)\n",
    "    list_as_series = pd.Series(list_of_lists)\n",
    "    return list_as_series\n",
    "\n",
    "def convert_word_to_ind(dataset_col,dictionary): # input the pandas column of texts and dictionary. This should be modular\n",
    "    # each input should be a string of cleaned words tokenized into a list (ex. ['this', 'is', 'an', 'item'])\n",
    "    # dictionary should be the dictionary obtained from build_dictionary\n",
    "    list_of_lists = []\n",
    "    unk_count = 0 # total 'unknown' words counted\n",
    "    for word_or_words in dataset_col: # words is the list of all words\n",
    "        list_of_inds = []\n",
    "        for word in word_or_words:\n",
    "            if word in dictionary:\n",
    "                index = np.int(dictionary[word]) # dictionary contains top words, so if in, it gets an index\n",
    "            else:\n",
    "                index = 0  #  or dictionary['UNK']? can figure out later\n",
    "                unk_count += 1\n",
    "            list_of_inds.append(index)\n",
    "        list_of_lists.append(list_of_inds)\n",
    "\n",
    "    # make list_of_lists into something that can be put into pd.DataFrame\n",
    "    #list_as_series = pd.Series(list_of_lists)\n",
    "    list_as_series = np.array(list_of_lists)\n",
    "    return list_as_series, unk_count\n",
    "\n",
    "def pad_word_indices(col_of_indices, pad_length): # col_of_indices can be a pd series. \n",
    "    temp_series = [] # append vectors into here\n",
    "    for list_inds in col_of_indices:\n",
    "        len_list = len(list_inds)\n",
    "        if len_list >= pad_length:\n",
    "            temp_series.append(np.array(list_inds[(len_list-pad_length):]))\n",
    "        else:\n",
    "            padded_vec = [0]*(pad_length-len_list)\n",
    "            padded_vec.extend(list_inds)\n",
    "            temp_series.append(np.array(padded_vec))\n",
    "    return temp_series\n",
    "\n",
    "def convert_word_to_padded(dataset_col,dictionary,pad_length): # input the pandas column of texts and dictionary. This should be modular\n",
    "    # each input should be a string of cleaned words tokenized into a list (ex. ['this', 'is', 'an', 'item'])\n",
    "    # dictionary should be the dictionary obtained from build_dictionary\n",
    "    # use this function when you know how long you want your pad_length\n",
    "    #   - otherwise, use convert_word_to_ind, and pad_word_indices\n",
    "    #   - eventually, will look into cleaning these three functions up.\n",
    "    list_of_lists = []\n",
    "    unk_count = 0 # total 'unknown' words counted\n",
    "    for word_or_words in dataset_col: # words is the list of all words\n",
    "        list_of_inds = []\n",
    "        count_inds = 0\n",
    "        for word in word_or_words:\n",
    "            if word in dictionary:\n",
    "                index = np.int(dictionary[word]) # dictionary contains top words, so if in, it gets an index\n",
    "            else:\n",
    "                index = 0  #  or dictionary['UNK']? can figure out later\n",
    "                unk_count += 1\n",
    "            count_inds +=1\n",
    "            list_of_inds.append(index) \n",
    "        if count_inds >= pad_length:\n",
    "            asdf = list_of_inds[(count_inds-pad_length):]\n",
    "        else: \n",
    "            asdf = [0]*(pad_length-count_inds)\n",
    "            asdf.extend(list_of_inds)\n",
    "        temp = np.array(asdf)\n",
    "        list_of_lists.append(temp)\n",
    "    list_as_series = np.array(list_of_lists)\n",
    "    return list_as_series, unk_count\n",
    "\n",
    "######## Word Embedding (this is after strings are transformed into vectors of indices)\n",
    "\n",
    "# generate batch data (for feeding into word embedding)\n",
    "# used http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/ for reference\n",
    "def generate_batch(data, batch_size, num_skips): \n",
    "    # data should be [[3,7,9],[7,4,5,9],...] kinda format\n",
    "    # num_skips configures number of context words to draw. skip_window defines size of window to draw context words from\n",
    "    assert batch_size % num_skips == 0 # if batch_size was 10, and num_skips was 3, then [cat,cat,cat,sat,sat,sat,...] wont equal\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32) # initialize batch variable (input word go in here)\n",
    "    context = np.ndarray(shape=(batch_size, 1), dtype=np.int32) # initialize context variable\n",
    "    counter = 0\n",
    "    rand_dat_ind = random.sample(range(0,len(data)-1),int(batch_size/num_skips))\n",
    "    for i in data[rand_dat_ind]:\n",
    "        while len(i) <= num_skips:\n",
    "            rnd_again = random.randint(0,len(data)-1)\n",
    "            i = data[rnd_again]\n",
    "        target = random.randint(0,len(i)-1) \n",
    "        targets_to_avoid = [target] # avoid this index when selecting rando words\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid: # this is to choose an index that isnt the index of the batch word\n",
    "                target = random.randint(0, len(i)-1) # target is a context word\n",
    "            targets_to_avoid.append(target) # so next time, this loop won't select this context word again \n",
    "            batch[counter] = i[targets_to_avoid[0]]  # this is the input word (same word repeated i*num_skips+j times)\n",
    "            context[counter, 0] = i[targets_to_avoid[j+1]]  # these are the context words to the batch word\n",
    "            counter += 1\n",
    "    return batch, context # batch is input, context is target variable(s)\n",
    "\n",
    "def generate_batch_general(x, y, batch_size):\n",
    "    # this is to generate batches for word2vec comparing against numeric values \n",
    "    # in this case, 'brand_name' and cat1/2/3 are compared against 'price'\n",
    "    rand_dat_ind = random.sample(range(0,len(data)-1),int(batch_size))\n",
    "    return x[rand_dat_ind], y[rand_dat_ind]\n",
    "    \n",
    "    \n",
    "######## word vector RNN # i dont think im using these yet lol 1/5/2018\n",
    "# used http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/ for reference\n",
    "def batch_producer(raw_data, batch_size, num_steps): # produces input/target batches for word vector rnn\n",
    "    raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32) # make tensor out of csv\n",
    "\n",
    "    data_len = tf.size(raw_data)\n",
    "    batch_len = data_len // batch_size # determines number of equally sized batches available from input data\n",
    "    data = tf.reshape(raw_data[0: batch_size * batch_len], # reshape to contain all batches (and exclude the remaining data)  \n",
    "                      [batch_size, batch_len])\n",
    "\n",
    "    epoch_size = (batch_len - 1) // num_steps # number of iterations in each epoch\n",
    "\n",
    "    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue() # extract asynchronous batches of data \n",
    "    x = data[:, i * num_steps:(i + 1) * num_steps] # x may be the input variaables\n",
    "    x.set_shape([batch_size, num_steps]) # shape for feeding\n",
    "    y = data[:, i * num_steps + 1: (i + 1) * num_steps + 1] # y would then be the target variable based on x (in descriptions)\n",
    "    y.set_shape([batch_size, num_steps])\n",
    "    return x, y\n",
    "\n",
    "class Input(object): # class for utilizing batch_producer (neater with class)\n",
    "    def __init__(self, batch_size, num_steps, data):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n",
    "        self.input_data, self.targets = batch_producer(data, batch_size, num_steps)\n",
    "        \n",
    "class Model(object): # rnn + LSTM model \n",
    "    def __init__(self, input, is_training, hidden_size, vocab_size, num_layers,\n",
    "                 dropout=0.5, init_scale=0.05):\n",
    "        self.is_training = is_training\n",
    "        self.input_obj = input\n",
    "        self.batch_size = input.batch_size\n",
    "        self.num_steps = input.num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## clean \"category_name\" and make numeric indicies for one-worded features (brand_name, cat1/2/3)\n",
    "\n",
    "train_raw['cat1'],train_raw['cat2'],train_raw['cat3'] = \\\n",
    "zip(*train_raw['category_name'].apply(lambda x: split_cat(x))) # split the categories into three new columns\n",
    "train_raw.drop('category_name',axis = 1, inplace = True) # remove the column that isn't needed anymore\n",
    "\n",
    "handle_missing_inplace(train_raw) # replaces NaN with a string placeholder\n",
    "\n",
    "#le = LabelEncoder() # use this to change categorical names into index numbers (0 1 2 3 or something)\n",
    "#train_raw.brand_name = le.fit_transform(train_raw.brand_name)\n",
    "#train_raw.cat1 = le.fit_transform(train_raw.cat1)\n",
    "#train_raw.cat2 = le.fit_transform(train_raw.cat2)\n",
    "#train_raw.cat3 = le.fit_transform(train_raw.cat3)\n",
    "\n",
    "# maybe look into exporting this updated train_raw cuz LabelEncoder takes way too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words (with repeats): 36559364\n",
      "total unassigned words in name and item_description: 36739 66485\n"
     ]
    }
   ],
   "source": [
    "## convert name and item_desc to indices, then configure a bit more\n",
    "\n",
    "all_name_desc = np.hstack((train_raw['name'],train_raw['item_description'])) # get all dem words\n",
    "all_name_desc = clean_and_tokenize(all_name_desc)\n",
    "all_name_desc = [item for sublist in all_name_desc for item in sublist]\n",
    "train_raw['name'] = clean_and_tokenize(train_raw['name'])\n",
    "train_raw['item_description'] = clean_and_tokenize(train_raw['item_description'])\n",
    "\n",
    "vocabulary_size = 100000 # keeping 100000 words in the dictionary. can adjust later. will use variable elsewhere\n",
    "word2vec_dict, reverse_dict = build_dictionary(all_name_desc,vocabulary_size) \n",
    "train_raw['name_inds'], count_unk_name = convert_word_to_ind(train_raw['name'],word2vec_dict) \n",
    "train_raw['item_desc_inds'], count_unk_item_desc = convert_word_to_ind(train_raw['item_description'], word2vec_dict)  \n",
    "\n",
    "# delete 'name' and 'item_description' if not needed anymore\n",
    "#train_raw.drop('name',axis = 1, inplace = True) # remove the column that isn't needed anymore\n",
    "#train_raw.drop('item_description',axis = 1, inplace = True) # remove the column that isn't needed anymore\n",
    "\n",
    "print(\"total words (with repeats): \" + str(len(all_name_desc)))\n",
    "print(\"total unassigned words in name and item_description: \"+ str(count_unk_name) +  ' ' + str(count_unk_item_desc))\n",
    "# 100k vocab: 88k \"UNK\" and 161k \"UNK\" in name and item desc, respectively. Out of 36mil tokens. \n",
    "\n",
    "# combine name and item_description for easier modeling\n",
    "# technically not correct because it might potentially match words between name and item_desc. \n",
    "train_raw['name_and_desc'] = train_raw['name_inds'] + train_raw['item_desc_inds']\n",
    "\n",
    "# padding the name and description column to have equally lengthed lists\n",
    "pad_length = 25\n",
    "#train_raw['padded_name_desc'] = pad_word_indices(train_raw['name_and_desc'],pad_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## alternative to above block\n",
    "\n",
    "#name_inds, _ = convert_word_to_padded(train_raw['name'],word2vec_dict,25) \n",
    "#item_desc_inds, _ = convert_word_to_padded(train_raw['item_description'], word2vec_dict,25)  \n",
    "name_and_desc, _  = convert_word_to_padded(list(map(lambda x,y: x+y,train_raw['name'], train_raw['item_description'])),word2vec_dict,pad_length) \n",
    "#name_and_desc = [np.append(name_inds[i],item_desc_inds[i]) for i in range(len(name_inds))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2382 0 1696 615\n"
     ]
    }
   ],
   "source": [
    "dict_brand_len = 3000\n",
    "dict_cat1_len = 12\n",
    "dict_cat2_len= 100\n",
    "dict_cat3_len = 700\n",
    "\n",
    "brand_name_dict, brand_name_dict_rev = build_dictionary(train_raw['brand_name'], dict_brand_len)\n",
    "train_raw['brand_name_inds'], count_unk_brand = convert_word_to_ind(train_raw['brand_name'].values.reshape((-1,1)), brand_name_dict)\n",
    "cat1_dict ,cat1_rev_dict= build_dictionary(train_raw['cat1'],dict_cat1_len)\n",
    "train_raw['cat1_inds'], count_unk_cat1 = convert_word_to_ind(train_raw['cat1'].values.reshape((-1,1)), cat1_dict)\n",
    "cat2_dict ,cat2_rev_dict= build_dictionary(train_raw['cat2'],dict_cat2_len)\n",
    "train_raw['cat2_inds'], count_unk_cat2 = convert_word_to_ind(train_raw['cat2'].values.reshape((-1,1)), cat2_dict)\n",
    "cat3_dict ,cat3_rev_dict= build_dictionary(train_raw['cat3'],dict_cat3_len)\n",
    "train_raw['cat3_inds'], count_unk_cat3 = convert_word_to_ind(train_raw['cat3'].values.reshape((-1,1)), cat3_dict)\n",
    "\n",
    "print(str(count_unk_brand) + ' ' + str(count_unk_cat1) + ' '+ str(count_unk_cat2) + \" \" + str(count_unk_cat3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>name_inds</th>\n",
       "      <th>item_desc_inds</th>\n",
       "      <th>brand_name_inds</th>\n",
       "      <th>cat1_inds</th>\n",
       "      <th>cat2_inds</th>\n",
       "      <th>cat3_inds</th>\n",
       "      <th>name_and_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[mlb, cincinnati, reds, shirt, size]</td>\n",
       "      <td>3</td>\n",
       "      <td>missing</td>\n",
       "      <td>-0.775703</td>\n",
       "      <td>1</td>\n",
       "      <td>[description, yet]</td>\n",
       "      <td>Men</td>\n",
       "      <td>Tops</td>\n",
       "      <td>T-shirts</td>\n",
       "      <td>[3578, 8211, 6439, 61, 5]</td>\n",
       "      <td>[45, 54]</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>24</td>\n",
       "      <td>[3578, 8211, 6439, 61, 5, 45, 54]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[razer, blackwidow, chroma, keyboard]</td>\n",
       "      <td>3</td>\n",
       "      <td>Razer</td>\n",
       "      <td>1.323039</td>\n",
       "      <td>0</td>\n",
       "      <td>[this, keyboard, great, condition, and, works,...</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>Computers &amp; Tablets</td>\n",
       "      <td>Components &amp; Parts</td>\n",
       "      <td>[10018, 22659, 14897, 2441]</td>\n",
       "      <td>[16, 2441, 22, 10, 1, 162, 34, 1005, 42, 2, 37...</td>\n",
       "      <td>586</td>\n",
       "      <td>4</td>\n",
       "      <td>44</td>\n",
       "      <td>201</td>\n",
       "      <td>[10018, 22659, 14897, 2441, 16, 2441, 22, 10, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[ava, viv, blouse]</td>\n",
       "      <td>1</td>\n",
       "      <td>Target</td>\n",
       "      <td>-0.775703</td>\n",
       "      <td>1</td>\n",
       "      <td>[adorable, top, with, hint, lace, and, key, ho...</td>\n",
       "      <td>Women</td>\n",
       "      <td>Tops &amp; Blouses</td>\n",
       "      <td>Blouse</td>\n",
       "      <td>[7073, 9791, 588]</td>\n",
       "      <td>[533, 41, 6, 4993, 150, 1, 791, 1172, 2, 81, 2...</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>[7073, 9791, 588, 533, 41, 6, 4993, 150, 1, 79...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_id                                   name  item_condition_id  \\\n",
       "0         0   [mlb, cincinnati, reds, shirt, size]                  3   \n",
       "1         1  [razer, blackwidow, chroma, keyboard]                  3   \n",
       "2         2                     [ava, viv, blouse]                  1   \n",
       "\n",
       "  brand_name     price  shipping  \\\n",
       "0    missing -0.775703         1   \n",
       "1      Razer  1.323039         0   \n",
       "2     Target -0.775703         1   \n",
       "\n",
       "                                    item_description         cat1  \\\n",
       "0                                 [description, yet]          Men   \n",
       "1  [this, keyboard, great, condition, and, works,...  Electronics   \n",
       "2  [adorable, top, with, hint, lace, and, key, ho...        Women   \n",
       "\n",
       "                  cat2                cat3                    name_inds  \\\n",
       "0                 Tops            T-shirts    [3578, 8211, 6439, 61, 5]   \n",
       "1  Computers & Tablets  Components & Parts  [10018, 22659, 14897, 2441]   \n",
       "2       Tops & Blouses              Blouse            [7073, 9791, 588]   \n",
       "\n",
       "                                      item_desc_inds  brand_name_inds  \\\n",
       "0                                           [45, 54]                1   \n",
       "1  [16, 2441, 22, 10, 1, 162, 34, 1005, 42, 2, 37...              586   \n",
       "2  [533, 41, 6, 4993, 150, 1, 791, 1172, 2, 81, 2...               82   \n",
       "\n",
       "   cat1_inds  cat2_inds  cat3_inds  \\\n",
       "0          5         19         24   \n",
       "1          4         44        201   \n",
       "2          1          3         13   \n",
       "\n",
       "                                       name_and_desc  \n",
       "0                  [3578, 8211, 6439, 61, 5, 45, 54]  \n",
       "1  [10018, 22659, 14897, 2441, 16, 2441, 22, 10, ...  \n",
       "2  [7073, 9791, 588, 533, 41, 6, 4993, 150, 1, 79...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Embed word ind vectors to be used for NN (set up for this dataset) \n",
    "\n",
    "# Initialize batch parameters for generate_batch()\n",
    "\n",
    "embedding_size = 15  # Dimension of the embedding vector.\n",
    "num_skips = 3         # How many times to reuse an input to generate a context.\n",
    "batch_size = 300\n",
    "\n",
    "# Initialize validation parameters #\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 20     # Random set of words to evaluate similarity on.\n",
    "valid_window = 1000  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False) # random word indices to use for validation \n",
    "\n",
    "# Set up tensorflow placeholders for inputs (batch words) and labels (context words)\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size]) # input words\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1]) # context words\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32) # validation words from dictionary\n",
    "\n",
    "# Look up embeddings for inputs.\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)) # each word will have embedding\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs) # api that optimizes looking up hidden layer vector for word indices\n",
    "\n",
    "# Setup the y = xw + b equation for training the embedding vector \n",
    "# Construct the variables for the NCE loss (special method to improve time through reduced searching computation)\n",
    "num_sampled = 1000\n",
    "nce_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / np.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "nce_loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                       biases=nce_biases,\n",
    "                       labels=train_labels,\n",
    "                       inputs=embed,\n",
    "                       num_sampled=num_sampled,\n",
    "                       num_classes=vocabulary_size))\n",
    "optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(nce_loss)\n",
    "\n",
    "# Utilize cosine similarity to measure \"distances\" between various word embedding vectors for the different words\n",
    "# This is to use the validation set and obtain the n most similar words to the validation words\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True)) # obtain A/||A||_2 for all words\n",
    "normalized_embeddings = embeddings / norm                               # this makes it easy to obtain cosine similarity\n",
    "valid_embeddings = tf.nn.embedding_lookup( # look up table \n",
    "      normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(                                                 # this is to obtain all A*B/||A||_2*||B|||_2\n",
    "      valid_embeddings, normalized_embeddings, transpose_b=True)        # between validation and vocab list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  3235.2734375\n",
      "Nearest to rate: ryk, highlighters, atr, marshmallowy, gentally, dignity, mariscal, 12p,\n",
      "Nearest to wii: lbr, hitchens, 1btexturestraightcan, lalaloopsys, leyvinid, galleria, nwtfree, wildkin,\n",
      "Nearest to stone: metric, quadcore, sshops, bohnanza, dashund, meechie, 4cm, koltov,\n",
      "Nearest to photos: 679, shocker, anarkali, fireman, jewell, missle, officiall, entire,\n",
      "Nearest to adidas: f20, society, biopeel, backstrom, 98pcs, magicsuit, stanford, you,\n",
      "Nearest to workout: proform, rlpc, erectile, redistribute, straightend, twinpower917, lrgeans, considered,\n",
      "Nearest to show: gist, islandragz, holden, lga1151, primark, rotate, icure, jhfb,\n",
      "Nearest to pure: 42d, pr2, 176kbps, indomitable, xb2, teaser, chindi, lularoeleggings,\n",
      "Nearest to year: loot, babolat, akihabara, artemisia, twentieth, tees, draggle, firmenich,\n",
      "Nearest to deep: swwet, aerospace, bigosh, samurai, ghoul, mk5165, kwan, megs,\n",
      "Nearest to bras: mechanix, coconverse, keeley, invitation, spandes, coby, ᖇiᑎg, 145cm,\n",
      "Nearest to ultimate: southernmama, moxley, kew, hadleyjhs, komasan, hace, lafemme, fatter,\n",
      "Nearest to shoulder: to48, tao, accuteck, 4rings, inclimate, 993, airpods, balletslip,\n",
      "Nearest to neck: nape, en162, strategies, presents, busterbrown, whitegray, tempting, estate,\n",
      "Nearest to polish: tricolored, gwpl, proactive, 10x50, xgames, combine, howes, 700mb,\n",
      "Nearest to leave: demibra, 492, cheltenham, akigalawood, heavyweight, voluptuous, owlette, lyssa,\n",
      "Nearest to metal: aziz, marketplace, barelyworn, figuring, s10, inn, levismom, times,\n",
      "Nearest to skinny: majora, hispanican, douce, sunglassses, take_____, unexplored, amateur, controled,\n",
      "Nearest to guaranteed: pricess, luanna, beatit, littest, h2ocean, tyranids, hollidays, whitesox,\n",
      "Nearest to controller: cyclists, warmness, waistband, billbart, retailers, vows, poster, prepare,\n",
      "Average loss at step  2000 :  386.108088018\n",
      "Average loss at step  4000 :  52.8024613781\n",
      "Average loss at step  6000 :  35.9037851129\n",
      "Average loss at step  8000 :  28.9790119631\n",
      "Average loss at step  10000 :  25.2095037479\n",
      "Nearest to rate: l320, come, easy, cream, buy, ryk, package, 16fl,\n",
      "Nearest to wii: farm, fair, b31, sweaty, swimsuit, cause, puni, life,\n",
      "Nearest to stone: knot, midi, quadcore, starbucks, chamilia, bohnanza, pottery, handling,\n",
      "Nearest to photos: bumdle, fireman, dolph, nkotb, zonderkidz, cory, jewell, compartment,\n",
      "Nearest to adidas: pristin, stanford, sized, custom, generally, diapers, halo, anthracite,\n",
      "Nearest to workout: 535, considered, castles, g15, diddy, visable, blacker, saucers,\n",
      "Nearest to show: emmit, shot, bypass, green, rotate, bathbomb, twice, email,\n",
      "Nearest to pure: teaser, emblem, poudre, msrp, proglide, suv, moro, 35om,\n",
      "Nearest to year: loot, draggle, activities, powdered, greys, florsheim, lile, chucky,\n",
      "Nearest to deep: importance, lemongrass, ghoul, citizen, primavera, keyhole, intensifier, venus,\n",
      "Nearest to bras: gymboree, tiny, mechanix, intimates, vape, alike, pour, backyardigans,\n",
      "Nearest to ultimate: khalil, seuss, fob, alias, nordic, paddles, rc32, tiki,\n",
      "Nearest to shoulder: lightweight, gucci, airpods, stick, rue21, hand, hills, yoshis,\n",
      "Nearest to neck: lined, pencil, large, usps, defiant, polo, dress, small,\n",
      "Nearest to polish: grabbed, 32inch, deeply, babies, proactive, 64ft, secretions, frays,\n",
      "Nearest to leave: calming, heavyweight, lyssa, equivalence, personnel, owlette, storms, floors,\n",
      "Nearest to metal: eau, hyperdunk, kareena, dahhling, memoir, dean, x16, blowout,\n",
      "Nearest to skinny: metallic, book, hand, inquiry, playstation, 3xl, twist, jars,\n",
      "Nearest to guaranteed: thumbholes, pricess, inclusion, tounge, b02, beatssolo2, cesar, tahereh,\n",
      "Nearest to controller: yogurt, explicit, begginers, ibeats, prepare, jafra, existing, seriously,\n",
      "Average loss at step  12000 :  22.5424692836\n",
      "Average loss at step  14000 :  21.3239135704\n",
      "Average loss at step  16000 :  19.2517002304\n",
      "Average loss at step  18000 :  17.8247548909\n",
      "Average loss at step  20000 :  16.9471409855\n",
      "Nearest to rate: come, buy, l320, teddy27, easy, package, ryk, dish,\n",
      "Nearest to wii: farm, fair, b31, sweaty, cause, 75ct, lalaloopsys, sequined,\n",
      "Nearest to stone: solid, tall, coffey, starbucks, handling, tourette, coat, knot,\n",
      "Nearest to photos: bumdle, fireman, dolph, anarkali, illu, nkotb, compartment, smallmedium,\n",
      "Nearest to adidas: toddler, fit, kids, maroon, tan, striped, euc, jean,\n",
      "Nearest to workout: 535, same, slim, g15, castles, shredz, considered, saucers,\n",
      "Nearest to show: cured, emmit, betula, flawlessly, bathbomb, bypass, recover, inc,\n",
      "Nearest to pure: teaser, clawden, emblem, suv, moro, 35om, poudre, msrp,\n",
      "Nearest to year: loot, draggle, activities, powdered, greys, florsheim, lile, propgirl11,\n",
      "Nearest to deep: importance, crowding, samurai, lemongrass, ixcc, céline, packaging, intensifier,\n",
      "Nearest to bras: tiny, gymboree, anout, slip, bnwot, hero4, pour, intimates,\n",
      "Nearest to ultimate: seuss, khalil, xxx1, mercantile, fob, shantae, 046, aperal,\n",
      "Nearest to shoulder: cblack, taught, 7pm, nude, rue21, equal, hand, tablets,\n",
      "Nearest to neck: lined, red, large, pencil, light, button, polo, small,\n",
      "Nearest to polish: grabbed, babies, tkuehl, 32inch, tricolored, proactive, deeply, deliver,\n",
      "Nearest to leave: heavyweight, lyssa, calming, dragonlord, owlette, personnel, eighty, equivalence,\n",
      "Nearest to metal: hyperdunk, eau, marketplace, figuring, tyvm, brook, x16, claiborn,\n",
      "Nearest to skinny: playstation, true, boatshoe, periwinkle, metallic, rips, miss, book,\n",
      "Nearest to guaranteed: thumbholes, tounge, pricess, b02, beatssolo2, inclusion, cesar, scarred,\n",
      "Nearest to controller: thefoxandi, jafra, ibeats, moldeador, yogurt, leopards, firmness, 4xlt,\n",
      "Average loss at step  22000 :  16.0408313503\n",
      "Average loss at step  24000 :  15.2220310996\n",
      "Average loss at step  26000 :  15.3142652941\n",
      "Average loss at step  28000 :  14.5736837862\n",
      "Average loss at step  30000 :  13.9910969698\n",
      "Nearest to rate: come, l320, plastic, easy, dsi, buy, dignity, teddy27,\n",
      "Nearest to wii: farm, sequined, fair, cause, sweaty, 75ct, robe, b31,\n",
      "Nearest to stone: solid, tourette, handling, starbucks, chokers, believe, skull, tall,\n",
      "Nearest to photos: fists, dolph, illu, rainicorn, delrin, smallmedium, nkotb, tpu,\n",
      "Nearest to adidas: toddler, kids, women, low, maroon, men, shoes, sandals,\n",
      "Nearest to workout: youth, slim, euc, pajama, slides, puma, vans, bevy,\n",
      "Nearest to show: inc, cured, betula, flawlessly, fails, recover, bathbomb, bracing,\n",
      "Nearest to pure: midrange, teaser, clawden, emblem, owing, comm, celestia, suv,\n",
      "Nearest to year: loot, draggle, babolat, activities, powdered, propgirl11, lile, florsheim,\n",
      "Nearest to deep: importance, tearproof, compositions, crowding, petlem, samurai, intensifier, lemongrass,\n",
      "Nearest to bras: tiny, slip, gymboree, push, anout, stork, deadpool, bnwot,\n",
      "Nearest to ultimate: seuss, xxx1, unbanded, khalil, mercantile, aperal, raccoons, ecosusi,\n",
      "Nearest to shoulder: vans, adorable, cblack, hat, nude, bands, gimmie, fall,\n",
      "Nearest to neck: lined, button, material, nice, large, red, fits, back,\n",
      "Nearest to polish: unregulated, tkuehl, babies, grabbed, maxidress, beautfiful, 32inch, tricolored,\n",
      "Nearest to leave: lyssa, heavyweight, owlette, calming, dragonlord, eighty, aconic, personnel,\n",
      "Nearest to metal: eau, hyperdunk, what, message, marketplace, final, prices, order,\n",
      "Nearest to skinny: true, miss, periwinkle, jeans, short, mens, rips, distressed,\n",
      "Nearest to guaranteed: thumbholes, tounge, beatssolo2, b02, descr, pricess, inclusion, hydroblur,\n",
      "Nearest to controller: thefoxandi, misteriosa, firmness, jafra, ibeats, leopards, discomfort, makeupandmooree,\n",
      "Average loss at step  32000 :  13.5897434444\n",
      "Average loss at step  34000 :  13.205168679\n",
      "Average loss at step  36000 :  12.8514687214\n",
      "Average loss at step  38000 :  12.295922987\n",
      "Average loss at step  40000 :  12.5613285642\n",
      "Nearest to rate: l320, geonosis, come, dsi, plastic, dignity, hello, easy,\n",
      "Nearest to wii: farm, sequined, cause, 18r, cord, fair, famous, sparkle,\n",
      "Nearest to stone: chokers, solid, starbucks, believe, handling, tourette, ball, choker,\n",
      "Nearest to photos: fists, motifs, wristlett, knotty, rainicorn, shopmycloset, cuddley, smallmedium,\n",
      "Nearest to adidas: toddler, kids, women, low, men, shoes, boys, tshirt,\n",
      "Nearest to workout: youth, slim, pajama, vans, euc, green, joggers, slides,\n",
      "Nearest to show: inc, guc, cured, attached, betula, 1free, earphone, tryons,\n",
      "Nearest to pure: litsea, midrange, clawden, studding, teaser, orajel, kersh, celestia,\n",
      "Nearest to year: loot, draggle, babolat, activities, propgirl11, individuality, lile, florsheim,\n",
      "Nearest to deep: importance, swwet, intensifier, compositions, tearproof, petlem, samurai, crowding,\n",
      "Nearest to bras: push, tiny, slip, gymboree, deadpool, dog, bohodress, sport,\n",
      "Nearest to ultimate: seuss, xxx1, unbanded, raccoons, aperal, purposefully, mercantile, outdoor,\n",
      "Nearest to shoulder: strap, fleece, bands, adorable, looks, pocket, back, vans,\n",
      "Nearest to neck: button, lined, cotton, lace, material, sweater, fits, sleeves,\n",
      "Nearest to polish: xillion, beautfiful, tkuehl, sublimage, maxidress, unregulated, babies, frays,\n",
      "Nearest to leave: demibra, lyssa, owlette, heavyweight, calming, dragonlord, eighty, personnel,\n",
      "Nearest to metal: final, what, eau, buyers, message, packs, happy, prices,\n",
      "Nearest to skinny: miss, true, jeans, distressed, periwinkle, stretch, denim, high,\n",
      "Nearest to guaranteed: tounge, beatssolo2, thumbholes, b02, descr, vsxraine, pricess, 1333,\n",
      "Nearest to controller: thefoxandi, firmness, floam, ideapad, enviro, discomfort, makeupandmooree, jon,\n",
      "Average loss at step  42000 :  12.0275317805\n",
      "Average loss at step  44000 :  12.0117564065\n",
      "Average loss at step  46000 :  11.6014388804\n",
      "Average loss at step  48000 :  11.2950241628\n",
      "Average loss at step  50000 :  11.3759459183\n",
      "Nearest to rate: dsi, l320, geonosis, dignity, next, room, hyaluronate, 5x16,\n",
      "Nearest to wii: sequined, farm, cord, famous, 18r, conehead, enormous, nintendo,\n",
      "Nearest to stone: chokers, choker, starbucks, believe, ball, solid, table, handling,\n",
      "Nearest to photos: motifs, wristlett, reinstallation, 712, fists, commited, closeness, mimos,\n",
      "Nearest to adidas: women, low, boys, men, kids, youth, toddler, tshirt,\n",
      "Nearest to workout: joggers, green, youth, gray, slim, sweat, pajama, euc,\n",
      "Nearest to show: inc, guc, attached, cured, beach, seen, tryons, holds,\n",
      "Nearest to pure: litsea, studding, orajel, kersh, danid66, midrange, ysbb, clawden,\n",
      "Nearest to year: loot, draggle, propgirl11, babolat, activities, efs, lile, individuality,\n",
      "Nearest to deep: intensifier, compositions, importance, swwet, hharville, w110, labbit, face89,\n",
      "Nearest to bras: push, sport, dog, sports, secret, tiny, bohodress, deadpool,\n",
      "Nearest to ultimate: seuss, unbanded, xxx1, raccoons, purposefully, aperal, outdoor, spot,\n",
      "Nearest to shoulder: strap, adjustable, pocket, back, lunatics, front, looks, nothing,\n",
      "Nearest to neck: cotton, lace, button, sweater, lined, fits, sleeves, crew,\n",
      "Nearest to polish: xillion, beautfiful, frays, maxidress, tkuehl, jokingly, 32inch, biscayne,\n",
      "Nearest to leave: demibra, lyssa, owlette, heavyweight, calming, personnel, eighty, desk,\n",
      "Nearest to metal: final, buyers, packs, what, happy, prices, after, order,\n",
      "Nearest to skinny: jeans, miss, stretch, true, distressed, denim, american, wash,\n",
      "Nearest to guaranteed: luanna, beatssolo2, tounge, b02, thumbholes, descr, h2ocean, homegrown,\n",
      "Nearest to controller: floam, ideapad, kang, enviro, thefoxandi, olivey, spraying, firmness,\n"
     ]
    }
   ],
   "source": [
    "## Running the Embedding vector training model\n",
    "# this should result in a trained dictionary for \"name_and_desc\" column. can also be used if i were to keep name and item_desc\n",
    "# separated. training with 50k steps seemed sufficient, but odd word pairings did occur, so can be improved here\n",
    "\n",
    "\n",
    "# initialize some variables\n",
    "num_steps = 50001\n",
    "\n",
    "data = train_raw['name_and_desc']\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as session: #(graph=graph)\n",
    "    \n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run() # initialize variables (resets them too if previously trained)\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0 # initialize loss \n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        batch_inputs, batch_context = generate_batch(data,\n",
    "            batch_size, num_skips)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_context}\n",
    "        \n",
    "       \n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        _, loss_val = session.run([optimizer, nce_loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val # adding up for calculating average every 2000 steps\n",
    "\n",
    "        if step % 2000 == 0: # calculating average loss\n",
    "            if step > 0:\n",
    "                average_loss /= 2000 # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "            \n",
    "        if step % 10000 == 0: # printing out 8 \"most similar\" words for validation (subjectively i guess)\n",
    "            sim = similarity.eval() \n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dict[valid_examples[i]]\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dict[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "                \n",
    "    final_embeddings = normalized_embeddings.eval() # this should be the dense matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value Variable_89\n\t [[Node: Variable_89/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable_89\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Variable_89)]]\n\t [[Node: Sqrt_11/_11 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_343_Sqrt_11\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Variable_89/read', defined at:\n  File \"/home/bsong/anaconda/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/bsong/anaconda/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/traitlets/config/application.py\", line 653, in launch_instance\n    app.start()\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-80-6060fbb63523>\", line 32, in <module>\n    stddev=1.0 / np.sqrt(embedding_size)))\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 213, in __init__\n    constraint=constraint)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 356, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 125, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2071, in identity\n    \"Identity\", input=input, name=name)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable_89\n\t [[Node: Variable_89/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable_89\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Variable_89)]]\n\t [[Node: Sqrt_11/_11 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_343_Sqrt_11\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/home/bsong/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bsong/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bsong/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value Variable_89\n\t [[Node: Variable_89/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable_89\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Variable_89)]]\n\t [[Node: Sqrt_11/_11 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_343_Sqrt_11\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-6060fbb63523>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m# in the list of returned values for session.run()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer_regress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0maverage_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[0;31m# adding up for calculating average every 2000 steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bsong/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bsong/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bsong/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bsong/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value Variable_89\n\t [[Node: Variable_89/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable_89\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Variable_89)]]\n\t [[Node: Sqrt_11/_11 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_343_Sqrt_11\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Variable_89/read', defined at:\n  File \"/home/bsong/anaconda/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/bsong/anaconda/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/traitlets/config/application.py\", line 653, in launch_instance\n    app.start()\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-80-6060fbb63523>\", line 32, in <module>\n    stddev=1.0 / np.sqrt(embedding_size)))\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 213, in __init__\n    constraint=constraint)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 356, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py\", line 125, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2071, in identity\n    \"Identity\", input=input, name=name)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/bsong/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable_89\n\t [[Node: Variable_89/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable_89\"], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Variable_89)]]\n\t [[Node: Sqrt_11/_11 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_343_Sqrt_11\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "'''## make an embedding vector for brand_name, cat1/2/3\n",
    "# make vector representation based on price i guess\n",
    "# not optimized, but really, what is\n",
    "\n",
    "# Initialize batch parameters for generate_batch()\n",
    "batch_size = 300\n",
    "embedding_size = 15  # Dimension of the embedding vector.\n",
    "num_skips = 3         # How many times to reuse an input to generate a context.\n",
    "\n",
    "# initialize some variables\n",
    "num_steps = 50001\n",
    "\n",
    "data1 = train_raw['brand_name_inds']\n",
    "data2 = train_raw['cat1_inds']\n",
    "data3 = train_raw['cat2_inds']\n",
    "data4 = train_raw['cat3_inds']\n",
    "price = train_raw['price']\n",
    "\n",
    "vocab_sizes = [dict_brand_len, dict_cat1_len, dict_cat2_len, dict_cat3_len]\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.float32, shape=[batch_size, 1]) # context words\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)) # each word will have embedding\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs) # api that optimizes looking up hidden layer vector for word indices\n",
    "\n",
    "\n",
    "\n",
    "num_sampled = 1000\n",
    "weights_ = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / np.sqrt(embedding_size)))\n",
    "biases_ = tf.Variable(tf.zeros([batch_size]))\n",
    "weights2_ = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, 1],\n",
    "                            stddev=1.0 / np.sqrt(embedding_size)))\n",
    "biases2_ = tf.Variable(tf.zeros([batch_size,1]))\n",
    "\n",
    "alayer = tf.nn.bias_add(tf.matmul(weights_,tf.transpose(embed)),biases_)\n",
    "blayer = tf.matmul(tf.transpose(alayer),weights2_) + biases2_\n",
    "\n",
    "loss_ = tf.sqrt(tf.losses.mean_squared_error(train_labels, blayer))\n",
    "optimizer_regress = tf.train.AdamOptimizer(1.0).minimize(loss_) # optimizer for regression \n",
    "\n",
    "with tf.Session() as sess: #(graph=graph)\n",
    "    \n",
    "    # We must initialize all variables before we use them.\n",
    "    sess.run(init) # initialize variables (resets them too if previously trained)\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0 # initialize loss \n",
    "    counter = 0\n",
    "    for data_ in [data1, data2, data3, data4]:\n",
    "        \n",
    "        vocabulary_size = vocab_sizes[counter]\n",
    "        for step in range(num_steps):\n",
    "            batch_inputs, batch_context = generate_batch_general(data_, price,\n",
    "            batch_size)\n",
    "            batch_context = batch_context.reshape(-1,1)\n",
    "            feed_dict = {train_inputs: batch_inputs.astype(np.int32).values, train_labels: batch_context.astype(np.float32)}\n",
    "            # We perform one update step by evaluating the optimizer op (including it\n",
    "            # in the list of returned values for session.run()\n",
    "        \n",
    "            _, loss_val = sess.run([optimizer_regress, loss_], feed_dict=feed_dict) \n",
    "            average_loss += loss_val # adding up for calculating average every 2000 steps\n",
    "\n",
    "            if step % 2000 == 0: # calculating average loss\n",
    "                if step > 0:\n",
    "                    average_loss /= 2000 # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print('Average loss at step ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "        counter += 1\n",
    "        \n",
    "    \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## regular feed forward neural network function define here\n",
    "# This is to use for the simpler columns (brand_name, item_condition, cat1/2/3)\n",
    "# note to self: maybe separating dropout is better for manipulation purposes (and pooling and dropout lol.)\n",
    "\n",
    "def RegNN(x, dropout_keep_prob, W_shape, b_shape, embed_size, batch_len):\n",
    "  \n",
    "    # x should be of size [batch_len,embed_size] \n",
    "    #x_expand is now of length [batch_len, embed_size,1]\n",
    "    #x_expand = tf.expand_dims(x,-1) #channel dimension which i still dont know what it does\n",
    "    \n",
    "    # set up some weights/bias stuff\n",
    "    W1 = tf.Variable(tf.truncated_normal( [W_shape,batch_len], stddev=0.1))\n",
    "    b1 = tf.Variable(tf.constant(0.1, shape=[b_shape,1]))    \n",
    "    print(W1.shape)\n",
    "    print(b1.shape)\n",
    "    print(x.shape)\n",
    "    # xW + b \n",
    "    NN_layer = tf.matmul(W1,x) + b1\n",
    "    \n",
    "    # ReLU layer\n",
    "    h = tf.nn.relu(NN_layer)\n",
    "    \n",
    "    # Drop Layer\n",
    "    h_drop = tf.nn.dropout(h, dropout_keep_prob)\n",
    "    \n",
    "    # Dense layer (is this necessary?)\n",
    "    \n",
    "    return h_drop\n",
    "\n",
    "def embed(inputs, size, dim):\n",
    "    std = np.sqrt(2 / dim)\n",
    "    emb = tf.Variable(tf.random_uniform([size, dim], -std, std))\n",
    "    lookup = tf.nn.embedding_lookup(emb, inputs)\n",
    "    #print(lookup.shape)\n",
    "    return lookup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_lookup_46:0\", shape=(1482535, 15), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "brand_name_emb = embed(train_raw.brand_name_inds,dict_brand_len, 15)\n",
    "cat1_emb = embed(train_raw.cat1_inds,dict_cat1_len,10)\n",
    "cat2_emb = embed(train_raw.cat2_inds,dict_cat2_len,10)\n",
    "cat3_emb = embed(train_raw.cat3_inds,dict_cat3_len,10)\n",
    "\n",
    "print(brand_name_emb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Start here to rerun NN with different fitting and o/w\n",
    "\n",
    "# shuffle and split to train/val/test\n",
    "train_raw = train_raw.sample(frac=1).reset_index(drop=True) # shuffle the data \n",
    "nrow_train = round(train_raw.shape[0]*0.95) # index to split train/val with test (basically last 5% is test)\n",
    "\n",
    "dtest = train_raw.iloc[nrow_train:, ] # the last 5% will be test\n",
    "dtrain, dvalid = train_test_split(train_raw.iloc[:nrow_train, ], train_size=0.7, test_size = 0.3) # train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test block for CNN \n",
    "# based on http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n",
    "# note to self: maybe separating dropout is better for manipulation purposes (and pooling and dropout lol.)\n",
    "\n",
    "def test_cnn(input_x,input_y,W_shape,b_shape,dropout_keep_prob): \n",
    "    embedded_chars = tf.nn.embedding_lookup(embeddings, input_x)\n",
    "    embedded_chars_expanded = tf.expand_dims(embedded_chars, -1) # [batch,pad,embed,1]\n",
    "    W1 = tf.Variable(tf.truncated_normal(W_shape, stddev=0.1), name=\"W1\")\n",
    "    b1 = tf.Variable(tf.constant(0.1, shape=[b_shape]), name=\"b1\")\n",
    "    conv = tf.nn.conv2d(\n",
    "        embedded_chars_expanded,\n",
    "        W1,\n",
    "        strides = [1,1,1,1],\n",
    "        padding=\"VALID\",\n",
    "        name=\"conv\")\n",
    "    print('shape of CNN output:' + str(conv.shape))\n",
    "    h = tf.nn.relu(tf.nn.bias_add(conv, b1), name=\"relu\")\n",
    "    print('shape after ReLU: ' + str(h.shape))\n",
    "    pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, pad_length, 1, 1],\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    "    print('shape after max pooling: ' + str(pooled.shape))\n",
    "    pool_flat = tf.reshape(pooled, [-1, out_nodes])\n",
    "    print(\"shape after flattening:\" + str(pool_flat.shape))\n",
    "    # Add dropout\n",
    "    #with tf.name_scope(\"dropout\"):\n",
    "    h_drop = tf.nn.dropout(pool_flat, dropout_keep_prob)\n",
    "    print('shape after dropout: ' + str(h_drop.shape))\n",
    "    # output\n",
    "    #with tf.name_scope(\"output\"):\n",
    "    W2 = tf.Variable(tf.truncated_normal([out_nodes, 1], stddev=0.1), name=\"W2\")\n",
    "    b2 = tf.Variable(tf.constant(0.1, shape=[batch_len,1]), name=\"b2\")\n",
    "    predictions = tf.matmul(h_drop,W2) + b2\n",
    "    print('shape of predictions: ' + str(predictions.shape))\n",
    "    #predictions = tf.nn.xw_plus_b(h_drop, W2, b2, name=\"scores\")  \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None,1])\n",
    "\n",
    "\n",
    "out = RegNN(x, .5, 10, 10, 10,batch_len)\n",
    "feed_x = train_raw['brand_name'][:1000].values.reshape((1000,-1))\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init) \n",
    "sess.run(out,{x: feed_x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of CNN output:(?, 25, 1, 10)\n",
      "shape after ReLU: (?, 25, 1, 10)\n",
      "shape after max pooling: (?, 1, 1, 10)\n",
      "shape after flattening:(?, 10)\n",
      "shape after dropout: (?, 10)\n",
      "shape of predictions: (1000, 1)\n",
      "running step: 0\n",
      "1.02222\n",
      "running step: 1\n",
      "1.03364\n",
      "running step: 2\n",
      "0.946861\n",
      "running step: 3\n",
      "1.06321\n",
      "running step: 4\n",
      "1.00374\n",
      "running step: 5\n",
      "1.01493\n",
      "running step: 6\n",
      "1.00195\n",
      "running step: 7\n",
      "1.02219\n",
      "running step: 8\n",
      "0.97089\n",
      "running step: 9\n",
      "1.02509\n"
     ]
    }
   ],
   "source": [
    "input_x = tf.placeholder(tf.int64,[None, pad_length], name = \"input_x\") # pad_length = 25 or something defined earlier\n",
    "input_y = tf.placeholder(tf.float32,[None,1], name = \"input_y\") # train agianst this\n",
    "#batch_len = len(name_and_desc)\n",
    "#batch = name_and_desc[:batch_len]\n",
    "temp_set = np.concatenate((name_and_desc,train_raw['price'].values.reshape((-1,1))),axis = 1)\n",
    " \n",
    "batch_len = 1000\n",
    "\n",
    "out_nodes = 10\n",
    "dropout_keep_prob = .5\n",
    "\n",
    "W_shape = [1,embedding_size,1,out_nodes] #figure this out if it works\n",
    "b_shape = out_nodes # same as last dimension in W\n",
    "\n",
    "y_out = test_cnn(input_x,input_y,W_shape,b_shape,dropout_keep_prob)\n",
    "#with tf.name_scope(\"loss\"):\n",
    "loss = tf.sqrt(tf.losses.mean_squared_error(input_y, y_out))        \n",
    "train_step = tf.train.AdamOptimizer(learning_rate = .001).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init) \n",
    "#sess.run(train_step,{input_x: batch, input_y: batch_resp})\n",
    "\n",
    "losses = []\n",
    "for i in range(10): \n",
    "    print('running step: ' + str(i))\n",
    "    np.random.shuffle(temp_set) # shuffle the data\n",
    "    batch = temp_set[:batch_len,0:25]\n",
    "    batch_resp = temp_set[:batch_len,25].reshape((-1,1))\n",
    "    loss_ = sess.run(loss,{input_x: batch, input_y: batch_resp})\n",
    "    print(loss_)\n",
    "    sess.run(train_step,{input_x: batch, input_y: batch_resp})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## set up all the input (feature column) information for tensorflow\n",
    "\n",
    "max_brand = np.max(train_raw['brand_name']) + 1 # to set num_buckets in feature_columns \n",
    "max_cat1 = np.max(train_raw['cat1']) + 1 # these +1 are because of 0 index\n",
    "max_cat2 = np.max(train_raw['cat2']) + 1\n",
    "max_cat3 = np.max(train_raw['cat3']) + 1\n",
    "\n",
    "item_cond = tf.feature_column.categorical_column_with_identity(key = 'item_condition_id',num_buckets = 5,default_value= 3)\n",
    "brand_name = tf.feature_column.categorical_column_with_identity(key = 'brand_name',num_buckets = max_brand)\n",
    "shipping = tf.feature_column.categorical_column_with_identity(key = 'shipping',num_buckets = 2)\n",
    "cat1 = tf.feature_column.categorical_column_with_identity(key = 'cat1', num_buckets = max_cat1)\n",
    "cat2 = tf.feature_column.categorical_column_with_identity(key = 'cat2', num_buckets = max_cat2)\n",
    "cat3 = tf.feature_column.categorical_column_with_identity(key = 'cat3', num_buckets = max_cat3)\n",
    "#names = tf.feature_column.categorical_column_with_identity(key = 'name_inds', num_buckets = vocabulary_size)\n",
    "#item_desc = tf.feature_column.categorical_column_with_identity(key = 'item_desc_inds',num_buckets = vocabulary_size)\n",
    "name_and_desc = tf.feature_column.categorical_column_with_identity(key = 'padded_name_desc', num_buckets = vocabulary_size)\n",
    "\n",
    "price = tf.feature_column.numeric_column(key = 'price', dtype = tf.float32) # LABEL\n",
    "\n",
    "embed_ = tf.feature_column.embedding_column\n",
    "#feature_columns = [item_cond, brand_name, shipping, cat1, cat2, cat3] #, names, item_desc\n",
    "feature_columns = [embed_(item_cond,5), embed_(brand_name,max_brand), \n",
    "                   embed_(shipping,2), embed_(cat1,max_cat1), embed_(cat2, max_cat2), \n",
    "                   embed_(cat3,max_cat3), embed_(name_and_desc,25)] #, names, item_desc\n",
    "feature_names = ['item_condition_id','brand_name','shipping','cat1','cat2','cat3','name_and_desc'] #,'name_inds','item_desc_inds'\n",
    "label_name = 'price'\n",
    "\n",
    "#with tf.device(\"/cpu:0\"): # used to convert input word index vectors to meaningful vectors for neural network \n",
    "#    embedding = tf.Variable(tf.random_uniform([vocab_size, self.hidden_size], -init_scale, init_scale))\n",
    "#    inputs = tf.nn.embedding_lookup(embedding, self.input_obj.input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.feature_column.embedding_column(item_cond,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## build an input_fn to feed data into tensorflow properly\n",
    "\n",
    "def input_fn(data_file, num_epochs=1, batch_size = 1000): \n",
    "    # make columns into dictionary\n",
    "    feat_dict = dict()\n",
    "    for col_name in feature_names:\n",
    "        feat_dict[col_name] = data_file[col_name]\n",
    "    label = tf.constant(data_file[label_name].values,dtype = tf.float32)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((feat_dict, label))\n",
    "\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    dataset = dataset.batch(batch_size) \n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    features, label = iterator.get_next()\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## high level api for basic deep feed forward neural network\n",
    "\n",
    "regressor = tf.estimator.DNNRegressor( \n",
    "    model_dir = '/home/bsong/Python_Stuff/Data/Kaggle_Mercari/model/',\n",
    "    feature_columns = feature_columns, \n",
    "    hidden_units = [256,128,64],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## train the model\n",
    "\n",
    "regressor.train(input_fn = lambda: input_fn(dtrain), steps = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## evaluate the model on validation set (assuming this is useful for cross-entropy or something)\n",
    "\n",
    "ev = regressor.evaluate(input_fn = lambda: input_fn(dvalid))\n",
    "loss_score = ev['loss']\n",
    "print('Loss: {0:f}'.format(loss_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## prediction on test set\n",
    "\n",
    "y = regressor.predict(input_fn = lambda: input_fn(dtest))\n",
    "\n",
    "predictions = [i for i in y] # y is a generator, so have to manipulate to extract\n",
    "preds = list()\n",
    "for i in predictions:\n",
    "    temp = list(i.values())\n",
    "    preds.append(temp[0])\n",
    "preds = np.array(preds)\n",
    "preds = np.squeeze(preds, axis=1)\n",
    "preds = np.expm1(preds) # revert the 1+log(x) transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## calculate the RMSLE score to compare to online submissions\n",
    "\n",
    "rmsle_pred = rmsle(preds,np.expm1(dtest['price'].values)) \n",
    "print(rmsle_pred)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
