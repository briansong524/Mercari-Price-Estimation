{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Made price into bins\n",
    "'''\n",
    "\n",
    "## import packages\n",
    "import time\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import random\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import nltk\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import data took 6.030135631561279 seconds.\n"
     ]
    }
   ],
   "source": [
    "## import data \n",
    "start_time = time.time()\n",
    "\n",
    "train_raw = pd.read_csv('/home/bsong/Python_Stuff/Data/Kaggle_Mercari/train.tsv',delimiter= '\\t')\n",
    "test_raw = pd.read_csv('/home/bsong/Python_Stuff/Data/Kaggle_Mercari/test.tsv', delimiter='\\t')\n",
    "#train_raw = train_raw.iloc[0:10000,] # just a bit\n",
    "# standardize price here because may as well\n",
    "#normalized_price = np.log1p(train_raw['price'].values)\n",
    "#mean_price_norm = np.mean(normalized_price)\n",
    "#std_price_norm = np.std(normalized_price) \n",
    "#train_raw['price'] = (normalized_price - mean_price_norm)/std_price_norm \n",
    "\n",
    "end_time = time.time()\n",
    "print('import data took ' + str(end_time - start_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsong/anaconda/lib/python3.5/site-packages/pandas/core/indexing.py:132: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "## bin price one method\n",
    "\n",
    "binsize_sub200 = 10\n",
    "binsize_200_1000 = 10\n",
    "binsize_sup1000 = 15\n",
    "\n",
    "total_bins = binsize_sub200 + binsize_200_1000 + binsize_sup1000\n",
    "\n",
    "inds_sub200 = train_raw.train_id[train_raw.price <= 200].values\n",
    "inds_200_1000 = train_raw.train_id[(train_raw.price > 200) & (train_raw.price <= 1000)].values\n",
    "inds_sup1000 = train_raw.train_id[train_raw.price > 1000].values\n",
    "\n",
    "price_sub200 = train_raw.price[train_raw.price <= 200].values\n",
    "price_200_1000 = train_raw.price[(train_raw.price > 200) & (train_raw.price <= 1000)].values\n",
    "price_sup1000 = train_raw.price[train_raw.price > 1000].values\n",
    "\n",
    "bins_sub200 = np.linspace(1, 200, binsize_sub200)\n",
    "bins_200_1000 = np.linspace(200,1000, binsize_200_1000)\n",
    "bins_sup1000 = np.linspace(1000, np.max(train_raw.price.values), binsize_sup1000)\n",
    "\n",
    "price_bins_sub200 = np.digitize(price_sub200, bins_sub200)\n",
    "price_bins_200_1000 = np.digitize(price_200_1000, bins_200_1000)\n",
    "price_bins_sup1000 = np.digitize(price_sup1000, bins_sup1000)\n",
    "#bin_means_sub_1000 = [train_raw.price.values[bins_sub1000 == i].mean() for i in range(1, len(bins))]\n",
    "\n",
    "bin_mean_sub200 = np.array([price_sub200[price_bins_sub200 == i].mean() for i in range(1, binsize_sub200+1)])\n",
    "bin_mean_200_1000 = np.array([price_200_1000[price_bins_200_1000 == i].mean() for i in range(1,binsize_200_1000+1)])\n",
    "bin_mean_sup1000 = np.array([price_sup1000[price_bins_sup1000 == i].mean() for i in range(1, binsize_sup1000+1)])\n",
    "\n",
    "bin_std_sub200 = [price_sub200[price_bins_sub200 == i].std() for i in range(1, binsize_sub200+1)]\n",
    "bin_std_200_1000 = [price_200_1000[price_bins_200_1000 == i].std() for i in range(1,binsize_200_1000+1)]\n",
    "bin_std_sup1000 = [price_sup1000[price_bins_sup1000 == i].std() for i in range(1, binsize_sup1000+1)]\n",
    "\n",
    "train_raw['price_bins'] = [0]*train_raw.shape[0]\n",
    "train_raw['price_bins_mean'] = [0]*train_raw.shape[0]\n",
    "\n",
    "train_raw.price_bins.iloc[inds_sub200] = price_bins_sub200\n",
    "train_raw.price_bins.iloc[inds_200_1000] = price_bins_200_1000 + binsize_sub200\n",
    "train_raw.price_bins.iloc[inds_sup1000] = price_bins_sup1000 + binsize_sub200 + binsize_200_1000\n",
    "\n",
    "train_raw.price_bins_mean.iloc[inds_sub200] = bin_mean_sub200[price_bins_sub200-1]\n",
    "train_raw.price_bins_mean.iloc[inds_200_1000] = bin_mean_200_1000[price_bins_200_1000-1]\n",
    "train_raw.price_bins_mean.iloc[inds_sup1000] = bin_mean_sup1000[price_bins_sup1000-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsong/anaconda/lib/python3.5/site-packages/pandas/core/indexing.py:132: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "## bin price another method\n",
    "sort_price = sorted(train_raw.price.values)\n",
    "perc1 = np.percentile(sort_price,25)\n",
    "perc2 = np.percentile(sort_price,75)\n",
    "perc3 = np.percentile(sort_price,90)\n",
    "perc4 = np.percentile(sort_price,98)\n",
    "\n",
    "total_bins = 5\n",
    "inds1 = train_raw.train_id[train_raw.price <= perc1].values\n",
    "inds2 = train_raw.train_id[(train_raw.price > perc1) & (train_raw.price <= perc2)].values\n",
    "inds3 = train_raw.train_id[(train_raw.price > perc2) & (train_raw.price <= perc3)].values\n",
    "inds4 = train_raw.train_id[(train_raw.price > perc3) & (train_raw.price <= perc4)].values\n",
    "inds5 = train_raw.train_id[(train_raw.price > perc4)].values \n",
    "\n",
    "price_1 = train_raw.price[train_raw.price <= perc1].values\n",
    "price_2 = train_raw.price[(train_raw.price > perc1) & (train_raw.price <= perc2)].values\n",
    "price_3 = train_raw.price[(train_raw.price > perc2) & (train_raw.price <= perc3)].values\n",
    "price_4 = train_raw.price[(train_raw.price > perc3) & (train_raw.price <= perc4)].values\n",
    "price_5 = train_raw.price[train_raw.price > perc4].values\n",
    "\n",
    "train_raw['price_bins'] = [0]*train_raw.shape[0]\n",
    "train_raw['price_bins_mean'] = [0]*train_raw.shape[0]\n",
    "\n",
    "train_raw.price_bins.iloc[inds1] = 1\n",
    "train_raw.price_bins.iloc[inds2] = 2\n",
    "train_raw.price_bins.iloc[inds3] = 3\n",
    "train_raw.price_bins.iloc[inds4] = 4\n",
    "train_raw.price_bins.iloc[inds5] = 5\n",
    "\n",
    "train_raw.price_bins_mean.iloc[inds1] = price_1.mean()\n",
    "train_raw.price_bins_mean.iloc[inds2] = price_2.mean()\n",
    "train_raw.price_bins_mean.iloc[inds3] = price_3.mean()\n",
    "train_raw.price_bins_mean.iloc[inds4] = price_4.mean()\n",
    "train_raw.price_bins_mean.iloc[inds5] = price_5.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.0, 29.0, 51.0, 122.0]\n"
     ]
    }
   ],
   "source": [
    "print([perc1,perc2,perc3, perc4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define functions to use\n",
    "\n",
    "######## Basic text manipulation functions (some specific to Mercari Kaggle Competition) \n",
    "\n",
    "def split_cat(text): # this one is to reduce the categoriy_name into three subcategories\n",
    "    try: return text.split(\"/\")\n",
    "    except: return (\"No Label\", \"No Label\", \"No Label\")\n",
    "\n",
    "def handle_missing_inplace(dataset):  # this one is to put placeholders in place of missing values (NaN)\n",
    "    dataset['cat1'].fillna(value='No Label', inplace=True)\n",
    "    dataset['cat2'].fillna(value='No Label', inplace=True)\n",
    "    dataset['cat3'].fillna(value='No Label', inplace=True)\n",
    "    dataset['brand_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['item_description'].fillna(value='No description yet', inplace=True)\n",
    "     \n",
    "def build_dictionary(words, n_words): # dictionary that maps words to indices. this function should be modular.\n",
    "    #input is [['a','b','c'],['a','b','c']]\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]] # word indexed as \"unknown\" if not one of the top #n_words (popular/common) words (-1 is filler #)\n",
    "    count.extend(Counter(words).most_common(n_words - 1)) # most_common returns the top (n_words-1) ['word',count]\n",
    "    dictionary = dict()\n",
    "    for word, _ in count: # the 'word, _' is writted because count is a list of list(2), so defining 'word' as the first term per\n",
    "        dictionary[word] = len(dictionary) # {'word': some number incrementing by one. fyi, no repeats because from most_common)}\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys())) # {ind. : 'word'} I guess for looking up if needed?\n",
    "    return dictionary, reversed_dictionary\n",
    "\n",
    "def clean_and_tokenize(dataset_col): # input is a column of strings\n",
    "    pattern = '[A-Za-z]+' # does this only keep words\n",
    "    pattern2 = '[!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n]' # to rid of special characters\n",
    "    list_of_lists = list()\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    words_to_remove = ['description', 'yet','like','the','any','they']\n",
    "    words_to_remove.extend(stopwords.words('english'))\n",
    "    words_to_remove = set(words_to_remove)\n",
    "    \n",
    "    for word in dataset_col:\n",
    "        list_of_words = list()\n",
    "        word = re.sub(pattern2, r'', word)\n",
    "        tokenized = tokenizer.tokenize(word)\n",
    "        tokenized_filtered = filter(lambda token: token not in words_to_remove, tokenized)\n",
    "        for i in tokenized_filtered:\n",
    "            if (len(i) > 2 ): #ignore words of length 2 or less\n",
    "                list_of_words.append(i.lower()) # append all words to one list\n",
    "        list_of_lists.append(list_of_words)\n",
    "    list_as_series = pd.Series(list_of_lists)\n",
    "    return list_as_series\n",
    "\n",
    "def convert_word_to_ind(dataset_col,dictionary): # input the pandas column of texts and dictionary. This should be modular\n",
    "    # each input should be a string of cleaned words tokenized into a list (ex. ['this', 'is', 'an', 'item'])\n",
    "    # dictionary should be the dictionary obtained from build_dictionary\n",
    "    list_of_lists = []\n",
    "    unk_count = 0 # total 'unknown' words counted\n",
    "    for word_or_words in dataset_col: # words is the list of all words\n",
    "        list_of_inds = []\n",
    "        for word in word_or_words:\n",
    "            if word in dictionary:\n",
    "                index = np.int(dictionary[word]) # dictionary contains top words, so if in, it gets an index\n",
    "            else:\n",
    "                index = 0  #  or dictionary['UNK']? can figure out later\n",
    "                unk_count += 1\n",
    "            list_of_inds.append(index)\n",
    "        list_of_lists.append(list_of_inds)\n",
    "\n",
    "    # make list_of_lists into something that can be put into pd.DataFrame\n",
    "    #list_as_series = pd.Series(list_of_lists)\n",
    "    list_as_series = np.array(list_of_lists)\n",
    "    return list_as_series, unk_count\n",
    "\n",
    "def pad_word_indices(col_of_indices, pad_length): # col_of_indices can be a pd series. \n",
    "    temp_series = [] # append vectors into here\n",
    "    for list_inds in col_of_indices:\n",
    "        len_list = len(list_inds)\n",
    "        if len_list >= pad_length:\n",
    "            temp_series.append(np.array(list_inds[(len_list-pad_length):]))\n",
    "        else:\n",
    "            padded_vec = [0]*(pad_length-len_list)\n",
    "            padded_vec.extend(list_inds)\n",
    "            temp_series.append(np.array(padded_vec))\n",
    "    return temp_series\n",
    "\n",
    "def convert_word_to_padded(dataset_col,dictionary,pad_length): # input the pandas column of texts and dictionary. This should be modular\n",
    "    # each input should be a string of cleaned words tokenized into a list (ex. ['this', 'is', 'an', 'item'])\n",
    "    # dictionary should be the dictionary obtained from build_dictionary\n",
    "    # use this function when you know how long you want your pad_length\n",
    "    #   - otherwise, use convert_word_to_ind, and pad_word_indices\n",
    "    #   - eventually, will look into cleaning these three functions up.\n",
    "    list_of_lists = []\n",
    "    unk_count = 0 # total 'unknown' words counted\n",
    "    for word_or_words in dataset_col: # words is the list of all words\n",
    "        list_of_inds = []\n",
    "        count_inds = 0\n",
    "        for word in word_or_words:\n",
    "            if word in dictionary:\n",
    "                index = np.int(dictionary[word]) # dictionary contains top words, so if in, it gets an index\n",
    "            else:\n",
    "                index = 0  #  or dictionary['UNK']? can figure out later\n",
    "                unk_count += 1\n",
    "            count_inds +=1\n",
    "            list_of_inds.append(index) \n",
    "        if count_inds >= pad_length:\n",
    "            asdf = list_of_inds[(count_inds-pad_length):]\n",
    "        else: \n",
    "            asdf = [0]*(pad_length-count_inds)\n",
    "            asdf.extend(list_of_inds)\n",
    "        temp = np.array(asdf)\n",
    "        list_of_lists.append(temp)\n",
    "    list_as_series = np.array(list_of_lists)\n",
    "    return list_as_series, unk_count\n",
    "\n",
    "######## Word Embedding (this is after strings are transformed into vectors of indices)\n",
    "\n",
    "# generate batch data (for feeding into word embedding)\n",
    "# used http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/ for reference\n",
    "def generate_batch(data, batch_size, num_skips): \n",
    "    # data should be [[3,7,9],[7,4,5,9],...] kinda format\n",
    "    # num_skips configures number of context words to draw. skip_window defines size of window to draw context words from\n",
    "    assert batch_size % num_skips == 0 # if batch_size was 10, and num_skips was 3, then [cat,cat,cat,sat,sat,sat,...] wont equal\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32) # initialize batch variable (input word go in here)\n",
    "    context = np.ndarray(shape=(batch_size, 1), dtype=np.int32) # initialize context variable\n",
    "    counter = 0\n",
    "    rand_dat_ind = random.sample(range(0,len(data)-1),int(batch_size/num_skips))\n",
    "    for i in data[rand_dat_ind]:\n",
    "        while len(i) <= num_skips:\n",
    "            rnd_again = random.randint(0,len(data)-1)\n",
    "            i = data[rnd_again]\n",
    "        target = random.randint(0,len(i)-1) \n",
    "        targets_to_avoid = [target] # avoid this index when selecting rando words\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid: # this is to choose an index that isnt the index of the batch word\n",
    "                target = random.randint(0, len(i)-1) # target is a context word\n",
    "            targets_to_avoid.append(target) # so next time, this loop won't select this context word again \n",
    "            batch[counter] = i[targets_to_avoid[0]]  # this is the input word (same word repeated i*num_skips+j times)\n",
    "            context[counter, 0] = i[targets_to_avoid[j+1]]  # these are the context words to the batch word\n",
    "            counter += 1\n",
    "    return batch, context # batch is input, context is target variable(s)\n",
    "\n",
    "def generate_batch_general(x, y, batch_size):\n",
    "    # this is to generate batches for word2vec comparing against numeric values \n",
    "    # in this case, 'brand_name' and cat1/2/3 are compared against 'price'\n",
    "    rand_dat_ind = random.sample(range(0,len(data)-1),int(batch_size))\n",
    "    return x[rand_dat_ind], y[rand_dat_ind]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning \"category_name\" and making one-worded features to indices took 9.355466842651367 seconds.\n"
     ]
    }
   ],
   "source": [
    "## clean \"category_name\" and make numeric indicies for one-worded features (brand_name, cat1/2/3)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_raw['cat1'],train_raw['cat2'],train_raw['cat3'] = \\\n",
    "zip(*train_raw['category_name'].apply(lambda x: split_cat(x))) # split the categories into three new columns\n",
    "train_raw.drop('category_name',axis = 1, inplace = True) # remove the column that isn't needed anymore\n",
    "\n",
    "handle_missing_inplace(train_raw) # replaces NaN with a string placeholder 'missing'\n",
    "\n",
    "test_raw['cat1'],test_raw['cat2'],test_raw['cat3'] = \\\n",
    "zip(*test_raw['category_name'].apply(lambda x: split_cat(x))) # split the categories into three new columns\n",
    "test_raw.drop('category_name',axis = 1, inplace = True) # remove the column that isn't needed anymore\n",
    "\n",
    "handle_missing_inplace(test_raw) # replaces NaN with a string placeholder 'missing'\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print('cleaning \"category_name\" and making one-worded features to indices took ' + str(end_time - start_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack all name and item description words took 68.577 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "all_name_desc_train = np.hstack((train_raw['name'],train_raw['item_description'])) # get all dem words\n",
    "all_name_desc_test = np.hstack((test_raw.name,test_raw.item_description))\n",
    "all_name_desc = np.hstack((all_name_desc_train, all_name_desc_test))\n",
    "all_name_desc = clean_and_tokenize(all_name_desc)\n",
    "all_name_desc = [item for sublist in all_name_desc for item in sublist]\n",
    "print(\"stack all name and item description words took %5.3f seconds\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obtain_reasonable_vocab_size(list_words, perc_words = .95):\n",
    "    counter_ = Counter(list_words)\n",
    "    counts = [i for _,i in counter_.most_common()]\n",
    "    tot_words = len(list_words)\n",
    "    print('total words (with repeats): ' + str(tot_words))\n",
    "    tot_count = 0\n",
    "    runs = 0\n",
    "    while tot_count < round(perc_words*tot_words):\n",
    "        tot_count += counts[runs]\n",
    "        runs += 1\n",
    "    print('reasonable vocab size: ' + str(runs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting name and item_desc to indices and config took 83.5229868888855 seconds.\n"
     ]
    }
   ],
   "source": [
    "## convert name and item_desc to indices, then configure a bit more\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_raw['name'] = clean_and_tokenize(train_raw['name'])\n",
    "train_raw['item_description'] = clean_and_tokenize(train_raw['item_description'])\n",
    "test_raw['name'] = clean_and_tokenize(test_raw['name'])\n",
    "test_raw['item_description'] = clean_and_tokenize(test_raw['item_description'])\n",
    "\n",
    "#make new columns of just the indices of the words for name and item_description\n",
    "vocabulary_size = 9803 # keeping 9803 words in the dictionary. can adjust later. will use variable elsewhere\n",
    "word2vec_dict, reverse_dict = build_dictionary(all_name_desc,vocabulary_size) \n",
    "#are these even necessary lol. for analysis only i guess\n",
    "#train_raw['name_inds'], _ = convert_word_to_ind(train_raw['name'],word2vec_dict) \n",
    "#train_raw['item_desc_inds'], _= convert_word_to_ind(train_raw['item_description'], word2vec_dict)  \n",
    "#test_raw['name_inds'], _ = convert_word_to_ind(test_raw['name'],word2vec_dict) \n",
    "#test_raw['item_desc_inds'], _ = convert_word_to_ind(test_raw['item_description'], word2vec_dict)  \n",
    "\n",
    "end_time = time.time()\n",
    "print('converting name and item_desc to indices and config took ' + str(end_time - start_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ANALYSIS ONLY\\n## deciding pad length for name and item_desc  \\ngroup_name = np.hstack((train_raw.name_inds, test_raw.name_inds))\\ngroup_itemdesc = np.hstack((train_raw.item_desc_inds, test_raw.item_desc_inds))\\na = [len(i) for i in group_name ]\\nprint(\"max tokens in \\'name\\': \" + str(max(a)))\\n\\nb = [len(i) for i in group_itemdesc]\\nplt.hist(b,20)\\nplt.show()\\nprint(\"max tokens in \\'item_desc\\': \" + str(max(b)))\\n\\nsort_b = sorted(b) #sorted length in increasing order\\nperc_data = .95\\nlen_item_desc_potential = sort_b[round(perc_data*len(sort_b))]\\nprint(len_item_desc_potential) # this represents (perc_data)% of item descriptions are under (len_item_desc_potential) words\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' ANALYSIS ONLY\n",
    "## deciding pad length for name and item_desc  \n",
    "group_name = np.hstack((train_raw.name_inds, test_raw.name_inds))\n",
    "group_itemdesc = np.hstack((train_raw.item_desc_inds, test_raw.item_desc_inds))\n",
    "a = [len(i) for i in group_name ]\n",
    "print(\"max tokens in 'name': \" + str(max(a)))\n",
    "\n",
    "b = [len(i) for i in group_itemdesc]\n",
    "plt.hist(b,20)\n",
    "plt.show()\n",
    "print(\"max tokens in 'item_desc': \" + str(max(b)))\n",
    "\n",
    "sort_b = sorted(b) #sorted length in increasing order\n",
    "perc_data = .95\n",
    "len_item_desc_potential = sort_b[round(perc_data*len(sort_b))]\n",
    "print(len_item_desc_potential) # this represents (perc_data)% of item descriptions are under (len_item_desc_potential) words\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique brands: 5290\n",
      "Number of unique categories in cat1: 11\n",
      "Number of unique categories in cat1: 114\n",
      "Number of unique categories in cat1: 883\n"
     ]
    }
   ],
   "source": [
    "## deciding dictionary size for brand and cat1/2/3\n",
    "\n",
    "group_brand = np.hstack((train_raw.brand_name.values,test_raw.brand_name.values)) \n",
    "group_cat1 = np.hstack((train_raw.cat1.values, test_raw.cat1.values))\n",
    "group_cat2 = np.hstack((train_raw.cat2.values, test_raw.cat2.values))\n",
    "group_cat3 = np.hstack((train_raw.cat3.values, test_raw.cat3.values))\n",
    "\n",
    "brand_unique_set = set(group_brand)\n",
    "print('Number of unique brands: ' + str(len(brand_unique_set)))\n",
    "\n",
    "cat1_set = set(group_cat1)\n",
    "print('Number of unique categories in cat1: ' + str(len(cat1_set)))\n",
    "\n",
    "cat2_set = set(group_cat2)\n",
    "print('Number of unique categories in cat1: ' + str(len(cat2_set)))\n",
    "\n",
    "cat3_set = set(group_cat3)\n",
    "print('Number of unique categories in cat1: ' + str(len(cat3_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words (with repeats): 2175894\n",
      "reasonable vocab size: 304\n"
     ]
    }
   ],
   "source": [
    "obtain_reasonable_vocab_size(group_cat3,.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making dictionaries for brand and categories took 37.27608680725098 seconds.\n"
     ]
    }
   ],
   "source": [
    "## make dictionaries for brand_name and cat1/2/3\n",
    "\n",
    "start_time = time.time()\n",
    "# define dictionary lengths here\n",
    "dict_brand_len = 378 # total words + 1 for \"UNK\" is minimal size\n",
    "dict_cat1_len = 12 \n",
    "dict_cat2_len= 115 \n",
    "dict_cat3_len = 305 \n",
    "\n",
    "brand_name_dict, brand_name_dict_rev = build_dictionary(group_brand, dict_brand_len)\n",
    "train_raw['brand_name_inds'], _ = convert_word_to_ind(train_raw['brand_name'].values.reshape((-1,1)), brand_name_dict)\n",
    "test_raw['brand_name_inds'], _ = convert_word_to_ind(test_raw['brand_name'].values.reshape((-1,1)), brand_name_dict)\n",
    "\n",
    "cat1_dict ,cat1_rev_dict= build_dictionary(group_cat1,dict_cat1_len)\n",
    "train_raw['cat1_inds'], _ = convert_word_to_ind(train_raw['cat1'].values.reshape((-1,1)), cat1_dict)\n",
    "test_raw['cat1_inds'], _ = convert_word_to_ind(test_raw['cat1'].values.reshape((-1,1)), cat1_dict)\n",
    "\n",
    "cat2_dict ,cat2_rev_dict= build_dictionary(group_cat2,dict_cat2_len)\n",
    "train_raw['cat2_inds'], _ = convert_word_to_ind(train_raw['cat2'].values.reshape((-1,1)), cat2_dict)\n",
    "test_raw['cat2_inds'], _ = convert_word_to_ind(test_raw['cat2'].values.reshape((-1,1)), cat2_dict)\n",
    "\n",
    "cat3_dict ,cat3_rev_dict= build_dictionary(group_cat3,dict_cat3_len)\n",
    "train_raw['cat3_inds'], _ = convert_word_to_ind(train_raw['cat3'].values.reshape((-1,1)), cat3_dict)\n",
    "test_raw['cat3_inds'], _ = convert_word_to_ind(test_raw['cat3'].values.reshape((-1,1)), cat3_dict)\n",
    "\n",
    "end_time = time.time()\n",
    "print('making dictionaries for brand and categories took ' + str(end_time - start_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting name and item_desc to padded indices took 38.100067377090454 seconds.\n"
     ]
    }
   ],
   "source": [
    "## padding name and item_desc here. these will be trained in the final model (as opposed to pretrained)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "name_pad_size = 9 # max length of name\n",
    "itemdesc_pad_size = 60 # 95th percentile of length of item descriptions\n",
    "\n",
    "name_padded , _ = convert_word_to_padded(train_raw.name,word2vec_dict,name_pad_size) # without _, will get tuple lol.\n",
    "itemdesc_padded , _ = convert_word_to_padded(train_raw.item_description,word2vec_dict,itemdesc_pad_size) \n",
    "name_padded_test , _ = convert_word_to_padded(test_raw.name,word2vec_dict,name_pad_size) # without _, will get tuple lol.\n",
    "itemdesc_padded_test , _ = convert_word_to_padded(test_raw.item_description,word2vec_dict,itemdesc_pad_size) \n",
    "\n",
    "end_time = time.time()\n",
    "print('converting name and item_desc to padded indices took ' + str(end_time - start_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## regular neural network function define here\n",
    "# This is to use for the simpler columns (brand_name, item_condition, cat1/2/3)\n",
    "# note to self: maybe separating dropout is better for manipulation purposes (and pooling and dropout lol.)\n",
    "\n",
    "# RegNN used for converting embedded features into whatever out_nodes. \n",
    "# I feel dense_NN achieves the exact same thing, but one layer, but this happened because I was iteratively progressing through this \n",
    "# project and didn't want to erase too many things. 1/11/18\n",
    "\n",
    "def RegNN(x, dropout_keep_prob, vocab_size, embed_size, batch_len, out_len):\n",
    "    #print('shape of input:' + str(x.shape))\n",
    "    # x should be of size [batch_len,embed_size] \n",
    "    # set up some weights/bias stuff\n",
    "    W1 = tf.Variable(tf.truncated_normal([vocab_size,embed_size], stddev=0.1))\n",
    "    b1 = tf.Variable(tf.constant(0.1, shape=[vocab_size,1])) # maybe batch_len   \n",
    "    #print('shape of W1:' + str(W1.shape))\n",
    "    #print('shape of b1:' + str(b1.shape))\n",
    "    \n",
    "    # xW + b \n",
    "    NN_layer = tf.matmul(W1,tf.transpose(x)) + b1 # this outputs shape (vocab_size,batch_len)\n",
    "    #print('NN_layer shape: ' + str(NN_layer.shape)) \n",
    "    # ReLU layer\n",
    "    \n",
    "    h = tf.nn.relu(NN_layer)\n",
    "    \n",
    "    # Drop Layer\n",
    "    h_drop = tf.nn.dropout(h, dropout_keep_prob) # still (vocab_size,batch_len)\n",
    "    \n",
    "    #W2 = tf.Variable(tf.truncated_normal([vocab_size,out_len]))\n",
    "    #b2 = tf.constant(0.1, shape=[batch_len,1])\n",
    "    \n",
    "    #NN_layer2 = tf.matmul(tf.transpose(h_drop),W2) + b2 # this outputs shape (batch_len,out_len)\n",
    "    #print('NN_layer2 shape: ' + str(NN_layer2.shape))\n",
    "    #h2 = tf.nn.relu(NN_layer2)\n",
    "    #h2_drop = tf.nn.dropout(h2, dropout_keep_prob) # should be of length (batch_len, out_len)\n",
    "    \n",
    "    return h_drop #h2_drop\n",
    "\n",
    "\n",
    "def embed(inputs, size, dim,name):\n",
    "    # inputs is a list of indices\n",
    "    # size is the number of unique indices (look for max index to achieve this if ordered)\n",
    "    # dim is the number of embedded numbers \n",
    "    std = np.sqrt(2 / dim)\n",
    "    emb = tf.Variable(tf.random_uniform([size, dim], -std, std))\n",
    "    lookup = tf.nn.embedding_lookup(emb, inputs,name = name)\n",
    "    #print(lookup.shape)\n",
    "    return lookup\n",
    "\n",
    "# test block for CNN \n",
    "# based on http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n",
    "# note to self: maybe separating dropout is better for manipulation purposes (and pooling and dropout lol.)\n",
    "\n",
    "def CNN(x,W_shape,b_shape,pad_length, name_w, name_b):\n",
    "    # x is the expanded lookup tables that will be trained\n",
    "    W1 = tf.Variable(tf.truncated_normal(W_shape, stddev=0.1), name= name_w) #\"W1\"\n",
    "    b1 = tf.Variable(tf.constant(0.1, shape=[b_shape]), name = name_b) # \"b1\"\n",
    "    conv = tf.nn.conv2d( #tf.layers.conv2d is also used, with  more parameters. Probably a slightly higher API because of that.\n",
    "        x,\n",
    "        W1,\n",
    "        strides = [1,1,1,1],\n",
    "        padding=\"VALID\",\n",
    "        name=\"conv\")\n",
    "    #print('shape of CNN output:' + str(conv.shape))\n",
    "    h = tf.nn.relu(tf.nn.bias_add(conv, b1), name=\"relu\")\n",
    "    #print('shape after ReLU: ' + str(h.shape))\n",
    "    pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, pad_length, 1, 1],\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    "    #print('shape after max pooling: ' + str(pooled.shape))\n",
    "    pool_flat = tf.reshape(pooled, [-1, out_nodes])\n",
    "    #print(\"shape after flattening:\" + str(pool_flat.shape))\n",
    "\n",
    "    #h_drop = tf.nn.dropout(pool_flat, dropout_keep_prob)\n",
    "    #print('shape after dropout: ' + str(h_drop.shape))\n",
    "    return pool_flat\n",
    "    \n",
    "    \n",
    "def dense_NN(x,out_len, name_w, name_b):\n",
    "\n",
    "    tot_nodes = x.shape[1]\n",
    "    W_dense = tf.Variable(tf.truncated_normal([int(tot_nodes) , out_len], stddev=0.1), name=name_w) #\"W2\"\n",
    "    b_dense = tf.Variable(tf.constant(0.1, shape=[out_len]), name=name_b) # \"b2\"\n",
    "    return tf.matmul(x,W_dense) + b_dense \n",
    "\n",
    "def dropout_layer(layer, dropout_keep_prob):\n",
    "    return tf.nn.dropout(layer, dropout_keep_prob)\n",
    "\n",
    "def relu_layer(layer):\n",
    "    return tf.nn.relu(layer)\n",
    "\n",
    "def residual_block(layer, out_len):\n",
    "    shortcut = layer\n",
    "    \n",
    "    layer_ = dense_NN(layer, out_len, 'res_w', 'res_b')\n",
    "    layer_ = relu_layer(layer_)\n",
    "    layer_ = dense_NN(layer_, out_len, 'res_w2', 'res_b2')\n",
    "    layer_ = relu_layer(layer_)\n",
    "    \n",
    "    shortcut = dense_NN(shortcut, out_len, 'shortcut_w','shortcut_b')\n",
    "    \n",
    "    added_ = tf.add(layer_, shortcut)\n",
    "    added_ = relu_layer(added_)\n",
    "    return added_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## make one hot vector for price_bins\n",
    "onehot_pricebins = np.zeros((train_raw.shape[0],np.max(train_raw.price_bins.values)))\n",
    "onehot_pricebins[np.arange(train_raw.shape[0]),train_raw.price_bins.values-1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting labels and features...\n",
      "making tensor slices...\n",
      "shuffling...\n",
      "making epochs...\n",
      "making batches...\n",
      "setting up input took 0.5037775039672852 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "perc_split = .85 # use 30% as validation\n",
    "split_ind = round(train_raw.shape[0]*perc_split)\n",
    "\n",
    "# training set inputs\n",
    "input_name = name_padded\n",
    "input_itemdesc = itemdesc_padded\n",
    "#input_price = train_raw['price'].values.reshape((-1,1))\n",
    "input_price = onehot_pricebins\n",
    "input_brand = train_raw.brand_name_inds.values.reshape((-1,1))\n",
    "input_cat1 = train_raw.cat1_inds.values.reshape((-1,1))\n",
    "input_cat2 = train_raw.cat2_inds.values.reshape((-1,1))\n",
    "input_cat3 = train_raw.cat3_inds.values.reshape((-1,1))\n",
    "input_itemcond = train_raw.item_condition_id.values.reshape((-1,1))\n",
    "input_ship = train_raw.shipping.values.reshape((-1,1))\n",
    "\n",
    "# define some lengths for partitioning data after feeding\n",
    "input_name_len = input_name.shape[1]\n",
    "input_itemdesc_len = input_itemdesc.shape[1]\n",
    "\n",
    "\n",
    "# concatenate data to make into tensor slices\n",
    "temp_set = np.concatenate((input_name, input_itemdesc,input_cat1,input_cat2,input_cat3,\n",
    "                           input_brand, input_itemcond, input_ship),axis = 1) #name_and_desc ,input_itemcond,input_shipping\n",
    "\n",
    "#split the data into train and validation sets\n",
    "train_temp_set = temp_set[:split_ind,]\n",
    "train_price = input_price[:split_ind,].astype(np.float32)\n",
    "val_temp_set = temp_set[split_ind:,]\n",
    "val_price = input_price[split_ind:,].astype(np.float32)\n",
    "\n",
    "shape_set = train_temp_set.shape[1] \n",
    "\n",
    "# define some training step stuff here\n",
    "batch_len = 1000\n",
    "num_epoch = 25\n",
    "tot_iter = train_raw.shape[0]* num_epoch // batch_len + 1 # might not use this \n",
    "\n",
    "\n",
    "print('splitting labels and features...')\n",
    "features_input = train_temp_set.astype(np.int32)\n",
    "label_input = train_price\n",
    "# make some placeholders to avoid GraphDef exceeding 2GB\n",
    "feat_placeholder = tf.placeholder(features_input.dtype, features_input.shape)\n",
    "label_placeholder = tf.placeholder(label_input.dtype, label_input.shape)\n",
    "print('making tensor slices...')\n",
    "dataset = tf.data.Dataset.from_tensor_slices((feat_placeholder, label_placeholder))\n",
    "print('shuffling...')\n",
    "#np.random.shuffle(temp_set) # shuffle the data\n",
    "dataset = dataset.shuffle(buffer_size =10000)\n",
    "print('making epochs...')\n",
    "dataset = dataset.repeat() # epoch\n",
    "print('making batches...')\n",
    "dataset = dataset.batch(batch_len) \n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_batch = iterator.get_next()\n",
    "\n",
    "end_time = time.time()\n",
    "print('setting up input took ' + str(end_time - start_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding step took 0.04414200782775879 seconds.\n"
     ]
    }
   ],
   "source": [
    "## initialize embedding layers for training step\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "name_emb_size = 25\n",
    "itemdesc_emb_size = 25\n",
    "brand_emb_size = 15\n",
    "cat1_emb_size = 15\n",
    "cat2_emb_size = 10\n",
    "cat3_emb_size = 10\n",
    "itemcond_emb_size = 5\n",
    "shipping_emb_size = 5\n",
    "\n",
    "# lengths needed here and a bit later\n",
    "itemcond_len = np.max(train_raw.item_condition_id.values)\n",
    "\n",
    "name_itemdesc_emb = embed([i for i in range(vocabulary_size)],vocabulary_size,name_emb_size, name= 'name_itemdesc_emb')\n",
    "brand_emb = embed([i for i in range(dict_brand_len)],dict_brand_len, brand_emb_size, name= 'brand_emb')\n",
    "cat1_emb = embed([i for i in range(dict_cat1_len)],dict_cat1_len,cat1_emb_size, name= 'cat1_emb')\n",
    "cat2_emb = embed([i for i in range(dict_cat2_len)],dict_cat2_len,cat2_emb_size, name= 'cat2_emb')\n",
    "cat3_emb = embed([i for i in range(dict_cat3_len)],dict_cat3_len,cat3_emb_size, name= 'cat3_emb')\n",
    "itemcond_emb = embed([i for i in range(itemcond_len)],itemcond_len ,itemcond_emb_size, name= 'itemcond_emb')\n",
    "shipping_emb = embed([i for i in range(2)], 2, shipping_emb_size, name= 'shipping_emb')\n",
    "\n",
    "end_time = time.time()\n",
    "print('embedding step took ' + str(end_time - start_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tensorflow running block (is this __main__?)\n",
    "\n",
    "input_x = tf.placeholder(tf.int32,[None, shape_set], name = \"input_x\") # pad_length = 25 or something defined earlier\n",
    "input_y = tf.placeholder(tf.float32,[None,input_price.shape[1]], name = \"input_y\") # train agianst this\n",
    "\n",
    "input_x_name = input_x[:,:input_name_len]\n",
    "input_x_itemdesc = input_x[:,input_name_len:(input_name_len + input_itemdesc_len)]\n",
    "input_x_cat1 = input_x[:,(input_name_len + input_itemdesc_len)]\n",
    "input_x_cat2 = input_x[:,(input_name_len + input_itemdesc_len)+1]\n",
    "input_x_cat3 = input_x[:,(input_name_len + input_itemdesc_len)+2]\n",
    "input_x_brand = input_x[:,(input_name_len + input_itemdesc_len)+3]\n",
    "input_x_itemcond = input_x[:,(input_name_len + input_itemdesc_len)+4]\n",
    "input_x_shipping = input_x[:,(input_name_len + input_itemdesc_len)+5]\n",
    "\n",
    "\n",
    "name_emb_lookup = tf.nn.embedding_lookup(name_itemdesc_emb, input_x_name)\n",
    "itemdesc_emb_lookup = tf.nn.embedding_lookup(name_itemdesc_emb,input_x_itemdesc)\n",
    "brand_emb_lookup = tf.nn.embedding_lookup(brand_emb,input_x_brand)\n",
    "cat1_emb_lookup = tf.nn.embedding_lookup(cat1_emb,input_x_cat1)\n",
    "cat2_emb_lookup = tf.nn.embedding_lookup(cat2_emb,input_x_cat2)\n",
    "cat3_emb_lookup = tf.nn.embedding_lookup(cat3_emb,input_x_cat3)\n",
    "itemcond_emb_lookup = tf.nn.embedding_lookup(itemcond_emb, input_x_itemcond)\n",
    "shipping_emb_lookup = tf.nn.embedding_lookup(shipping_emb, input_x_shipping)\n",
    "\n",
    "# expand name and item_desc because conv2d wants it 4-d\n",
    "name_emb_lookup_expand = tf.expand_dims(name_emb_lookup,-1)\n",
    "itemdesc_emb_lookup_expand = tf.expand_dims(itemdesc_emb_lookup,-1)\n",
    "\n",
    "# set some lazy parameters here\n",
    "learn_rate = .01\n",
    "out_nodes = 15\n",
    "dropout_keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "W_shape_name = [1,name_emb_size,1,out_nodes] #figure this out if it works\n",
    "b_shape_name = out_nodes # same as last dimension in W\n",
    "\n",
    "W_shape_itemdesc = [1,itemdesc_emb_size,1,out_nodes]\n",
    "b_shape_itemdesc = out_nodes\n",
    "\n",
    "layers_name = CNN(name_emb_lookup_expand,W_shape_name,b_shape_name,name_pad_size,\"W_name\", \"b_name\")\n",
    "layers_itemdesc = CNN(itemdesc_emb_lookup_expand,W_shape_itemdesc,b_shape_itemdesc,itemdesc_pad_size,\"W_itemdesc\",\"b_itemdesc\")\n",
    "\n",
    "#combine the layers\n",
    "comb_layers = tf.concat([layers_name,layers_itemdesc, brand_emb_lookup, cat1_emb_lookup,\n",
    "                         cat2_emb_lookup, cat3_emb_lookup, itemcond_emb_lookup, shipping_emb_lookup],axis=1)\n",
    "\n",
    "#dense \n",
    "dense1 = dense_NN(comb_layers, 64, \"W_1\",\"b_1\")\n",
    "dense1 = dropout_layer(dense1, dropout_keep_prob)\n",
    "\n",
    "dense2 = dense_NN(dense1, 128, \"W_2\",\"b_2\")\n",
    "dense2 = dropout_layer(dense2, dropout_keep_prob)\n",
    "\n",
    "\n",
    "'''# resnet\n",
    "\n",
    "resnet1 = residual_block(comb_layers, 64)\n",
    "resnet2 = residual_block(resnet1, 128)\n",
    "#resnet3 = residual_block(resnet2, 256)\n",
    "'''\n",
    "#predictions = dense_NN(dense2, onehot_pricebins.shape[1], \"W_pred\", \"b_pred\")\n",
    "predictions = dense_NN(dense2, onehot_pricebins.shape[1], \"W_pred\", \"b_pred\")\n",
    "predictions = tf.nn.softmax(predictions)\n",
    "\n",
    "#loss_ = tf.sqrt(tf.losses.mean_squared_error(input_y, predictions))\n",
    "loss_ = tf.reduce_mean(-tf.reduce_sum(input_y * tf.log(predictions), reduction_indices=[1])) # cross entropy\n",
    "train_step  = tf.train.AdamOptimizer(learning_rate = learn_rate).minimize(loss_)\n",
    "# as is, normalized predictions cause NaN in rmsle solving. adding .00001 just in case\n",
    "#unwind_true = tf.expm1((input_y* std_price_norm) + mean_price_norm)+ .00001\n",
    "#unwind_pred = tf.expm1((predictions* std_price_norm) + mean_price_norm)+ .00001\n",
    "#rmsle_ = tf.sqrt(tf.reduce_mean(tf.square(tf.log(unwind_true) - tf.log(unwind_pred))))\n",
    "argmax_pred = tf.argmax(predictions,1)\n",
    "correct_prediction = tf.equal(argmax_pred, tf.argmax(input_y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define savedmodel here\n",
    "\n",
    "export_dir = '/home/bsong/Python_Stuff/Scripts/model_save/'\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 100: 0.599\n",
      "resetting counter after 99 steps\n",
      "One hundred steps took 2.088 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 200: 0.612\n",
      "resetting counter after 100 steps\n",
      "One hundred steps took 1.449 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 300: 0.613\n",
      "resetting counter after 100 steps\n",
      "One hundred steps took 1.463 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 400: 0.608\n",
      "One hundred steps took 1.056 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 500: 0.614\n",
      "resetting counter after 200 steps\n",
      "One hundred steps took 1.461 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 600: 0.616\n",
      "resetting counter after 100 steps\n",
      "One hundred steps took 1.468 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 700: 0.617\n",
      "resetting counter after 100 steps\n",
      "One hundred steps took 1.471 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 800: 0.621\n",
      "resetting counter after 100 steps\n",
      "One hundred steps took 1.472 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 900: 0.618\n",
      "One hundred steps took 1.040 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 1000: 0.623\n",
      "resetting counter after 200 steps\n",
      "One hundred steps took 1.460 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 1100: 0.617\n",
      "One hundred steps took 1.030 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 1200: 0.611\n",
      "One hundred steps took 1.044 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 1300: 0.617\n",
      "One hundred steps took 1.017 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 1400: 0.624\n",
      "resetting counter after 400 steps\n",
      "One hundred steps took 1.458 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 1500: 0.618\n",
      "One hundred steps took 1.023 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 1600: 0.616\n",
      "One hundred steps took 1.036 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 1700: 0.624\n",
      "resetting counter after 300 steps\n",
      "One hundred steps took 1.455 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 1800: 0.623\n",
      "One hundred steps took 1.042 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 1900: 0.627\n",
      "resetting counter after 200 steps\n",
      "One hundred steps took 1.467 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 2000: 0.625\n",
      "One hundred steps took 1.029 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 2100: 0.626\n",
      "One hundred steps took 1.092 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 2200: 0.623\n",
      "One hundred steps took 1.059 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 2300: 0.621\n",
      "One hundred steps took 1.030 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 2400: 0.623\n",
      "One hundred steps took 1.039 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 2500: 0.629\n",
      "resetting counter after 600 steps\n",
      "One hundred steps took 1.460 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 2600: 0.626\n",
      "One hundred steps took 1.026 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 2700: 0.628\n",
      "One hundred steps took 1.029 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 2800: 0.619\n",
      "One hundred steps took 1.028 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 2900: 0.625\n",
      "One hundred steps took 1.024 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 3000: 0.626\n",
      "One hundred steps took 1.038 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 3100: 0.624\n",
      "One hundred steps took 1.045 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 3200: 0.624\n",
      "One hundred steps took 1.034 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 3300: 0.617\n",
      "One hundred steps took 1.014 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 3400: 0.623\n",
      "One hundred steps took 1.035 seconds.\n",
      " \n",
      "calculating accuracy\n",
      "Accuracy of validation set at step 3500: 0.625\n",
      "One hundred steps took 1.060 seconds.\n",
      " \n",
      "Best validation accuracy is: 0.628523\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('Start training...')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "learn_rate = 1e-4\n",
    "counter = 0 \n",
    "i = 1\n",
    "best_acc = 0 # just has to be greater than 2 really\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer, {feat_placeholder: features_input, label_placeholder: label_input})\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)  \n",
    "    while counter <= 1001:\n",
    "        features_, label_ = sess.run(next_batch)\n",
    "        sess.run(train_step,{input_x: features_, input_y: label_, dropout_keep_prob:.7})\n",
    "        if i % 100 == 0:\n",
    "            print('calculating accuracy')\n",
    "            acc_val, pred_val = sess.run([accuracy,argmax_pred], {input_x: val_temp_set, input_y: val_price, dropout_keep_prob:1})\n",
    "            print('Accuracy of validation set at step %1d: %5.3f' % (i, acc_val))\n",
    "            if acc_val > best_acc:\n",
    "                best_acc = acc_val\n",
    "                saver.save(sess,save_path=export_dir)\n",
    "                best_confmat = confusion_matrix(pred_val, np.argmax(val_price,1))\n",
    "                #all_confmat += best_confmat\n",
    "                print('resetting counter after ' + str(counter) + ' steps')\n",
    "                counter = 0\n",
    "                \n",
    "            tot_time = time.time() - start_time\n",
    "            print('One hundred steps took %5.3f seconds.' % tot_time)\n",
    "            print(' ')\n",
    "            start_time = time.time()\n",
    "        counter += 1\n",
    "        i += 1 \n",
    "        if i % 500 == 0: \n",
    "            learn_rate = learn_rate/10\n",
    "print('Best validation accuracy is: ' + str(best_acc))\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAFKCAYAAAAADdTJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa616860f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum_rows = np.sum(best_confmat,axis=1)\n",
    "nonzero_axis = [i for i in range(len(sum_rows)) if sum_rows[i] !=0]\n",
    "scaled_confmat = best_confmat[nonzero_axis] / sum_rows[nonzero_axis,None]\n",
    "\n",
    "plt.matshow(scaled_confmat)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(total_bins)\n",
    "plt.xticks(tick_marks, range(total_bins))\n",
    "plt.yticks(tick_marks,nonzero_axis)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Setup test set\n",
    "\n",
    "input_name_test = name_padded_test\n",
    "input_itemdesc_test = itemdesc_padded_test\n",
    "input_brand_test = test_raw.brand_name_inds.values.reshape((-1,1))\n",
    "input_cat1_test = test_raw.cat1_inds.values.reshape((-1,1))\n",
    "input_cat2_test = test_raw.cat2_inds.values.reshape((-1,1))\n",
    "input_cat3_test = test_raw.cat3_inds.values.reshape((-1,1))\n",
    "input_itemcond_test = test_raw.item_condition_id.values.reshape((-1,1))\n",
    "input_ship_test = test_raw.shipping.values.reshape((-1,1))\n",
    "\n",
    "temp_set_test = np.concatenate((input_name_test, input_itemdesc_test,input_cat1_test,input_cat2_test,input_cat3_test,\n",
    "                                input_brand_test, input_itemcond_test, \n",
    "                                input_ship_test),axis = 1) #name_and_desc ,input_itemcond,input_shipping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test set too fat to just plug. consider batches\n",
    "features_input_test = temp_set_test.astype(np.int32)\n",
    "\n",
    "feat_placeholder_test = tf.placeholder(features_input_test.dtype, features_input_test.shape)\n",
    "print('making tensor slices...')\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices(feat_placeholder_test)\n",
    "print('making batches...')\n",
    "dataset_test = dataset_test.batch(10000) \n",
    "iterator_test = dataset_test.make_initializable_iterator()\n",
    "next_batch_test = iterator_test.get_next()\n",
    "\n",
    "tot_runs = np.ceil(features_input_test.shape[0]/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## run on test set\n",
    "with tf.Session() as sess:\n",
    "    all_preds = []\n",
    "    sess.run(iterator_test.initializer, {feat_placeholder_test: features_input_test}) \n",
    "    saver.restore(sess=sess, save_path=export_dir)\n",
    "    for _ in range(int(tot_runs)):\n",
    "        feed_x = sess.run(next_batch_test)\n",
    "        predictions_final = sess.run(predictions, {input_x: feed_x, dropout_keep_prob: 1})\n",
    "        all_preds.extend(predictions_final)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array(all_preds)\n",
    "print(a.shape)\n",
    "b = np.argmax(a, axis = 1)\n",
    "\n",
    "c = [sum(b==i) for i in set(b)]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set(b)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
