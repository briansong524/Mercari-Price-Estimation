{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsong/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Update from v02:\n",
    "-use variable_scopes to define graph more specifically\n",
    "-optimize the text preprocessing step\n",
    "-implement rnn\n",
    "'''\n",
    "\n",
    "## import packages\n",
    "import time\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import random\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import data took 8.957505464553833 seconds.\n"
     ]
    }
   ],
   "source": [
    "## import data \n",
    "start_time = time.time()\n",
    "\n",
    "train_raw = pd.read_csv('/home/bsong/Python_Stuff/Data/Kaggle_Mercari/train.tsv',delimiter= '\\t')\n",
    "#train_raw = train_raw.iloc[0:10000,] # just a bit\n",
    "# standardize price here because may as well\n",
    "normalized_price = np.log1p(train_raw['price'].values)\n",
    "mean_price_norm = np.mean(normalized_price)\n",
    "std_price_norm = np.std(normalized_price) \n",
    "train_raw['price'] = (normalized_price - mean_price_norm)/std_price_norm \n",
    "\n",
    "end_time = time.time()\n",
    "print('import data took ' + str(end_time - start_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define functions to use\n",
    "\n",
    "######## Basic text manipulation functions (some specific to Mercari Kaggle Competition) \n",
    "\n",
    "def split_cat(text): # this one is to reduce the categoriy_name into three subcategories\n",
    "    try: return text.split(\"/\")\n",
    "    except: return (\"No Label\", \"No Label\", \"No Label\")\n",
    "\n",
    "def handle_missing_inplace(dataset):  # this one is to put placeholders in place of missing values (NaN)\n",
    "    dataset['cat1'].fillna(value='No Label', inplace=True)\n",
    "    dataset['cat2'].fillna(value='No Label', inplace=True)\n",
    "    dataset['cat3'].fillna(value='No Label', inplace=True)\n",
    "    dataset['brand_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['item_description'].fillna(value='No description yet', inplace=True)\n",
    "     \n",
    "def build_dictionary(words, n_words): # dictionary that maps words to indices. this function should be modular.\n",
    "    #input is [['a','b','c'],['a','b','c']]\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]] # word indexed as \"unknown\" if not one of the top #n_words (popular/common) words (-1 is filler #)\n",
    "    count.extend(Counter(words).most_common(n_words - 1)) # most_common returns the top (n_words-1) ['word',count]\n",
    "    dictionary = dict()\n",
    "    for word, _ in count: # the 'word, _' is writted because count is a list of list(2), so defining 'word' as the first term per\n",
    "        dictionary[word] = len(dictionary) # {'word': some number incrementing by one. fyi, no repeats because from most_common)}\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys())) # {ind. : 'word'} I guess for looking up if needed?\n",
    "    return dictionary, reversed_dictionary\n",
    "\n",
    "def clean_and_tokenize(dataset_col): # input is a column of strings\n",
    "    pattern = '[A-Za-z]+' # does this only keep words\n",
    "    pattern2 = '[!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n]' # to rid of special characters\n",
    "    list_of_lists = list()\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for word in dataset_col:\n",
    "        list_of_words = list()\n",
    "        word = re.sub(pattern2, r'', word)\n",
    "        tokenized = tokenizer.tokenize(word)\n",
    "        tokenized_filtered = filter(lambda token: token not in stop_words, tokenized)\n",
    "        for i in tokenized_filtered:\n",
    "            if (len(i) > 2 ): #ignore words of length 2 or less\n",
    "                list_of_words.append(i.lower()) # append all words to one list\n",
    "        list_of_lists.append(list_of_words)\n",
    "    list_as_series = pd.Series(list_of_lists)\n",
    "    return list_as_series\n",
    "\n",
    "def just_punc_and_lower(dataset_col):\n",
    "    pattern = '[A-Za-z]+' # does this only keep words\n",
    "    pattern2 = '[!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n]' # to rid of special characters\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    list_of_lists = list()\n",
    "    for word in dataset_col:\n",
    "        word = re.sub(pattern2,r'',word)\n",
    "        list_of_words = word.lower().split(' ')\n",
    "        list_of_lists.append(list_of_words)\n",
    "    list_as_series = pd.Series(list_of_lists)\n",
    "    return list_as_series\n",
    "\n",
    "def convert_word_to_ind(dataset_col,dictionary): # input the pandas column of texts and dictionary. This should be modular\n",
    "    # each input should be a string of cleaned words tokenized into a list (ex. ['this', 'is', 'an', 'item'])\n",
    "    # dictionary should be the dictionary obtained from build_dictionary\n",
    "    list_of_lists = []\n",
    "    unk_count = 0 # total 'unknown' words counted\n",
    "    for word_or_words in dataset_col: # words is the list of all words\n",
    "        list_of_inds = []\n",
    "        for word in word_or_words:\n",
    "            if word in dictionary:\n",
    "                index = np.int(dictionary[word]) # dictionary contains top words, so if in, it gets an index\n",
    "            else:\n",
    "                index = 0  #  or dictionary['UNK']? can figure out later\n",
    "                unk_count += 1\n",
    "            list_of_inds.append(index)\n",
    "        list_of_lists.append(list_of_inds)\n",
    "\n",
    "    # make list_of_lists into something that can be put into pd.DataFrame\n",
    "    #list_as_series = pd.Series(list_of_lists)\n",
    "    list_as_series = np.array(list_of_lists)\n",
    "    return list_as_series, unk_count\n",
    "\n",
    "def pad_word_indices(col_of_indices, pad_length): # col_of_indices can be a pd series. \n",
    "    temp_series = [] # append vectors into here\n",
    "    for list_inds in col_of_indices:\n",
    "        len_list = len(list_inds)\n",
    "        if len_list >= pad_length:\n",
    "            temp_series.append(np.array(list_inds[(len_list-pad_length):]))\n",
    "        else:\n",
    "            padded_vec = [0]*(pad_length-len_list)\n",
    "            padded_vec.extend(list_inds)\n",
    "            temp_series.append(np.array(padded_vec))\n",
    "    return temp_series\n",
    "\n",
    "def convert_word_to_padded(dataset_col,dictionary,pad_length): # input the pandas column of texts and dictionary. This should be modular\n",
    "    # each input should be a string of cleaned words tokenized into a list (ex. ['this', 'is', 'an', 'item'])\n",
    "    # dictionary should be the dictionary obtained from build_dictionary\n",
    "    # use this function when you know how long you want your pad_length\n",
    "    #   - otherwise, use convert_word_to_ind, and pad_word_indices\n",
    "    #   - eventually, will look into cleaning these three functions up.\n",
    "    list_of_lists = []\n",
    "    unk_count = 0 # total 'unknown' words counted\n",
    "    for word_or_words in dataset_col: # words is the list of all words\n",
    "        list_of_inds = []\n",
    "        count_inds = 0\n",
    "        for word in word_or_words:\n",
    "            if word in dictionary:\n",
    "                index = np.int(dictionary[word]) # dictionary contains top words, so if in, it gets an index\n",
    "            else:\n",
    "                index = 0  #  or dictionary['UNK']? can figure out later\n",
    "                unk_count += 1\n",
    "            count_inds +=1\n",
    "            list_of_inds.append(index) \n",
    "        if count_inds >= pad_length:\n",
    "            asdf = list_of_inds[(count_inds-pad_length):]\n",
    "        else: \n",
    "            asdf = [0]*(pad_length-count_inds)\n",
    "            asdf.extend(list_of_inds)\n",
    "        temp = np.array(asdf)\n",
    "        list_of_lists.append(temp)\n",
    "    list_as_series = np.array(list_of_lists)\n",
    "    return list_as_series, unk_count\n",
    "\n",
    "######## Word Embedding (this is after strings are transformed into vectors of indices)\n",
    "\n",
    "# generate batch data (for feeding into word embedding)\n",
    "# used http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/ for reference\n",
    "def generate_batch(data, batch_size, num_skips): \n",
    "    # data should be [[3,7,9],[7,4,5,9],...] kinda format\n",
    "    # num_skips configures number of context words to draw. skip_window defines size of window to draw context words from\n",
    "    assert batch_size % num_skips == 0 # if batch_size was 10, and num_skips was 3, then [cat,cat,cat,sat,sat,sat,...] wont equal\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32) # initialize batch variable (input word go in here)\n",
    "    context = np.ndarray(shape=(batch_size, 1), dtype=np.int32) # initialize context variable\n",
    "    counter = 0\n",
    "    rand_dat_ind = random.sample(range(0,len(data)-1),int(batch_size/num_skips))\n",
    "    for i in data[rand_dat_ind]:\n",
    "        while len(i) <= num_skips:\n",
    "            rnd_again = random.randint(0,len(data)-1)\n",
    "            i = data[rnd_again]\n",
    "        target = random.randint(0,len(i)-1) \n",
    "        targets_to_avoid = [target] # avoid this index when selecting rando words\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid: # this is to choose an index that isnt the index of the batch word\n",
    "                target = random.randint(0, len(i)-1) # target is a context word\n",
    "            targets_to_avoid.append(target) # so next time, this loop won't select this context word again \n",
    "            batch[counter] = i[targets_to_avoid[0]]  # this is the input word (same word repeated i*num_skips+j times)\n",
    "            context[counter, 0] = i[targets_to_avoid[j+1]]  # these are the context words to the batch word\n",
    "            counter += 1\n",
    "    return batch, context # batch is input, context is target variable(s)\n",
    "\n",
    "def generate_batch_general(x, y, batch_size):\n",
    "    # this is to generate batches for word2vec comparing against numeric values \n",
    "    # in this case, 'brand_name' and cat1/2/3 are compared against 'price'\n",
    "    rand_dat_ind = random.sample(range(0,len(data)-1),int(batch_size))\n",
    "    return x[rand_dat_ind], y[rand_dat_ind]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning \"category_name\" and making one-worded features to indices took 8.736202001571655 seconds.\n"
     ]
    }
   ],
   "source": [
    "## clean \"category_name\" and make numeric indicies for one-worded features (brand_name, cat1/2/3)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_raw['cat1'],train_raw['cat2'],train_raw['cat3'] = \\\n",
    "zip(*train_raw['category_name'].apply(lambda x: split_cat(x))) # split the categories into three new columns\n",
    "train_raw.drop('category_name',axis = 1, inplace = True) # remove the column that isn't needed anymore\n",
    "\n",
    "handle_missing_inplace(train_raw) # replaces NaN with a string placeholder 'missing'\n",
    "\n",
    "end_time = time.time()\n",
    "print('cleaning \"category_name\" and making one-worded features to indices took ' + str(end_time - start_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06178545951843262\n",
      "total words (with repeats): 30278275\n",
      "total unassigned words in name and item_description: 381710 1133021\n",
      "converting name and item_desc to indices and config took 184.25964546203613 seconds.\n"
     ]
    }
   ],
   "source": [
    "## convert name and item_desc to indices, then configure a bit more\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "all_name_desc = np.hstack((train_raw['name'],train_raw['item_description'])) # get all dem words\n",
    "all_name_desc = clean_and_tokenize(all_name_desc)\n",
    "all_name_desc = [item for sublist in all_name_desc for item in sublist]\n",
    "train_raw['name'] = clean_and_tokenize(train_raw['name'])\n",
    "train_raw['item_description'] = clean_and_tokenize(train_raw['item_description'])\n",
    "\n",
    "#make new columns of just the indices of the words for name and item_description\n",
    "vocabulary_size = 9803 # keeping 100000 words in the dictionary. can adjust later. will use variable elsewhere\n",
    "word2vec_dict, reverse_dict = build_dictionary(all_name_desc,vocabulary_size) \n",
    "train_raw['name_inds'], count_unk_name = convert_word_to_ind(train_raw['name'],word2vec_dict) \n",
    "train_raw['item_desc_inds'], count_unk_item_desc = convert_word_to_ind(train_raw['item_description'], word2vec_dict)  \n",
    "\n",
    "print(\"total words (with repeats): \" + str(len(all_name_desc)))\n",
    "print(\"total unassigned words in name and item_description: \"+ str(count_unk_name) +  ' ' + str(count_unk_item_desc))\n",
    "\n",
    "end_time = time.time()\n",
    "print('converting name and item_desc to indices and config took ' + str(end_time - start_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max tokens in 'name': 9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0JAkdTM0JEnd/h/KNB4mLJbuDwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efe8568ccc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max tokens in 'item_desc': 156\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "## deciding pad length for name and item_desc  \n",
    "\n",
    "a = [len(i) for i in train_raw.name_inds]\n",
    "print(\"max tokens in 'name': \" + str(max(a)))\n",
    "\n",
    "b = [len(i) for i in train_raw.item_desc_inds]\n",
    "plt.hist(b,20)\n",
    "plt.show()\n",
    "print(\"max tokens in 'item_desc': \" + str(max(b)))\n",
    "\n",
    "sort_b = sorted(b) #sorted length in increasing order\n",
    "perc_data = .95\n",
    "len_item_desc_potential = sort_b[round(perc_data*len(sort_b))]\n",
    "print(len_item_desc_potential) # this represents (perc_data)% of item descriptions are under (len_item_desc_potential) words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique brands: 4810\n",
      "Number of unique categories in cat1: 11\n",
      "Number of unique categories in cat1: 114\n",
      "Number of unique categories in cat1: 871\n"
     ]
    }
   ],
   "source": [
    "## deciding dictionary size for brand and cat1/2/3\n",
    "\n",
    "brand_unique_set = set(train_raw.brand_name.values)\n",
    "print('Number of unique brands: ' + str(len(brand_unique_set)))\n",
    "\n",
    "cat1_set = set(train_raw.cat1.values)\n",
    "print('Number of unique categories in cat1: ' + str(len(cat1_set)))\n",
    "\n",
    "cat2_set = set(train_raw.cat2.values)\n",
    "print('Number of unique categories in cat1: ' + str(len(cat2_set)))\n",
    "\n",
    "cat3_set = set(train_raw.cat3.values)\n",
    "print('Number of unique categories in cat1: ' + str(len(cat3_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0\n",
      "making dictionaries for brand and categories took 29.997793197631836 seconds.\n"
     ]
    }
   ],
   "source": [
    "## make dictionaries for brand_name and cat1/2/3\n",
    "\n",
    "start_time = time.time()\n",
    "# define dictionary lengths here\n",
    "dict_brand_len = 4811 # total words + 1 for \"UNK\" is minimal size\n",
    "dict_cat1_len = 12 \n",
    "dict_cat2_len= 115 \n",
    "dict_cat3_len = 872 \n",
    "\n",
    "brand_name_dict, brand_name_dict_rev = build_dictionary(train_raw['brand_name'], dict_brand_len)\n",
    "train_raw['brand_name_inds'], count_unk_brand = convert_word_to_ind(train_raw['brand_name'].values.reshape((-1,1)), brand_name_dict)\n",
    "cat1_dict ,cat1_rev_dict= build_dictionary(train_raw['cat1'],dict_cat1_len)\n",
    "train_raw['cat1_inds'], count_unk_cat1 = convert_word_to_ind(train_raw['cat1'].values.reshape((-1,1)), cat1_dict)\n",
    "cat2_dict ,cat2_rev_dict= build_dictionary(train_raw['cat2'],dict_cat2_len)\n",
    "train_raw['cat2_inds'], count_unk_cat2 = convert_word_to_ind(train_raw['cat2'].values.reshape((-1,1)), cat2_dict)\n",
    "cat3_dict ,cat3_rev_dict= build_dictionary(train_raw['cat3'],dict_cat3_len)\n",
    "train_raw['cat3_inds'], count_unk_cat3 = convert_word_to_ind(train_raw['cat3'].values.reshape((-1,1)), cat3_dict)\n",
    "\n",
    "print(str(count_unk_brand) + ' ' + str(count_unk_cat1) + ' '+ str(count_unk_cat2) + \" \" + str(count_unk_cat3))\n",
    "\n",
    "end_time = time.time()\n",
    "print('making dictionaries for brand and categories took ' + str(end_time - start_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting name and item_desc to padded indices took 35.706812381744385 seconds.\n"
     ]
    }
   ],
   "source": [
    "## padding name and item_desc here. these will be trained in the final model (as opposed to pretrained)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "name_pad_size = 9 # max length of name\n",
    "itemdesc_pad_size = 25 # 95th percentile of length of item descriptions\n",
    "\n",
    "name_padded , _ = convert_word_to_padded(train_raw.name,word2vec_dict,name_pad_size) # without _, will get tuple lol.\n",
    "itemdesc_padded , _ = convert_word_to_padded(train_raw.item_description,word2vec_dict,itemdesc_pad_size) \n",
    "\n",
    "end_time = time.time()\n",
    "print('converting name and item_desc to padded indices took ' + str(end_time - start_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obtain_reasonable_vocab_size(list_words, perc_words = .95):\n",
    "    counter_ = Counter(list_words)\n",
    "    counts = [i for _,i in counter_.most_common()]\n",
    "    tot_words = len(list_words)\n",
    "    print('total words (with repeats): ' + str(tot_words))\n",
    "    tot_count = 0\n",
    "    runs = 0\n",
    "    while tot_count < round(perc_words*tot_words):\n",
    "        tot_count += counts[runs]\n",
    "        runs += 1\n",
    "    print('reasonable vocab size: ' + str(runs))\n",
    "\n",
    "def obtain_reasonable_pad_length(list_words, perc_words = 0.95):\n",
    "    len_list = [len(i) for i in list_words]\n",
    "    sort_list = sorted(len_list)\n",
    "    ind_good = sort_list[round(perc_words*len(sort_list))]\n",
    "    print('reasonable pad length: ' + str(ind_good))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"## tokenized name and item_description without removing works. only lowercase\\n\\ntrain_raw['name_token'] = just_punc_and_lower(train_raw.name)\\ntrain_raw['itemdesc_token'] = just_punc_and_lower(train_raw.item_description)\\n\\n## used below two blocks or so\\nall_name = [item for sublist in train_raw.name_token for item in sublist]\\nall_item_desc = [item for sublist in train_raw.itemdesc_token for item in sublist]\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''## tokenized name and item_description without removing works. only lowercase\n",
    "\n",
    "train_raw['name_token'] = just_punc_and_lower(train_raw.name)\n",
    "train_raw['itemdesc_token'] = just_punc_and_lower(train_raw.item_description)\n",
    "\n",
    "## used below two blocks or so\n",
    "all_name = [item for sublist in train_raw.name_token for item in sublist]\n",
    "all_item_desc = [item for sublist in train_raw.itemdesc_token for item in sublist]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Get an idea of what length of padding should be done\\n\\nlen_list = [len(i) for i in train_raw.name_token.values]\\n\\nplt.hist(len_list, bins = 10)\\nplt.show()\\nobtain_reasonable_pad_length(train_raw.name_token.values)\\n\\nlen_list = [len(i) for i in train_raw.itemdesc_token.values]\\nplt.hist(len_list, bins = 20)\\nplt.show()\\nobtain_reasonable_pad_length(train_raw.itemdesc_token.values)\\n\\n## Also obtain a decent dictionary size\\nobtain_reasonable_vocab_size(all_name)\\nobtain_reasonable_vocab_size(all_item_desc)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''## Get an idea of what length of padding should be done\n",
    "\n",
    "len_list = [len(i) for i in train_raw.name_token.values]\n",
    "\n",
    "plt.hist(len_list, bins = 10)\n",
    "plt.show()\n",
    "obtain_reasonable_pad_length(train_raw.name_token.values)\n",
    "\n",
    "len_list = [len(i) for i in train_raw.itemdesc_token.values]\n",
    "plt.hist(len_list, bins = 20)\n",
    "plt.show()\n",
    "obtain_reasonable_pad_length(train_raw.itemdesc_token.values)\n",
    "\n",
    "## Also obtain a decent dictionary size\n",
    "obtain_reasonable_vocab_size(all_name)\n",
    "obtain_reasonable_vocab_size(all_item_desc)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## Recurrent NN setup for name and item_description\\n\\ndict_name_len = 12000\\ndict_itemdesc_len = 8737\\nname_pad_len = 7\\nitemdesc_pad_len = 91\\n\\n# build dictionary for indices and use it to convert word to indices\\n# same time, the indices will be padded to maintain np.array dtype\\n\\nname_dict, name_dict_rev = build_dictionary(all_name, dict_name_len)\\nname_padded, _ = convert_word_to_padded(train_raw.name_token, name_dict, name_pad_len)\\n\\nitemdesc_dict, itemdesc_dict_rev = build_dictionary(all_item_desc, dict_itemdesc_len)\\nitemdesc_padded, _ = convert_word_to_padded(train_raw.itemdesc_token, itemdesc_dict, itemdesc_pad_len)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "## Recurrent NN setup for name and item_description\n",
    "\n",
    "dict_name_len = 12000\n",
    "dict_itemdesc_len = 8737\n",
    "name_pad_len = 7\n",
    "itemdesc_pad_len = 91\n",
    "\n",
    "# build dictionary for indices and use it to convert word to indices\n",
    "# same time, the indices will be padded to maintain np.array dtype\n",
    "\n",
    "name_dict, name_dict_rev = build_dictionary(all_name, dict_name_len)\n",
    "name_padded, _ = convert_word_to_padded(train_raw.name_token, name_dict, name_pad_len)\n",
    "\n",
    "itemdesc_dict, itemdesc_dict_rev = build_dictionary(all_item_desc, dict_itemdesc_len)\n",
    "itemdesc_padded, _ = convert_word_to_padded(train_raw.itemdesc_token, itemdesc_dict, itemdesc_pad_len)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## RNN specific stuff\\n\\ndef dense_rnn_layer(rnn_output, hidden_size, vocab_size, name_w, name_b):\\n    W = tf.Variable(tf.random_uniform([hidden_size, vocab_size], -1,1), name = name_w)\\n    b = tf.Variable(tf.random_uniform([vocab_size], -1,1), name = name_b)\\n    logits = tf.nn.xw_plus_b(rnn_output, W, b)\\n    return logits\\n\\n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## regular neural network function define here\n",
    "# This is to use for the simpler columns (brand_name, item_condition, cat1/2/3)\n",
    "# note to self: maybe separating dropout is better for manipulation purposes (and pooling and dropout lol.)\n",
    "\n",
    "# RegNN used for converting embedded features into whatever out_nodes. \n",
    "# I feel dense_NN achieves the exact same thing, but one layer, but this happened because I was iteratively progressing through this \n",
    "# project and didn't want to erase too many things. 1/11/18\n",
    "\n",
    "def RegNN(x, dropout_keep_prob, vocab_size, embed_size, batch_len, out_len):\n",
    "    #print('shape of input:' + str(x.shape))\n",
    "    # x should be of size [batch_len,embed_size] \n",
    "    # set up some weights/bias stuff\n",
    "    W1 = tf.Variable(tf.truncated_normal([vocab_size,embed_size], stddev=0.1))\n",
    "    b1 = tf.Variable(tf.constant(0.1, shape=[vocab_size,1])) # maybe batch_len   \n",
    "    #print('shape of W1:' + str(W1.shape))\n",
    "    #print('shape of b1:' + str(b1.shape))\n",
    "    \n",
    "    # xW + b \n",
    "    NN_layer = tf.matmul(W1,tf.transpose(x)) + b1 # this outputs shape (vocab_size,batch_len)\n",
    "    #print('NN_layer shape: ' + str(NN_layer.shape)) \n",
    "    # ReLU layer\n",
    "    \n",
    "    h = tf.nn.relu(NN_layer)\n",
    "    \n",
    "    # Drop Layer\n",
    "    h_drop = tf.nn.dropout(h, dropout_keep_prob) # still (vocab_size,batch_len)\n",
    "    \n",
    "    #W2 = tf.Variable(tf.truncated_normal([vocab_size,out_len]))\n",
    "    #b2 = tf.constant(0.1, shape=[batch_len,1])\n",
    "    \n",
    "    #NN_layer2 = tf.matmul(tf.transpose(h_drop),W2) + b2 # this outputs shape (batch_len,out_len)\n",
    "    #print('NN_layer2 shape: ' + str(NN_layer2.shape))\n",
    "    #h2 = tf.nn.relu(NN_layer2)\n",
    "    #h2_drop = tf.nn.dropout(h2, dropout_keep_prob) # should be of length (batch_len, out_len)\n",
    "    \n",
    "    return h_drop #h2_drop\n",
    "\n",
    "\n",
    "def embed(inputs, size, dim,name):\n",
    "    # inputs is a list of indices\n",
    "    # size is the number of unique indices (look for max index to achieve this if ordered)\n",
    "    # dim is the number of embedded numbers \n",
    "    std = np.sqrt(2 / dim)\n",
    "    emb = tf.Variable(tf.random_uniform([size, dim], -std, std), name = name)\n",
    "    return emb\n",
    "\n",
    "# test block for CNN \n",
    "# based on http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n",
    "# note to self: maybe separating dropout is better for manipulation purposes (and pooling and dropout lol.)\n",
    "\n",
    "def CNN(x,W_shape,b_shape,pad_length, name_w, name_b):\n",
    "    # x is the expanded lookup tables that will be trained\n",
    "    W1 = tf.Variable(tf.truncated_normal(W_shape, stddev=0.1), name= name_w) #\"W1\"\n",
    "    b1 = tf.Variable(tf.constant(0.1, shape=[b_shape]), name = name_b) # \"b1\"\n",
    "    conv = tf.nn.conv2d( #tf.layers.conv2d is also used, with  more parameters. Probably a slightly higher API because of that.\n",
    "        x,\n",
    "        W1,\n",
    "        strides = [1,1,1,1],\n",
    "        padding=\"VALID\",\n",
    "        name=\"conv\")\n",
    "    #print('shape of CNN output:' + str(conv.shape))\n",
    "    h = tf.nn.relu(tf.nn.bias_add(conv, b1), name=\"relu\")\n",
    "    #print('shape after ReLU: ' + str(h.shape))\n",
    "    pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, pad_length, 1, 1], # pad_length\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    "    #print('shape after max pooling: ' + str(pooled.shape))\n",
    "    pool_flat = tf.reshape(pooled, [-1, out_nodes])\n",
    "    #print(\"shape after flattening:\" + str(pool_flat.shape))\n",
    "\n",
    "    #h_drop = tf.nn.dropout(pool_flat, dropout_keep_prob)\n",
    "    #print('shape after dropout: ' + str(h_drop.shape))\n",
    "    return pool_flat\n",
    "    \n",
    "    \n",
    "def dense_NN(x,out_len, name_w, name_b):\n",
    "\n",
    "    tot_nodes = x.shape[1]\n",
    "    W_dense = tf.Variable(tf.truncated_normal([int(tot_nodes) , out_len], stddev=0.1), name=name_w) #\"W2\"\n",
    "    b_dense = tf.Variable(tf.constant(0.1, shape=[out_len]), name=name_b) # \"b2\"\n",
    "    return tf.matmul(x,W_dense) + b_dense \n",
    "\n",
    "def train_the_NN(outnode,true_val,loss_val):\n",
    "    loss_ = tf.sqrt(tf.losses.mean_squared_error(true_val, outnode))\n",
    "    if loss_val > .7:\n",
    "        train_step_ = tf.train.AdamOptimizer(learning_rate = .001).minimize(loss_)\n",
    "    else:\n",
    "        train_step_ = tf.train.AdamOptimizer(learning_rate = 1e-4).minimize(loss_)\n",
    "    return loss_, train_step_\n",
    "\n",
    "def dropout_layer(layer, dropout_keep_prob):\n",
    "    return tf.nn.dropout(layer, dropout_keep_prob)\n",
    "\n",
    "def relu_layer(layer):\n",
    "    return tf.nn.relu(layer)\n",
    "'''\n",
    "## RNN specific stuff\n",
    "\n",
    "def dense_rnn_layer(rnn_output, hidden_size, vocab_size, name_w, name_b):\n",
    "    W = tf.Variable(tf.random_uniform([hidden_size, vocab_size], -1,1), name = name_w)\n",
    "    b = tf.Variable(tf.random_uniform([vocab_size], -1,1), name = name_b)\n",
    "    logits = tf.nn.xw_plus_b(rnn_output, W, b)\n",
    "    return logits\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting labels and features...\n",
      "making tensor slices...\n",
      "shuffling...\n",
      "making epochs...\n",
      "making batches...\n",
      "setting up input took 0.8807697296142578 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# somewhat state which variables will be used here\n",
    "# reshaped to fit better (not sure if too necessary in hindsight, but minimal loss in time)\n",
    "input_name = name_padded\n",
    "input_itemdesc = itemdesc_padded\n",
    "input_price = train_raw['price'].values.reshape((-1,1))\n",
    "input_brand = train_raw.brand_name_inds.values.reshape((-1,1))\n",
    "input_cat1 = train_raw.cat1_inds.values.reshape((-1,1))\n",
    "input_cat2 = train_raw.cat2_inds.values.reshape((-1,1))\n",
    "input_cat3 = train_raw.cat3_inds.values.reshape((-1,1))\n",
    "input_itemcond = train_raw.item_condition_id.values.reshape((-1,1))\n",
    "input_ship = train_raw.shipping.values.reshape((-1,1))\n",
    "\n",
    "# define some lengths for partitioning data after feeding\n",
    "input_name_len = input_name.shape[1]\n",
    "input_itemdesc_len = input_itemdesc.shape[1]\n",
    "\n",
    "# concatenate data to make into tensor slices\n",
    "temp_set = np.concatenate((input_name, input_itemdesc,input_cat1,input_cat2,input_cat3,\n",
    "                           input_brand, input_itemcond, input_ship),axis = 1) #name_and_desc ,input_itemcond,input_shipping\n",
    "shape_set = temp_set.shape[1] \n",
    "\n",
    "#split the data into train and validation sets\n",
    "perc_split = .85 # use 30% as validation\n",
    "split_ind = round(train_raw.shape[0]*perc_split)\n",
    "\n",
    "train_temp_set = temp_set[:split_ind,]\n",
    "train_price = input_price[:split_ind,].astype(np.float32)\n",
    "val_temp_set = temp_set[split_ind:,]\n",
    "val_price = input_price[split_ind:,].astype(np.float32)\n",
    "\n",
    "batch_len = 10000\n",
    "\n",
    "tot_iter = train_raw.shape[0]* num_epoch // batch_len + 1\n",
    "\n",
    "# make iterator for train set\n",
    "print('splitting labels and features...')\n",
    "features_input = train_temp_set.astype(np.int32)\n",
    "label_input = train_price.astype(np.float32)\n",
    "# make some placeholders to avoid GraphDef exceeding 2GB\n",
    "feat_placeholder = tf.placeholder(features_input.dtype, features_input.shape)\n",
    "label_placeholder = tf.placeholder(label_input.dtype, label_input.shape)\n",
    "print('making tensor slices...')\n",
    "dataset = tf.data.Dataset.from_tensor_slices((feat_placeholder, label_placeholder))\n",
    "print('shuffling...')\n",
    "#np.random.shuffle(temp_set) # shuffle the data\n",
    "dataset = dataset.shuffle(buffer_size =10000)\n",
    "print('making epochs...')\n",
    "dataset = dataset.repeat() # epoch\n",
    "print('making batches...')\n",
    "dataset = dataset.batch(batch_len) \n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_batch = iterator.get_next()\n",
    "\n",
    "end_time = time.time()\n",
    "print('setting up input took ' + str(end_time - start_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding step took 0.0541989803314209 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "name_emb_size = 10\n",
    "itemdesc_emb_size = 10\n",
    "brand_emb_size = 50\n",
    "cat1_emb_size = 10\n",
    "cat2_emb_size = 20\n",
    "cat3_emb_size = 30\n",
    "itemcond_emb_size = 10\n",
    "shipping_emb_size = 10\n",
    "\n",
    "# lengths needed here and a bit later\n",
    "itemcond_len = np.max(train_raw.item_condition_id.values) + 1\n",
    "\n",
    "\n",
    "name_emb = embed(name_padded, vocabulary_size, name_emb_size, 'name_embedding_table')\n",
    "itemdesc_emb = embed(itemdesc_padded, vocabulary_size, itemdesc_emb_size, 'item_desc_embedding_table')\n",
    "brand_emb = embed(train_raw.brand_name_inds,dict_brand_len, brand_emb_size, name= 'brand_emb')\n",
    "cat1_emb = embed(train_raw.cat1_inds, dict_cat1_len, cat1_emb_size, name= 'cat1_emb')\n",
    "cat2_emb = embed(train_raw.cat2_inds, dict_cat2_len, cat2_emb_size, name= 'cat2_emb')\n",
    "cat3_emb = embed(train_raw.cat3_inds, dict_cat3_len, cat3_emb_size, name= 'cat3_emb')\n",
    "itemcond_emb = embed(train_raw.item_condition_id,itemcond_len ,itemcond_emb_size, name= 'itemcond_emb')\n",
    "shipping_emb = embed(train_raw.shipping, 2, shipping_emb_size, name= 'shipping_emb')\n",
    "\n",
    "end_time = time.time()\n",
    "print('embedding step took ' + str(end_time - start_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tensorflow running block (is this __main__?)\n",
    "\n",
    "input_x = tf.placeholder(tf.int32,[None, shape_set], name = \"input_x\") # pad_length = 25 or something defined earlier\n",
    "input_y = tf.placeholder(tf.float32,[None,1], name = \"input_y\") # train agianst this\n",
    "\n",
    "\n",
    "input_x_name = input_x[:,:input_name_len]\n",
    "input_x_itemdesc = input_x[:,input_name_len:(input_name_len + input_itemdesc_len)]\n",
    "input_x_cat1 = input_x[:,(input_name_len + input_itemdesc_len)]\n",
    "input_x_cat2 = input_x[:,(input_name_len + input_itemdesc_len)+1]\n",
    "input_x_cat3 = input_x[:,(input_name_len + input_itemdesc_len)+2]\n",
    "input_x_brand = input_x[:,(input_name_len + input_itemdesc_len)+3]\n",
    "input_x_itemcond = input_x[:,(input_name_len + input_itemdesc_len)+4]\n",
    "input_x_shipping = input_x[:,(input_name_len + input_itemdesc_len)+5]\n",
    "\n",
    "name_emb_lookup = tf.nn.embedding_lookup(name_emb, input_x_name)\n",
    "itemdesc_emb_lookup = tf.nn.embedding_lookup(itemdesc_emb,input_x_itemdesc)\n",
    "#name_emb_lookup = tf.nn.embedding_lookup(name_itemdesc_emb, input_x_name)\n",
    "#itemdesc_emb_lookup = tf.nn.embedding_lookup(name_itemdesc_emb,input_x_itemdesc)\n",
    "brand_emb_lookup = tf.nn.embedding_lookup(brand_emb,input_x_brand)\n",
    "cat1_emb_lookup = tf.nn.embedding_lookup(cat1_emb,input_x_cat1)\n",
    "cat2_emb_lookup = tf.nn.embedding_lookup(cat2_emb,input_x_cat2)\n",
    "cat3_emb_lookup = tf.nn.embedding_lookup(cat3_emb,input_x_cat3)\n",
    "itemcond_emb_lookup = tf.nn.embedding_lookup(itemcond_emb, input_x_itemcond)\n",
    "shipping_emb_lookup = tf.nn.embedding_lookup(shipping_emb, input_x_shipping)\n",
    "\n",
    "# expand name and item_desc because conv2d wants it 4-d (For CNN)\n",
    "name_emb_lookup_expand = tf.expand_dims(name_emb_lookup,-1)\n",
    "itemdesc_emb_lookup_expand = tf.expand_dims(itemdesc_emb_lookup,-1)\n",
    "\n",
    "# set some lazy parameters here\n",
    "out_nodes = 15\n",
    "dropout_keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "W_shape_name = [1,name_emb_size,1,out_nodes] #figure this out if it works\n",
    "b_shape_name = out_nodes # same as last dimension in W\n",
    "\n",
    "W_shape_itemdesc = [1,itemdesc_emb_size,1,out_nodes]\n",
    "b_shape_itemdesc = out_nodes\n",
    "\n",
    "layers_name = CNN(name_emb_lookup_expand,W_shape_name,b_shape_name,name_pad_size,\"W_name\", \"b_name\")\n",
    "layers_itemdesc = CNN(itemdesc_emb_lookup_expand,W_shape_itemdesc,b_shape_itemdesc,itemdesc_pad_size,\"W_itemdesc\",\"b_itemdesc\")\n",
    "'''\n",
    "layers_name = RNN_layer\n",
    "layers_itemdesc = RNN_layer()'''\n",
    "#combine the layers\n",
    "comb_layers = tf.concat([layers_name,layers_itemdesc, brand_emb_lookup, cat1_emb_lookup,\n",
    "                         cat2_emb_lookup, cat3_emb_lookup, itemcond_emb_lookup, shipping_emb_lookup],axis=1)\n",
    "\n",
    "#dense \n",
    "dense1 = dense_NN(comb_layers, 64, \"W_1\",\"b_1\")\n",
    "dense1 = relu_layer(dense1)\n",
    "dense1 = dropout_layer(dense1, dropout_keep_prob)\n",
    "\n",
    "dense2 = dense_NN(dense1, 128, \"W_2\",\"b_2\")\n",
    "dense2 = relu_layer(dense2)\n",
    "dense2 = dropout_layer(dense2, dropout_keep_prob)\n",
    "\n",
    "predictions = dense_NN(dense2, 1, \"W_pred\", \"b_pred\") \n",
    "\n",
    "\n",
    "loss = tf.losses.mean_squared_error(input_y, predictions)\n",
    "#loss,train_step  = train_the_NN(predictions,input_y,loss)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate = .001).minimize(loss)\n",
    "# as is, normalized predictions cause NaN in rmsle solving. adding .00001 just in case\n",
    "unwind_true = tf.log(tf.expm1((input_y* std_price_norm) + mean_price_norm)+ .00001) \n",
    "unwind_pred = tf.log(tf.expm1((predictions* std_price_norm) + mean_price_norm)+ .00001) \n",
    "rmsle_ = tf.sqrt(tf.reduce_mean(tf.square(unwind_true - unwind_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define savedmodel here\n",
    "\n",
    "export_dir = '/home/bsong/Python_Stuff/Scripts/model_save/'\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "running step: 100\n",
      "rmsle of validation set at this step: 0.699 \n",
      "loss: 0.557695\n",
      "resetting counter after 99 steps\n",
      "One hundred steps took 12.040 seconds.\n",
      " \n",
      "running step: 200\n",
      "rmsle of validation set at this step: 0.662 \n",
      "loss: 0.478642\n",
      "resetting counter after 100 steps\n",
      "One hundred steps took 11.574 seconds.\n",
      " \n",
      "running step: 300\n",
      "rmsle of validation set at this step: 0.649 \n",
      "loss: 0.450046\n",
      "resetting counter after 100 steps\n",
      "One hundred steps took 11.602 seconds.\n",
      " \n",
      "running step: 400\n",
      "rmsle of validation set at this step: 0.641 \n",
      "loss: 0.435661\n",
      "resetting counter after 100 steps\n",
      "One hundred steps took 11.526 seconds.\n",
      " \n",
      "running step: 500\n",
      "rmsle of validation set at this step: 0.637 \n",
      "loss: 0.425735\n",
      "resetting counter after 100 steps\n",
      "One hundred steps took 11.519 seconds.\n",
      " \n",
      "running step: 600\n",
      "rmsle of validation set at this step: 0.634 \n",
      "loss: 0.419745\n",
      "resetting counter after 100 steps\n",
      "One hundred steps took 11.511 seconds.\n",
      " \n",
      "running step: 700\n",
      "rmsle of validation set at this step: 0.633 \n",
      "loss: 0.416273\n",
      "resetting counter after 100 steps\n",
      "One hundred steps took 11.560 seconds.\n",
      " \n",
      "running step: 800\n",
      "rmsle of validation set at this step: 0.631 \n",
      "loss: 0.412751\n",
      "resetting counter after 100 steps\n",
      "One hundred steps took 11.559 seconds.\n",
      " \n",
      "running step: 900\n",
      "rmsle of validation set at this step: 0.630 \n",
      "loss: 0.411247\n",
      "resetting counter after 100 steps\n",
      "One hundred steps took 11.544 seconds.\n",
      " \n",
      "running step: 1000\n",
      "rmsle of validation set at this step: 0.629 \n",
      "loss: 0.408161\n",
      "resetting counter after 100 steps\n",
      "One hundred steps took 11.608 seconds.\n",
      " \n",
      "running step: 1100\n",
      "rmsle of validation set at this step: 0.628 \n",
      "loss: 0.406026\n",
      "resetting counter after 100 steps\n",
      "One hundred steps took 11.551 seconds.\n",
      " \n",
      "running step: 1200\n",
      "rmsle of validation set at this step: 0.628 \n",
      "loss: 0.407511\n",
      "One hundred steps took 11.592 seconds.\n",
      " \n",
      "running step: 1300\n",
      "rmsle of validation set at this step: 0.627 \n",
      "loss: 0.406143\n",
      "One hundred steps took 11.530 seconds.\n",
      " \n",
      "running step: 1400\n",
      "rmsle of validation set at this step: 0.626 \n",
      "loss: 0.402997\n",
      "resetting counter after 300 steps\n",
      "One hundred steps took 11.557 seconds.\n",
      " \n",
      "running step: 1500\n",
      "rmsle of validation set at this step: 0.627 \n",
      "loss: 0.403353\n",
      "One hundred steps took 11.550 seconds.\n",
      " \n",
      "running step: 1600\n",
      "rmsle of validation set at this step: 0.627 \n",
      "loss: 0.403768\n",
      "One hundred steps took 11.496 seconds.\n",
      " \n",
      "running step: 1700\n",
      "rmsle of validation set at this step: 0.627 \n",
      "loss: 0.403579\n",
      "One hundred steps took 11.512 seconds.\n",
      " \n",
      "running step: 1800\n",
      "rmsle of validation set at this step: 0.627 \n",
      "loss: 0.402879\n",
      "resetting counter after 400 steps\n",
      "One hundred steps took 11.613 seconds.\n",
      " \n",
      "running step: 1900\n",
      "rmsle of validation set at this step: 0.627 \n",
      "loss: 0.403776\n",
      "One hundred steps took 11.529 seconds.\n",
      " \n",
      "running step: 2000\n",
      "rmsle of validation set at this step: 0.628 \n",
      "loss: 0.405026\n",
      "One hundred steps took 11.672 seconds.\n",
      " \n",
      "running step: 2100\n",
      "rmsle of validation set at this step: 0.625 \n",
      "loss: 0.400335\n",
      "resetting counter after 300 steps\n",
      "One hundred steps took 11.561 seconds.\n",
      " \n",
      "running step: 2200\n",
      "rmsle of validation set at this step: 0.627 \n",
      "loss: 0.403691\n",
      "One hundred steps took 11.571 seconds.\n",
      " \n",
      "running step: 2300\n",
      "rmsle of validation set at this step: 0.626 \n",
      "loss: 0.40328\n",
      "One hundred steps took 11.562 seconds.\n",
      " \n",
      "running step: 2400\n",
      "rmsle of validation set at this step: 0.627 \n",
      "loss: 0.402884\n",
      "One hundred steps took 11.594 seconds.\n",
      " \n",
      "running step: 2500\n",
      "rmsle of validation set at this step: 0.626 \n",
      "loss: 0.401102\n",
      "One hundred steps took 11.573 seconds.\n",
      " \n",
      "running step: 2600\n",
      "rmsle of validation set at this step: 0.627 \n",
      "loss: 0.403773\n",
      "One hundred steps took 11.597 seconds.\n",
      " \n",
      "running step: 2700\n",
      "rmsle of validation set at this step: 0.628 \n",
      "loss: 0.404567\n",
      "One hundred steps took 11.547 seconds.\n",
      " \n",
      "running step: 2800\n",
      "rmsle of validation set at this step: 0.628 \n",
      "loss: 0.404696\n",
      "One hundred steps took 11.640 seconds.\n",
      " \n",
      "running step: 2900\n",
      "rmsle of validation set at this step: 0.627 \n",
      "loss: 0.403824\n",
      "One hundred steps took 11.479 seconds.\n",
      " \n",
      "running step: 3000\n",
      "rmsle of validation set at this step: 0.627 \n",
      "loss: 0.402812\n",
      "One hundred steps took 11.572 seconds.\n",
      " \n",
      "running step: 3100\n",
      "rmsle of validation set at this step: 0.628 \n",
      "loss: 0.40529\n",
      "One hundred steps took 11.542 seconds.\n",
      " \n",
      "running step: 3200\n",
      "rmsle of validation set at this step: 0.626 \n",
      "loss: 0.402476\n",
      "One hundred steps took 11.531 seconds.\n",
      " \n",
      "running step: 3300\n",
      "rmsle of validation set at this step: 0.627 \n",
      "loss: 0.402587\n",
      "One hundred steps took 11.517 seconds.\n",
      " \n",
      "running step: 3400\n",
      "rmsle of validation set at this step: 0.628 \n",
      "loss: 0.405969\n",
      "One hundred steps took 11.608 seconds.\n",
      " \n",
      "running step: 3500\n",
      "rmsle of validation set at this step: 0.630 \n",
      "loss: 0.408695\n",
      "One hundred steps took 11.566 seconds.\n",
      " \n",
      "running step: 3600\n",
      "rmsle of validation set at this step: 0.628 \n",
      "loss: 0.404241\n",
      "One hundred steps took 11.542 seconds.\n",
      " \n",
      "running step: 3700\n",
      "rmsle of validation set at this step: 0.628 \n",
      "loss: 0.406305\n",
      "One hundred steps took 11.540 seconds.\n",
      " \n",
      "running step: 3800\n",
      "rmsle of validation set at this step: 0.630 \n",
      "loss: 0.409076\n",
      "One hundred steps took 11.541 seconds.\n",
      " \n",
      "running step: 3900\n",
      "rmsle of validation set at this step: 0.627 \n",
      "loss: 0.404063\n",
      "One hundred steps took 11.536 seconds.\n",
      " \n",
      "running step: 4000\n",
      "rmsle of validation set at this step: 0.625 \n",
      "loss: 0.400493\n",
      "One hundred steps took 11.531 seconds.\n",
      " \n",
      "running step: 4100\n",
      "rmsle of validation set at this step: 0.629 \n",
      "loss: 0.407277\n",
      "One hundred steps took 11.543 seconds.\n",
      " \n",
      "running step: 4200\n",
      "rmsle of validation set at this step: 0.628 \n",
      "loss: 0.406367\n",
      "One hundred steps took 11.656 seconds.\n",
      " \n",
      "running step: 4300\n",
      "rmsle of validation set at this step: 0.629 \n",
      "loss: 0.406387\n",
      "One hundred steps took 11.586 seconds.\n",
      " \n",
      "running step: 4400\n",
      "rmsle of validation set at this step: 0.628 \n",
      "loss: 0.405227\n",
      "One hundred steps took 11.613 seconds.\n",
      " \n",
      "running step: 4500\n",
      "rmsle of validation set at this step: 0.629 \n",
      "loss: 0.407023\n",
      "One hundred steps took 11.536 seconds.\n",
      " \n",
      "running step: 4600\n",
      "rmsle of validation set at this step: 0.629 \n",
      "loss: 0.407278\n",
      "One hundred steps took 11.617 seconds.\n",
      " \n",
      "running step: 4700\n",
      "rmsle of validation set at this step: 0.630 \n",
      "loss: 0.407602\n",
      "One hundred steps took 11.502 seconds.\n",
      " \n",
      "running step: 4800\n",
      "rmsle of validation set at this step: 0.628 \n",
      "loss: 0.405529\n",
      "One hundred steps took 11.660 seconds.\n",
      " \n",
      "running step: 4900\n",
      "rmsle of validation set at this step: 0.628 \n",
      "loss: 0.406454\n",
      "One hundred steps took 11.579 seconds.\n",
      " \n",
      "running step: 5000\n",
      "rmsle of validation set at this step: 0.628 \n",
      "loss: 0.405309\n",
      "One hundred steps took 11.637 seconds.\n",
      " \n",
      "running step: 5100\n",
      "rmsle of validation set at this step: 0.628 \n",
      "loss: 0.405638\n",
      "One hundred steps took 11.746 seconds.\n",
      " \n",
      "running step: 5200\n",
      "rmsle of validation set at this step: 0.628 \n",
      "loss: 0.405833\n",
      "One hundred steps took 11.587 seconds.\n",
      " \n",
      "running step: 5300\n",
      "rmsle of validation set at this step: 0.629 \n",
      "loss: 0.406997\n",
      "One hundred steps took 11.502 seconds.\n",
      " \n",
      "running step: 5400\n",
      "rmsle of validation set at this step: 0.630 \n",
      "loss: 0.40931\n",
      "One hundred steps took 11.559 seconds.\n",
      " \n",
      "running step: 5500\n",
      "rmsle of validation set at this step: 0.628 \n",
      "loss: 0.405601\n",
      "One hundred steps took 11.600 seconds.\n",
      " \n",
      "running step: 5600\n",
      "rmsle of validation set at this step: 0.631 \n",
      "loss: 0.410028\n",
      "One hundred steps took 11.521 seconds.\n",
      " \n",
      "running step: 5700\n",
      "rmsle of validation set at this step: 0.630 \n",
      "loss: 0.408543\n",
      "One hundred steps took 11.648 seconds.\n",
      " \n",
      "running step: 5800\n",
      "rmsle of validation set at this step: 0.628 \n",
      "loss: 0.405972\n",
      "One hundred steps took 11.582 seconds.\n",
      " \n",
      "running step: 5900\n",
      "rmsle of validation set at this step: 0.628 \n",
      "loss: 0.405637\n",
      "One hundred steps took 11.597 seconds.\n",
      " \n",
      "running step: 6000\n",
      "rmsle of validation set at this step: 0.631 \n",
      "loss: 0.410832\n",
      "One hundred steps took 11.573 seconds.\n",
      " \n",
      "running step: 6100\n",
      "rmsle of validation set at this step: 0.629 \n",
      "loss: 0.407288\n",
      "One hundred steps took 11.591 seconds.\n",
      " \n",
      "running step: 6200\n",
      "rmsle of validation set at this step: 0.629 \n",
      "loss: 0.406876\n",
      "One hundred steps took 11.513 seconds.\n",
      " \n",
      "running step: 6300\n",
      "rmsle of validation set at this step: 0.630 \n",
      "loss: 0.40753\n",
      "One hundred steps took 11.607 seconds.\n",
      " \n",
      "running step: 6400\n",
      "rmsle of validation set at this step: 0.631 \n",
      "loss: 0.410284\n",
      "One hundred steps took 11.538 seconds.\n",
      " \n",
      "running step: 6500\n",
      "rmsle of validation set at this step: 0.628 \n",
      "loss: 0.405145\n",
      "One hundred steps took 11.579 seconds.\n",
      " \n",
      "running step: 6600\n",
      "rmsle of validation set at this step: 0.630 \n",
      "loss: 0.408509\n",
      "One hundred steps took 11.542 seconds.\n",
      " \n",
      "running step: 6700\n",
      "rmsle of validation set at this step: 0.629 \n",
      "loss: 0.407703\n",
      "One hundred steps took 11.586 seconds.\n",
      " \n",
      "running step: 6800\n",
      "rmsle of validation set at this step: 0.628 \n",
      "loss: 0.406769\n",
      "One hundred steps took 11.626 seconds.\n",
      " \n",
      "running step: 6900\n",
      "rmsle of validation set at this step: 0.628 \n",
      "loss: 0.405892\n",
      "One hundred steps took 11.635 seconds.\n",
      " \n",
      "running step: 7000\n",
      "rmsle of validation set at this step: 0.629 \n",
      "loss: 0.406967\n",
      "One hundred steps took 11.573 seconds.\n",
      " \n",
      "running step: 7100\n",
      "rmsle of validation set at this step: 0.627 \n",
      "loss: 0.404363\n",
      "One hundred steps took 11.620 seconds.\n",
      " \n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Start training...')\n",
    "\n",
    "start_time = time.time()\n",
    "learn_rate = 1e-4\n",
    "counter = 0 \n",
    "i = 1\n",
    "best_loss = 2 # just has to be greater than 2 really\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer, {feat_placeholder: features_input, label_placeholder: label_input})\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)  \n",
    "\n",
    "    while counter <= 5001:\n",
    "        features_, label_ = sess.run(next_batch)\n",
    "        sess.run(train_step,{input_x: features_, input_y: label_, dropout_keep_prob:.5})\n",
    "        \n",
    "        end_time = time.time()\n",
    "        if i % 100 == 0:\n",
    "            print('running step: ' + str(i))\n",
    "            loss_val, rmsle = sess.run([loss, rmsle_],{input_x: val_temp_set, input_y: val_price, dropout_keep_prob:1})\n",
    "            print(\"rmsle of validation set at this step: %5.3f \" % rmsle)\n",
    "            print(\"loss: \" + str(loss_val))\n",
    "            if loss_val < best_loss:\n",
    "                best_loss = loss_val\n",
    "                saver.save(sess, save_path=export_dir)\n",
    "                print('resetting counter after ' + str(counter) + ' steps')\n",
    "                counter = 0\n",
    "            tot_time = end_time - start_time\n",
    "            print('One hundred steps took %5.3f seconds.' % tot_time)\n",
    "            print(' ')\n",
    "            start_time = time.time()\n",
    "        counter += 1\n",
    "        i += 1 \n",
    "        if i % 500 == 0: \n",
    "            learn_rate = learn_rate/10\n",
    "\n",
    "    print('Done!')            "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
