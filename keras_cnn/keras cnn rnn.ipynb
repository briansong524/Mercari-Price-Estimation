{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import packages here\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "import gc\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "\n",
    "train_raw = pd.read_csv('/home/bsong/Python_Stuff/Data/Kaggle_Mercari/train.tsv',delimiter= '\\t')\n",
    "#test_raw = pd.read_csv('/home/bsong/Python_Stuff/Data/Kaggle_Mercari/test.tsv',delimiter = '\\t') # too lazy to submit kaggle kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define methods here\n",
    "\n",
    "def split_cat(text): # this one is to reduce the categoriy_name into three subcategories\n",
    "    try: return text.split(\"/\")\n",
    "    except: return (\"No Label\", \"No Label\", \"No Label\")\n",
    "\n",
    "def handle_missing_inplace(dataset):  # this one is to put placeholders in place of missing values (NaN)\n",
    "    dataset['cat1'].fillna(value='No Label', inplace=True)\n",
    "    dataset['cat2'].fillna(value='No Label', inplace=True)\n",
    "    dataset['cat3'].fillna(value='No Label', inplace=True)\n",
    "    dataset['brand_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['item_description'].fillna(value='No description yet', inplace=True)\n",
    "    \n",
    "def to_categorical(dataset): # this is to define the datatype as \"category\"\n",
    "    dataset['cat1'] = dataset['cat1'].astype('category')\n",
    "    dataset['cat2'] = dataset['cat2'].astype('category')\n",
    "    dataset['cat3'] = dataset['cat3'].astype('category')\n",
    "    \n",
    "def raw_text(dataset):    # this is to tokenize, filter, clean, etc raw text values \n",
    "    raw_text = np.hstack([dataset.item_description.str.lower(), dataset.name.str.lower()])  #make into one array i think\n",
    "    tok_raw = Tokenizer(num_words=20000, #max words\n",
    "                    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                    lower=True,\n",
    "                    split=\" \",\n",
    "                    char_level=False) #if True, every character will be treated as a token.\n",
    "    tok_raw.fit_on_texts(raw_text) # fit_on_texts is to train whatever text on ( in this case, everything in item_desc & name)\n",
    "                                   # basically made it into a dictionary for texts_to_sequences\n",
    "                                   # it seems fit_on_texts counts words and returns a vocab list (ordered by count decreasing)\n",
    "    # texts_to_sequences maybe saves space? and lookup time?  \n",
    "    dataset[\"seq_item_description\"] = tok_raw.texts_to_sequences(dataset.item_description.str.lower())\n",
    "    dataset[\"seq_name\"] = tok_raw.texts_to_sequences(dataset.name.str.lower())\n",
    "    dataset[\"Raw Text Combined\"] = dataset.seq_name + dataset.seq_item_description # just for reference i think\n",
    "    \n",
    "def get_keras_data(dataset): # converts input dataset into something keras can use (seems to be dict)\n",
    "    X = {\n",
    "        'name': pad_sequences(dataset.seq_name, maxlen=10) # pad_sequences is making the values equally padded/truncated i guess \n",
    "        ,'item_desc': pad_sequences(dataset.seq_item_description, maxlen=75)\n",
    "        ,'brand_name': np.array(dataset.brand_name)\n",
    "        ,'cat1': np.array(dataset.cat1)\n",
    "        ,'cat2': np.array(dataset.cat2)\n",
    "        ,'cat3': np.array(dataset.cat3)\n",
    "        ,'item_condition': np.array(dataset.item_condition_id)\n",
    "        ,'num_vars': np.array(dataset.shipping)\n",
    "    }\n",
    "    return X\n",
    "\n",
    "def cutting(dataset):\n",
    "    pop_brand = dataset['brand_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n",
    "    dataset.loc[~dataset['brand_name'].isin(pop_brand), 'brand_name'] = 'missing'\n",
    "    \n",
    "    pop_category1 = dataset['cat1'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n",
    "    pop_category2 = dataset['cat2'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n",
    "    pop_category3 = dataset['cat3'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_CATEGORIES]\n",
    "    dataset.loc[~dataset['cat1'].isin(pop_category1), 'cat1'] = 'missing'\n",
    "    dataset.loc[~dataset['cat2'].isin(pop_category2), 'cat2'] = 'missing'\n",
    "    dataset.loc[~dataset['cat3'].isin(pop_category3), 'cat3'] = 'missing'\n",
    "\n",
    "NUM_BRANDS = 4000\n",
    "NUM_CATEGORIES = 1000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merge = train_raw\n",
    "#submission = test_raw[['test_id']]\n",
    "\n",
    "merge['cat1'],merge['cat2'],merge['cat3'] = \\\n",
    "zip(*merge['category_name'].apply(lambda x: split_cat(x))) # split the categories into three new columns\n",
    "merge.drop('category_name',axis = 1, inplace = True) # remove the column that isn't needed anymore\n",
    "\n",
    "handle_missing_inplace(merge) # replaces NaN with a string placeholder\n",
    "\n",
    "cutting(merge) # can't figure out what this does. maybe setting unpopular brands as 'missing'?\n",
    "\n",
    "raw_text(merge)\n",
    "\n",
    "le = LabelEncoder() # use this to change categorical names into index numbers (0 1 2 3 or something)\n",
    "merge.brand_name = le.fit_transform(merge.brand_name)\n",
    "merge.cat1 = le.fit_transform(merge.cat1)\n",
    "merge.cat2 = le.fit_transform(merge.cat2)\n",
    "merge.cat3 = le.fit_transform(merge.cat3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsong/anaconda/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(985885, 13)\n",
      "(422523, 13)\n"
     ]
    }
   ],
   "source": [
    "#EXTRACT DEVELOPTMENT TEST\n",
    "dtest = merge.iloc[nrow_train:, ]\n",
    "dtrain, dvalid = train_test_split(merge.iloc[:nrow_train, ], random_state=123, train_size=0.7)\n",
    "print(dtrain.shape)\n",
    "print(dvalid.shape)\n",
    "\n",
    "\n",
    "X_train = get_keras_data(dtrain)\n",
    "X_valid = get_keras_data(dvalid)\n",
    "X_test = get_keras_data(dtest)\n",
    "\n",
    "Y_train =  np.log1p(np.array(dtrain.price))\n",
    "Y_valid =  np.log1p(np.array(dvalid.price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# upper parameter of text columns. max is equivalent to the maximum number of words in column(s)\n",
    "\n",
    "MAX_TEXT = np.max([np.max(merge.seq_name.max()), np.max(merge.seq_item_description.max())])+2 #max of max \n",
    "MAX_cat1 = np.max([merge.cat1.max()])+1 # maybe +1 because of 0 index. np.max is unnecessary here\n",
    "MAX_cat2 = np.max([merge.cat2.max()])+1\n",
    "MAX_cat3 = np.max([merge.cat3.max()])+1\n",
    "MAX_BRAND = np.max([merge.brand_name.max()])+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "item_desc (InputLayer)          (None, 75)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "name (InputLayer)               (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "brand_name (InputLayer)         (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat1 (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat2 (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cat3 (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, 75, 10)       199970      item_desc[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 10, 10)       199970      name[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)        (None, 1, 50)        200050      brand_name[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_16 (Embedding)        (None, 1, 10)        110         cat1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_17 (Embedding)        (None, 1, 20)        2280        cat2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_18 (Embedding)        (None, 1, 30)        26130       cat3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 73, 16)       496         embedding_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 8, 8)         248         embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 50)           0           embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 10)           0           embedding_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 20)           0           embedding_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 30)           0           embedding_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 16)           0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 8)            0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "num_vars (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_condition (InputLayer)     (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 136)          0           flatten_9[0][0]                  \n",
      "                                                                 flatten_10[0][0]                 \n",
      "                                                                 flatten_11[0][0]                 \n",
      "                                                                 flatten_12[0][0]                 \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 num_vars[0][0]                   \n",
      "                                                                 item_condition[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 256)          35072       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 256)          0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 128)          32896       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 128)          0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 64)           8256        dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 64)           0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1)            65          dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 705,543\n",
      "Trainable params: 705,543\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#KERAS MODEL DEFINITION\n",
    "from keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, Conv1D, GlobalMaxPooling1D, Embedding, Flatten, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras import backend as K\n",
    "\n",
    "def get_callbacks(filepath, patience=2):#when is this called?\n",
    "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\") # break training if small/no improvement after 2 epochs\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True) # save best model \n",
    "    return [es, msave]\n",
    "\n",
    "def rmsle_cust(y_true, y_pred): # calculating the rmsle\n",
    "    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1e-5)\n",
    "    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1e-5)\n",
    "    return K.sqrt(K.mean(K.square(first_log - second_log), axis=-1))\n",
    "\n",
    "def get_model():\n",
    "    #params\n",
    "    dr_r = 0.5\n",
    "    \n",
    "    #Inputs (initializing. similar to tf.placeholder maybe)\n",
    "    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\") # I guess make sure its Input(shape,name)\n",
    "    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n",
    "    brand_name = Input(shape=[1], name=\"brand_name\") # one word, so shape = [1]\n",
    "    cat1 = Input(shape=[1], name=\"cat1\") \n",
    "    cat2 = Input(shape=[1], name=\"cat2\")\n",
    "    cat3 = Input(shape=[1], name=\"cat3\")\n",
    "    item_condition = Input(shape=[1], name=\"item_condition\")\n",
    "    num_vars = Input(shape=[1], name=\"num_vars\")\n",
    "    \n",
    "    #Embeddings layers (text based columns go through this. can only be used as the first layer. basically word2vec)\n",
    "    emb_name = Embedding(MAX_TEXT, 10)(name) # Embedding(input_dim,output_dim)\n",
    "    emb_item_desc = Embedding(MAX_TEXT, 10)(item_desc)\n",
    "    emb_brand_name = Embedding(MAX_BRAND, 50)(brand_name)\n",
    "    emb_cat1 = Embedding(MAX_cat1, 10)(cat1)\n",
    "    emb_cat2 = Embedding(MAX_cat2, 20)(cat2)\n",
    "    emb_cat3 = Embedding(MAX_cat3, 30)(cat3)\n",
    " \n",
    "    #rnn layer\n",
    "    cnn_layer1 = Conv1D(filters=16, kernel_size=3, activation='relu') (emb_item_desc) # text cnn for 'item_description'\n",
    "    cnn_layer2 = Conv1D(filters=8, kernel_size=3, activation='relu')(emb_name) # text cnn for 'name'\n",
    "    \n",
    "    cnn_layer1 = GlobalMaxPooling1D()(cnn_layer1) \n",
    "    cnn_layer2 = GlobalMaxPooling1D()(cnn_layer2)\n",
    "    \n",
    "    #main layer\n",
    "    main_l = concatenate([\n",
    "        Flatten() (emb_brand_name) # embedding step may have output of shape (MAX_BRAND,50)\n",
    "        , Flatten() (emb_cat1)     # https://stackoverflow.com/questions/43237124/role-of-flatten-in-keras\n",
    "        , Flatten() (emb_cat2)\n",
    "        , Flatten() (emb_cat3)\n",
    "        , cnn_layer1\n",
    "        , cnn_layer2\n",
    "        , num_vars\n",
    "        , item_condition\n",
    "    ])\n",
    "    \n",
    "    main_l = Dropout(dr_r) (Dense(256, activation=\"relu\") (main_l))\n",
    "    main_l = Dropout(dr_r) (Dense(128, activation=\"relu\") (main_l))\n",
    "    main_l = Dropout(dr_r) (Dense(64, activation=\"relu\") (main_l))\n",
    "    \n",
    "    \n",
    "    #output\n",
    "    output = Dense(1, activation=\"linear\") (main_l)\n",
    "    \n",
    "    #model\n",
    "    model = Model([name, item_desc, brand_name, cat1, cat2, cat3, item_condition, num_vars], output)\n",
    "    \n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\", rmsle_cust])\n",
    "    \n",
    "    return model\n",
    "\n",
    "    \n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 985885 samples, validate on 422523 samples\n",
      "Epoch 1/25\n"
     ]
    }
   ],
   "source": [
    "# FITTING THE MODEL (keras method)\n",
    "BATCH_SIZE = 20000\n",
    "epochs = 25\n",
    "\n",
    "model = get_model()\n",
    "model.fit(X_train, Y_train, epochs=epochs, batch_size=BATCH_SIZE\n",
    "          , validation_data=(X_valid, Y_valid)\n",
    "          , verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "preds = np.squeeze(np.expm1(preds))\n",
    "\n",
    "#correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))    \n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(preds.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rmsle(h, y): \n",
    "    log_h = np.log(h+1) # the +1 is to prevent 0 \n",
    "    log_y = np.log(y+1) # writing these to prevent memoryerror\n",
    "    sq_logs = np.square(log_h - log_y)\n",
    "    score_ = np.sqrt(np.mean(sq_logs))\n",
    "    return score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmsle_score = rmsle(preds,Y_test)\n",
    "print(rmsle_score)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
