{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsong/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Update from v04:\n",
    "-implement rnn\n",
    "- convert back to regression\n",
    "'''\n",
    "\n",
    "## import packages\n",
    "import time\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import random\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import data took 7.557194709777832 seconds.\n"
     ]
    }
   ],
   "source": [
    "## import data \n",
    "start_time = time.time()\n",
    "\n",
    "train_raw = pd.read_csv('/home/bsong/Python_Stuff/Data/Kaggle_Mercari/train.tsv',delimiter= '\\t')\n",
    "#train_raw = train_raw.iloc[0:10000,] # just a bit\n",
    "# standardize price here because may as well\n",
    "normalized_price = np.log1p(train_raw['price'].values)\n",
    "mean_price_norm = np.mean(normalized_price)\n",
    "std_price_norm = np.std(normalized_price) \n",
    "train_raw['price'] = (normalized_price - mean_price_norm)/std_price_norm \n",
    "\n",
    "end_time = time.time()\n",
    "print('import data took ' + str(end_time - start_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define functions to use\n",
    "\n",
    "######## Basic text manipulation functions (some specific to Mercari Kaggle Competition) \n",
    "\n",
    "def split_cat(text): # this one is to reduce the categoriy_name into three subcategories\n",
    "    try: return text.split(\"/\")\n",
    "    except: return (\"No Label\", \"No Label\", \"No Label\")\n",
    "\n",
    "def handle_missing_inplace(dataset):  # this one is to put placeholders in place of missing values (NaN)\n",
    "    dataset['cat1'].fillna(value='No Label', inplace=True)\n",
    "    dataset['cat2'].fillna(value='No Label', inplace=True)\n",
    "    dataset['cat3'].fillna(value='No Label', inplace=True)\n",
    "    dataset['brand_name'].fillna(value='missing', inplace=True)\n",
    "    dataset['item_description'].fillna(value='No description yet', inplace=True)\n",
    "     \n",
    "def build_dictionary(words, n_words): # dictionary that maps words to indices. this function should be modular.\n",
    "    #input is [['a','b','c'],['a','b','c']]\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]] # word indexed as \"unknown\" if not one of the top #n_words (popular/common) words (-1 is filler #)\n",
    "    count.extend(Counter(words).most_common(n_words - 1)) # most_common returns the top (n_words-1) ['word',count]\n",
    "    dictionary = dict()\n",
    "    for word, _ in count: # the 'word, _' is writted because count is a list of list(2), so defining 'word' as the first term per\n",
    "        dictionary[word] = len(dictionary) # {'word': some number incrementing by one. fyi, no repeats because from most_common)}\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys())) # {ind. : 'word'} I guess for looking up if needed?\n",
    "    return dictionary, reversed_dictionary\n",
    "\n",
    "def clean_and_tokenize(dataset_col): # input is a column of strings\n",
    "    pattern = '[A-Za-z]+' # does this only keep words\n",
    "    pattern2 = '[!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n]' # to rid of special characters\n",
    "    list_of_lists = list()\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for word in dataset_col:\n",
    "        list_of_words = list()\n",
    "        word = re.sub(pattern2, r'', word)\n",
    "        tokenized = tokenizer.tokenize(word)\n",
    "        tokenized_filtered = filter(lambda token: token not in stop_words, tokenized)\n",
    "        for i in tokenized_filtered:\n",
    "            if (len(i) > 2 ): #ignore words of length 2 or less\n",
    "                list_of_words.append(i.lower()) # append all words to one list\n",
    "        list_of_lists.append(list_of_words)\n",
    "    list_as_series = pd.Series(list_of_lists)\n",
    "    return list_as_series\n",
    "\n",
    "def just_punc_and_lower(dataset_col):\n",
    "    pattern = '[A-Za-z]+' # does this only keep words\n",
    "    pattern2 = '[!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n]' # to rid of special characters\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    list_of_lists = list()\n",
    "    for word in dataset_col:\n",
    "        word = re.sub(pattern2,r'',word)\n",
    "        list_of_words = word.lower().split(' ')\n",
    "        list_of_lists.append(list_of_words)\n",
    "    list_as_series = pd.Series(list_of_lists)\n",
    "    return list_as_series\n",
    "\n",
    "def convert_word_to_ind(dataset_col,dictionary): # input the pandas column of texts and dictionary. This should be modular\n",
    "    # each input should be a string of cleaned words tokenized into a list (ex. ['this', 'is', 'an', 'item'])\n",
    "    # dictionary should be the dictionary obtained from build_dictionary\n",
    "    list_of_lists = []\n",
    "    unk_count = 0 # total 'unknown' words counted\n",
    "    for word_or_words in dataset_col: # words is the list of all words\n",
    "        list_of_inds = []\n",
    "        for word in word_or_words:\n",
    "            if word in dictionary:\n",
    "                index = np.int(dictionary[word]) # dictionary contains top words, so if in, it gets an index\n",
    "            else:\n",
    "                index = 0  #  or dictionary['UNK']? can figure out later\n",
    "                unk_count += 1\n",
    "            list_of_inds.append(index)\n",
    "        list_of_lists.append(list_of_inds)\n",
    "\n",
    "    # make list_of_lists into something that can be put into pd.DataFrame\n",
    "    #list_as_series = pd.Series(list_of_lists)\n",
    "    list_as_series = np.array(list_of_lists)\n",
    "    return list_as_series, unk_count\n",
    "\n",
    "def pad_word_indices(col_of_indices, pad_length): # col_of_indices can be a pd series. \n",
    "    temp_series = [] # append vectors into here\n",
    "    for list_inds in col_of_indices:\n",
    "        len_list = len(list_inds)\n",
    "        if len_list >= pad_length:\n",
    "            temp_series.append(np.array(list_inds[(len_list-pad_length):]))\n",
    "        else:\n",
    "            padded_vec = [0]*(pad_length-len_list)\n",
    "            padded_vec.extend(list_inds)\n",
    "            temp_series.append(np.array(padded_vec))\n",
    "    return temp_series\n",
    "\n",
    "def convert_word_to_padded(dataset_col,dictionary,pad_length): # input the pandas column of texts and dictionary. This should be modular\n",
    "    # each input should be a string of cleaned words tokenized into a list (ex. ['this', 'is', 'an', 'item'])\n",
    "    # dictionary should be the dictionary obtained from build_dictionary\n",
    "    # use this function when you know how long you want your pad_length\n",
    "    #   - otherwise, use convert_word_to_ind, and pad_word_indices\n",
    "    #   - eventually, will look into cleaning these three functions up.\n",
    "    list_of_lists = []\n",
    "    unk_count = 0 # total 'unknown' words counted\n",
    "    for word_or_words in dataset_col: # words is the list of all words\n",
    "        list_of_inds = []\n",
    "        count_inds = 0\n",
    "        for word in word_or_words:\n",
    "            if word in dictionary:\n",
    "                index = np.int(dictionary[word]) # dictionary contains top words, so if in, it gets an index\n",
    "            else:\n",
    "                index = 0  #  or dictionary['UNK']? can figure out later\n",
    "                unk_count += 1\n",
    "            count_inds +=1\n",
    "            list_of_inds.append(index) \n",
    "        if count_inds >= pad_length:\n",
    "            asdf = list_of_inds[(count_inds-pad_length):]\n",
    "        else: \n",
    "            asdf = [0]*(pad_length-count_inds)\n",
    "            asdf.extend(list_of_inds)\n",
    "        temp = np.array(asdf)\n",
    "        list_of_lists.append(temp)\n",
    "    list_as_series = np.array(list_of_lists)\n",
    "    return list_as_series, unk_count\n",
    "\n",
    "def obtain_reasonable_vocab_size(list_words, perc_words = .95):\n",
    "    counter_ = Counter(list_words)\n",
    "    counts = [i for _,i in counter_.most_common()]\n",
    "    tot_words = len(list_words)\n",
    "    print('total words (with repeats): ' + str(tot_words))\n",
    "    tot_count = 0\n",
    "    runs = 0\n",
    "    while tot_count < round(perc_words*tot_words):\n",
    "        tot_count += counts[runs]\n",
    "        runs += 1\n",
    "    print('reasonable vocab size: ' + str(runs))\n",
    "\n",
    "def obtain_reasonable_pad_length(list_words, perc_words = 0.95):\n",
    "    len_list = [len(i) for i in list_words]\n",
    "    sort_list = sorted(len_list)\n",
    "    ind_good = sort_list[round(perc_words*len(sort_list))]\n",
    "    print('reasonable pad length: ' + str(ind_good))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning \"category_name\" and making one-worded features to indices took 8.097156524658203 seconds.\n"
     ]
    }
   ],
   "source": [
    "## clean \"category_name\" and make numeric indicies for one-worded features (brand_name, cat1/2/3)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_raw['cat1'],train_raw['cat2'],train_raw['cat3'] = \\\n",
    "zip(*train_raw['category_name'].apply(lambda x: split_cat(x))) # split the categories into three new columns\n",
    "train_raw.drop('category_name',axis = 1, inplace = True) # remove the column that isn't needed anymore\n",
    "\n",
    "handle_missing_inplace(train_raw) # replaces NaN with a string placeholder 'missing'\n",
    "\n",
    "end_time = time.time()\n",
    "print('cleaning \"category_name\" and making one-worded features to indices took ' + str(end_time - start_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning name and item_description and getting all the words took 27.231472492218018 seconds.\n"
     ]
    }
   ],
   "source": [
    "## convert name and item_desc to indices, then configure a bit more\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_raw['name_token'] = just_punc_and_lower(train_raw.name)\n",
    "train_raw['itemdesc_token'] = just_punc_and_lower(train_raw.item_description)\n",
    "\n",
    "## used below two blocks or so\n",
    "all_name = [item for sublist in train_raw.name_token for item in sublist]\n",
    "all_item_desc = [item for sublist in train_raw.itemdesc_token for item in sublist]\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print('cleaning name and item_description and getting all the words took ' + str(end_time - start_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## analyze name and item_description\n",
    "\n",
    "len_list = [len(i) for i in train_raw.name_token.values]\n",
    "\n",
    "print(' ')\n",
    "print('name')\n",
    "obtain_reasonable_vocab_size(all_name)\n",
    "plt.hist(len_list, bins = 10)\n",
    "plt.show()\n",
    "obtain_reasonable_pad_length(train_raw.name_token.values)\n",
    "\n",
    "print(' ')\n",
    "print('item_description')\n",
    "obtain_reasonable_vocab_size(all_item_desc)\n",
    "len_list = [len(i) for i in train_raw.itemdesc_token.values]\n",
    "plt.hist(len_list, bins = 20)\n",
    "plt.show()\n",
    "obtain_reasonable_pad_length(train_raw.itemdesc_token.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Recurrent NN setup for name and item_description\n",
    "\n",
    "dict_name_len = 12000\n",
    "dict_itemdesc_len = 8737\n",
    "\n",
    "name_pad_len = 7\n",
    "itemdesc_pad_len = 20\n",
    "\n",
    "name_embed_len = 15\n",
    "itemdesc_embed_len = 15\n",
    "\n",
    "# build dictionary for indices and use it to convert word to indices\n",
    "# same time, the indices will be padded to maintain np.array dtype\n",
    "\n",
    "name_dict, name_dict_rev = build_dictionary(all_name, dict_name_len)\n",
    "name_padded, _ = convert_word_to_padded(train_raw.name_token, name_dict, name_pad_len)\n",
    "\n",
    "itemdesc_dict, itemdesc_dict_rev = build_dictionary(all_item_desc, dict_itemdesc_len)\n",
    "itemdesc_padded, _ = convert_word_to_padded(train_raw.itemdesc_token, itemdesc_dict, itemdesc_pad_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique brands: 4810\n",
      "Number of unique categories in cat1: 11\n",
      "Number of unique categories in cat2: 114\n",
      "Number of unique categories in cat3: 871\n"
     ]
    }
   ],
   "source": [
    "## deciding dictionary size for brand and cat1/2/3\n",
    "\n",
    "brand_unique_set = set(train_raw.brand_name.values)\n",
    "print('Number of unique brands: ' + str(len(brand_unique_set)))\n",
    "\n",
    "cat1_set = set(train_raw.cat1.values)\n",
    "print('Number of unique categories in cat1: ' + str(len(cat1_set)))\n",
    "\n",
    "cat2_set = set(train_raw.cat2.values)\n",
    "print('Number of unique categories in cat2: ' + str(len(cat2_set)))\n",
    "\n",
    "cat3_set = set(train_raw.cat3.values)\n",
    "print('Number of unique categories in cat3: ' + str(len(cat3_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73750 0 0 44510\n",
      "making dictionaries for brand and categories took 34.41069555282593 seconds.\n"
     ]
    }
   ],
   "source": [
    "## make dictionaries for brand_name and cat1/2/3\n",
    "\n",
    "start_time = time.time()\n",
    "# define dictionary lengths here\n",
    "dict_brand_len = 379 # total words + 1 for \"UNK\" is minimal size\n",
    "dict_cat1_len = 12 \n",
    "dict_cat2_len= 115 \n",
    "dict_cat3_len = 305\n",
    "\n",
    "brand_name_dict, brand_name_dict_rev = build_dictionary(train_raw['brand_name'], dict_brand_len)\n",
    "train_raw['brand_name_inds'], count_unk_brand = convert_word_to_ind(train_raw['brand_name'].values.reshape((-1,1)), brand_name_dict)\n",
    "cat1_dict ,cat1_rev_dict= build_dictionary(train_raw['cat1'],dict_cat1_len)\n",
    "train_raw['cat1_inds'], count_unk_cat1 = convert_word_to_ind(train_raw['cat1'].values.reshape((-1,1)), cat1_dict)\n",
    "cat2_dict ,cat2_rev_dict= build_dictionary(train_raw['cat2'],dict_cat2_len)\n",
    "train_raw['cat2_inds'], count_unk_cat2 = convert_word_to_ind(train_raw['cat2'].values.reshape((-1,1)), cat2_dict)\n",
    "cat3_dict ,cat3_rev_dict= build_dictionary(train_raw['cat3'],dict_cat3_len)\n",
    "train_raw['cat3_inds'], count_unk_cat3 = convert_word_to_ind(train_raw['cat3'].values.reshape((-1,1)), cat3_dict)\n",
    "\n",
    "print(str(count_unk_brand) + ' ' + str(count_unk_cat1) + ' '+ str(count_unk_cat2) + \" \" + str(count_unk_cat3))\n",
    "\n",
    "end_time = time.time()\n",
    "print('making dictionaries for brand and categories took ' + str(end_time - start_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## regular neural network function define here\n",
    "# This is to use for the simpler columns (brand_name, item_condition, cat1/2/3)\n",
    "# note to self: maybe separating dropout is better for manipulation purposes (and pooling and dropout lol.)\n",
    "\n",
    "# RegNN used for converting embedded features into whatever out_nodes. \n",
    "# I feel dense_NN achieves the exact same thing, but one layer, but this happened because I was iteratively progressing through this \n",
    "# project and didn't want to erase too many things. 1/11/18\n",
    "\n",
    "def RegNN(x, dropout_keep_prob, vocab_size, embed_size, batch_len, out_len):\n",
    "    #print('shape of input:' + str(x.shape))\n",
    "    # x should be of size [batch_len,embed_size] \n",
    "    # set up some weights/bias stuff\n",
    "    W1 = tf.Variable(tf.truncated_normal([vocab_size,embed_size], stddev=0.1))\n",
    "    b1 = tf.Variable(tf.constant(0.1, shape=[vocab_size,1])) # maybe batch_len   \n",
    "    #print('shape of W1:' + str(W1.shape))\n",
    "    #print('shape of b1:' + str(b1.shape))\n",
    "    \n",
    "    # xW + b \n",
    "    NN_layer = tf.matmul(W1,tf.transpose(x)) + b1 # this outputs shape (vocab_size,batch_len)\n",
    "    #print('NN_layer shape: ' + str(NN_layer.shape)) \n",
    "    # ReLU layer\n",
    "    \n",
    "    h = tf.nn.relu(NN_layer)\n",
    "    \n",
    "    # Drop Layer\n",
    "    h_drop = tf.nn.dropout(h, dropout_keep_prob) # still (vocab_size,batch_len)\n",
    "    \n",
    "    #W2 = tf.Variable(tf.truncated_normal([vocab_size,out_len]))\n",
    "    #b2 = tf.constant(0.1, shape=[batch_len,1])\n",
    "    \n",
    "    #NN_layer2 = tf.matmul(tf.transpose(h_drop),W2) + b2 # this outputs shape (batch_len,out_len)\n",
    "    #print('NN_layer2 shape: ' + str(NN_layer2.shape))\n",
    "    #h2 = tf.nn.relu(NN_layer2)\n",
    "    #h2_drop = tf.nn.dropout(h2, dropout_keep_prob) # should be of length (batch_len, out_len)\n",
    "    \n",
    "    return h_drop #h2_drop\n",
    "\n",
    "\n",
    "def embed(inputs, size, dim,name):\n",
    "    # inputs is a list of indices\n",
    "    # size is the number of unique indices (look for max index to achieve this if ordered)\n",
    "    # dim is the number of embedded numbers \n",
    "    std = np.sqrt(2 / dim)\n",
    "    emb = tf.Variable(tf.random_uniform([size, dim], -std, std), name = name)\n",
    "    return emb\n",
    "\n",
    "\n",
    "# test block for CNN \n",
    "# based on http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n",
    "# note to self: maybe separating dropout is better for manipulation purposes (and pooling and dropout lol.)\n",
    "\n",
    "def CNN(x,W_shape,b_shape,pad_length, name_w, name_b):\n",
    "    # x is the expanded lookup tables that will be trained\n",
    "    W1 = tf.Variable(tf.truncated_normal(W_shape, stddev=0.1), name= name_w) #\"W1\"\n",
    "    b1 = tf.Variable(tf.constant(0.1, shape=[b_shape]), name = name_b) # \"b1\"\n",
    "    conv = tf.nn.conv2d( #tf.layers.conv2d is also used, with  more parameters. Probably a slightly higher API because of that.\n",
    "        x,\n",
    "        W1,\n",
    "        strides = [1,1,1,1],\n",
    "        padding=\"VALID\",\n",
    "        name=\"conv\")\n",
    "    #print('shape of CNN output:' + str(conv.shape))\n",
    "    h = tf.nn.relu(tf.nn.bias_add(conv, b1), name=\"relu\")\n",
    "    #print('shape after ReLU: ' + str(h.shape))\n",
    "    pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, pad_length, 1, 1],\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    "    #print('shape after max pooling: ' + str(pooled.shape))\n",
    "    pool_flat = tf.reshape(pooled, [-1, out_nodes])\n",
    "    #print(\"shape after flattening:\" + str(pool_flat.shape))\n",
    "\n",
    "    #h_drop = tf.nn.dropout(pool_flat, dropout_keep_prob)\n",
    "    #print('shape after dropout: ' + str(h_drop.shape))\n",
    "    return pool_flat\n",
    "    \n",
    "    \n",
    "def dense_NN(x,out_len, name_w, name_b):\n",
    "\n",
    "    tot_nodes = x.shape[1]\n",
    "    W_dense = tf.Variable(tf.truncated_normal([int(tot_nodes) , out_len], stddev=0.1), name=name_w) #\"W2\"\n",
    "    b_dense = tf.Variable(tf.constant(0.1, shape=[out_len]), name=name_b) # \"b2\"\n",
    "    return tf.matmul(x,W_dense) + b_dense \n",
    "\n",
    "def dropout_layer(layer, dropout_keep_prob):\n",
    "    return tf.nn.dropout(layer, dropout_keep_prob)\n",
    "\n",
    "def relu_layer(layer):\n",
    "    return tf.nn.relu(layer)\n",
    "\n",
    "def dense_rnn_layer(rnn_output, hidden_size, vocab_size, name_w, name_b):\n",
    "    W = tf.Variable(tf.random_uniform([hidden_size, vocab_size], -1,1), name = name_w)\n",
    "    b = tf.Variable(tf.random_uniform([vocab_size], -1,1), name = name_b)\n",
    "    logits = tf.nn.xw_plus_b(rnn_output, W, b)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting labels and features...\n",
      "making tensor slices...\n",
      "shuffling...\n",
      "making epochs...\n",
      "making batches...\n",
      "setting up input took 0.7405116558074951 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# somewhat state which variables will be used here\n",
    "# reshaped to fit better (not sure if too necessary in hindsight, but minimal loss in time)\n",
    "input_name = name_padded\n",
    "input_itemdesc = itemdesc_padded\n",
    "input_price = train_raw['price'].values.reshape((-1,1))\n",
    "input_brand = train_raw.brand_name_inds.values.reshape((-1,1))\n",
    "input_cat1 = train_raw.cat1_inds.values.reshape((-1,1))\n",
    "input_cat2 = train_raw.cat2_inds.values.reshape((-1,1))\n",
    "input_cat3 = train_raw.cat3_inds.values.reshape((-1,1))\n",
    "input_itemcond = train_raw.item_condition_id.values.reshape((-1,1))\n",
    "input_ship = train_raw.shipping.values.reshape((-1,1))\n",
    "\n",
    "# define some lengths for partitioning data after feeding\n",
    "input_name_len = input_name.shape[1]\n",
    "input_itemdesc_len = input_itemdesc.shape[1]\n",
    "\n",
    "# concatenate data to make into tensor slices\n",
    "temp_set = np.concatenate((input_name, input_itemdesc,input_cat1,input_cat2,input_cat3,\n",
    "                           input_brand, input_itemcond, input_ship),axis = 1) #name_and_desc ,input_itemcond,input_shipping\n",
    "shape_set = temp_set.shape[1] \n",
    "\n",
    "#split the data into train and validation sets\n",
    "perc_split = .85 # use 30% as validation\n",
    "split_ind = round(train_raw.shape[0]*perc_split)\n",
    "\n",
    "train_temp_set = temp_set[:split_ind,]\n",
    "train_price = input_price[:split_ind,].astype(np.float32)\n",
    "val_temp_set = temp_set[split_ind:,]\n",
    "val_price = input_price[split_ind:,].astype(np.float32)\n",
    "\n",
    "batch_len = 1000\n",
    "\n",
    "num_epoch = 25\n",
    "tot_iter = train_raw.shape[0]* num_epoch // batch_len + 1\n",
    "\n",
    "# make iterator for train set\n",
    "print('splitting labels and features...')\n",
    "features_input = train_temp_set.astype(np.int32)\n",
    "label_input = train_price.astype(np.float32)\n",
    "# make some placeholders to avoid GraphDef exceeding 2GB\n",
    "feat_placeholder = tf.placeholder(features_input.dtype, features_input.shape)\n",
    "label_placeholder = tf.placeholder(label_input.dtype, label_input.shape)\n",
    "print('making tensor slices...')\n",
    "dataset = tf.data.Dataset.from_tensor_slices((feat_placeholder, label_placeholder))\n",
    "print('shuffling...')\n",
    "#np.random.shuffle(temp_set) # shuffle the data\n",
    "dataset = dataset.shuffle(buffer_size =10000)\n",
    "print('making epochs...')\n",
    "dataset = dataset.repeat(num_epoch) # epoch\n",
    "print('making batches...')\n",
    "dataset = dataset.batch(batch_len) \n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_batch = iterator.get_next()\n",
    "\n",
    "end_time = time.time()\n",
    "print('setting up input took ' + str(end_time - start_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding step took 0.05361485481262207 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "name_emb_size = 10\n",
    "itemdesc_emb_size = 10\n",
    "brand_emb_size = 50\n",
    "cat1_emb_size = 10\n",
    "cat2_emb_size = 20\n",
    "cat3_emb_size = 30\n",
    "itemcond_emb_size = 10\n",
    "shipping_emb_size = 10\n",
    "\n",
    "# lengths needed here and a bit later\n",
    "itemcond_len = np.max(train_raw.item_condition_id.values) + 1\n",
    "\n",
    "name_emb = embed(name_padded, dict_name_len, name_embed_len, 'name_embedding_table')\n",
    "itemdesc_emb = embed(itemdesc_padded, dict_itemdesc_len, itemdesc_embed_len, 'item_desc_embedding_table')\n",
    "brand_emb = embed(train_raw.brand_name_inds,dict_brand_len, brand_emb_size, name= 'brand_emb')\n",
    "cat1_emb = embed(train_raw.cat1_inds, dict_cat1_len, cat1_emb_size, name= 'cat1_emb')\n",
    "cat2_emb = embed(train_raw.cat2_inds, dict_cat2_len, cat2_emb_size, name= 'cat2_emb')\n",
    "cat3_emb = embed(train_raw.cat3_inds, dict_cat3_len, cat3_emb_size, name= 'cat3_emb')\n",
    "itemcond_emb = embed(train_raw.item_condition_id,itemcond_len ,itemcond_emb_size, name= 'itemcond_emb')\n",
    "shipping_emb = embed(train_raw.shipping, 2, shipping_emb_size, name= 'shipping_emb')\n",
    "\n",
    "end_time = time.time()\n",
    "print('embedding step took ' + str(end_time - start_time) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## RNN setup for tensorflow stuff\\n\\nname_emb_lookup = tf.nn.embedding_lookup(name_emb, input_x_name)\\nitemdesc_emb_lookup = tf.nn.embedding_lookup(itemdesc_emb,input_x_itemdesc)'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''## RNN setup for tensorflow stuff\n",
    "\n",
    "name_emb_lookup = tf.nn.embedding_lookup(name_emb, input_x_name)\n",
    "itemdesc_emb_lookup = tf.nn.embedding_lookup(itemdesc_emb,input_x_itemdesc)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## JUST RNN HERE\n",
    "# referense: http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/\n",
    "\n",
    "# define some parameters here \n",
    "num_layers = 2\n",
    "\n",
    "\n",
    "def RNN_junk(padded_emb_lookup, batch_size, hidden_size, num_layers, dict_len, drop_keep_prob, \n",
    "                name_W, name_b):\n",
    "    if drop_keep_prob == 1:\n",
    "        isTraining = False\n",
    "    else: \n",
    "        isTraining = True\n",
    "    init_state = np.zeros((num_layers, 2, batch_size, hidden_size),dtype = 'float32')\n",
    "\n",
    "    # setup input for feeding into lstm \n",
    "\n",
    "    state_per_layer_list = tf.unstack(init_state, axis=0)\n",
    "    rnn_tuple_state = tuple(\n",
    "                [tf.contrib.rnn.LSTMStateTuple(state_per_layer_list[idx][0], state_per_layer_list[idx][1])\n",
    "                 for idx in range(num_layers)]\n",
    "            )\n",
    "\n",
    "    # lstm layers \n",
    "\n",
    "    cell = tf.contrib.rnn.LSTMCell(hidden_size, forget_bias = 1)\n",
    "    if isTraining:\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob = drop_keep_prob) \n",
    "\n",
    "    if num_layers > 1:\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([cell for _ in range(num_layers)], state_is_tuple = True)\n",
    "\n",
    "    output, state = tf.nn.dynamic_rnn(cell, padded_emb_lookup, dtype = tf.float32, initial_state = rnn_tuple_state)\n",
    "    output = tf.reshape(output, [-1, hidden_size])\n",
    "    logits = dense_rnn_layer(output, hidden_size, 30, name_W, name_b)\n",
    "    logits = tf.reshape(logits, [batch_size, -1])\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tensorflow running block (is this __main__?)\n",
    "\n",
    "input_x = tf.placeholder(tf.int32,[None, shape_set], name = \"input_x\") # pad_length = 25 or something defined earlier\n",
    "input_y = tf.placeholder(tf.float32,[None,1], name = \"input_y\") # train agianst this\n",
    "\n",
    "input_x_name = input_x[:,:input_name_len]\n",
    "input_x_itemdesc = input_x[:,input_name_len:(input_name_len + input_itemdesc_len)]\n",
    "input_x_cat1 = input_x[:,(input_name_len + input_itemdesc_len)]\n",
    "input_x_cat2 = input_x[:,(input_name_len + input_itemdesc_len)+1]\n",
    "input_x_cat3 = input_x[:,(input_name_len + input_itemdesc_len)+2]\n",
    "input_x_brand = input_x[:,(input_name_len + input_itemdesc_len)+3]\n",
    "input_x_itemcond = input_x[:,(input_name_len + input_itemdesc_len)+4]\n",
    "input_x_shipping = input_x[:,(input_name_len + input_itemdesc_len)+5]\n",
    "\n",
    "# setup embedding and lookup\n",
    "\n",
    "name_emb_lookup = tf.nn.embedding_lookup(name_emb, input_x_name)\n",
    "itemdesc_emb_lookup = tf.nn.embedding_lookup(itemdesc_emb, input_x_itemdesc)\n",
    "brand_emb_lookup = tf.nn.embedding_lookup(brand_emb,input_x_brand)\n",
    "cat1_emb_lookup = tf.nn.embedding_lookup(cat1_emb,input_x_cat1)\n",
    "cat2_emb_lookup = tf.nn.embedding_lookup(cat2_emb,input_x_cat2)\n",
    "cat3_emb_lookup = tf.nn.embedding_lookup(cat3_emb,input_x_cat3)\n",
    "itemcond_emb_lookup = tf.nn.embedding_lookup(itemcond_emb, input_x_itemcond)\n",
    "shipping_emb_lookup = tf.nn.embedding_lookup(shipping_emb, input_x_shipping)\n",
    "\n",
    "# set some lazy parameters here\n",
    "\n",
    "dropout_keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.variable_scope('name_rnn'):\n",
    "    layers_name = RNN_junk(name_emb_lookup, batch_len, name_embed_len, 3, dict_name_len, dropout_keep_prob, \n",
    "                          'name_w_rnn', 'name_b_rnn')\n",
    "with tf.variable_scope('item_desc_rnn'):\n",
    "    layers_itemdesc = RNN_junk(itemdesc_emb_lookup, batch_len, itemdesc_embed_len, 3, \n",
    "                            dict_itemdesc_len, dropout_keep_prob, 'itemdesc_w_rnn', 'itemdesc_b_rnn')\n",
    "\n",
    "#combine the layers\n",
    "comb_layers = tf.concat([layers_name,layers_itemdesc, brand_emb_lookup, cat1_emb_lookup,\n",
    "                         cat2_emb_lookup, cat3_emb_lookup, itemcond_emb_lookup, shipping_emb_lookup],axis=1)\n",
    "\n",
    "#dense \n",
    "dense1 = dense_NN(comb_layers, 64, \"W_1\",\"b_1\")\n",
    "dense1 = relu_layer(dense1)\n",
    "dense1 = dropout_layer(dense1, dropout_keep_prob)\n",
    "\n",
    "dense2 = dense_NN(dense1, 128, \"W_2\",\"b_2\")\n",
    "dense2 = relu_layer(dense2)\n",
    "dense2 = dropout_layer(dense2, dropout_keep_prob)\n",
    "\n",
    "predictions = dense_NN(dense2, 1, \"W_pred\", \"b_pred\") \n",
    "\n",
    "\n",
    "loss = tf.losses.mean_squared_error(input_y, predictions)\n",
    "train_step  = tf.train.AdamOptimizer(learning_rate = .001).minimize(loss)\n",
    "\n",
    "# as is, normalized predictions cause NaN in rmsle solving. adding .00001 just in case\n",
    "unwind_true = tf.expm1((input_y* std_price_norm) + mean_price_norm)+ .00001 \n",
    "unwind_pred = tf.expm1((predictions* std_price_norm) + mean_price_norm)+ .00001\n",
    "log_true = tf.log(unwind_true)\n",
    "log_pred = tf.log(unwind_pred)\n",
    "rmsle_ = tf.sqrt(tf.reduce_mean(tf.square(log_true - log_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define savedmodel here\n",
    "\n",
    "export_dir = '/home/bsong/Python_Stuff/Scripts/model_save/'\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting labels and features...\n",
      "making tensor slices...\n",
      "shuffling...\n",
      "making batches...\n"
     ]
    }
   ],
   "source": [
    "# make iterator for train set\n",
    "print('splitting labels and features...')\n",
    "features_input_val = val_temp_set.astype(np.int32)\n",
    "label_input_val = val_price.astype(np.float32)\n",
    "# make some placeholders to avoid GraphDef exceeding 2GB\n",
    "feat_placeholder_val = tf.placeholder(features_input_val.dtype, features_input_val.shape)\n",
    "label_placeholder_val = tf.placeholder(label_input_val.dtype, label_input_val.shape)\n",
    "print('making tensor slices...')\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((feat_placeholder_val, label_placeholder_val))\n",
    "print('shuffling...')\n",
    "#np.random.shuffle(temp_set) # shuffle the data\n",
    "dataset_val = dataset_val.shuffle(buffer_size =10000)\n",
    "print('making batches...')\n",
    "dataset_val = dataset_val.batch(1000) \n",
    "iterator_val = dataset_val.make_initializable_iterator()\n",
    "next_batch_val = iterator_val.get_next()\n",
    "\n",
    "tot_runs = np.ceil(features_input_val.shape[0]/1000)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "running step: 100\n",
      "rmsle of validation set at this step: 0.785 \n",
      "resetting counter after 99 steps\n",
      "One hundred steps took 7.828 seconds.\n",
      " \n",
      "running step: 200\n",
      "rmsle of validation set at this step: 0.737 \n",
      "resetting counter after 100 steps\n",
      "One hundred steps took 7.109 seconds.\n",
      " \n",
      "running step: 300\n",
      "rmsle of validation set at this step: 0.719 \n",
      "resetting counter after 100 steps\n",
      "One hundred steps took 7.105 seconds.\n",
      " \n",
      "running step: 400\n",
      "rmsle of validation set at this step: 0.723 \n",
      "One hundred steps took 7.114 seconds.\n",
      " \n",
      "running step: 500\n",
      "rmsle of validation set at this step: 0.719 \n",
      "One hundred steps took 7.141 seconds.\n",
      " \n",
      "running step: 600\n",
      "rmsle of validation set at this step: 0.722 \n",
      "One hundred steps took 7.076 seconds.\n",
      " \n",
      "running step: 700\n",
      "rmsle of validation set at this step: 0.714 \n",
      "resetting counter after 400 steps\n",
      "One hundred steps took 7.111 seconds.\n",
      " \n",
      "running step: 800\n",
      "rmsle of validation set at this step: 0.715 \n",
      "One hundred steps took 7.127 seconds.\n",
      " \n",
      "running step: 900\n",
      "rmsle of validation set at this step: 0.719 \n",
      "One hundred steps took 7.079 seconds.\n",
      " \n",
      "running step: 1000\n",
      "rmsle of validation set at this step: 0.718 \n",
      "One hundred steps took 7.133 seconds.\n",
      " \n",
      "running step: 1100\n",
      "rmsle of validation set at this step: 0.718 \n",
      "One hundred steps took 7.111 seconds.\n",
      " \n",
      "running step: 1200\n",
      "rmsle of validation set at this step: 0.701 \n",
      "resetting counter after 500 steps\n",
      "One hundred steps took 7.132 seconds.\n",
      " \n",
      "running step: 1300\n",
      "rmsle of validation set at this step: 0.704 \n",
      "One hundred steps took 7.111 seconds.\n",
      " \n",
      "running step: 1400\n",
      "rmsle of validation set at this step: 0.701 \n",
      "resetting counter after 200 steps\n",
      "One hundred steps took 7.164 seconds.\n",
      " \n",
      "running step: 1500\n",
      "rmsle of validation set at this step: 0.710 \n",
      "One hundred steps took 7.176 seconds.\n",
      " \n",
      "running step: 1600\n",
      "rmsle of validation set at this step: 0.700 \n",
      "resetting counter after 200 steps\n",
      "One hundred steps took 7.134 seconds.\n",
      " \n",
      "running step: 1700\n",
      "rmsle of validation set at this step: 0.708 \n",
      "One hundred steps took 7.126 seconds.\n",
      " \n",
      "running step: 1800\n",
      "rmsle of validation set at this step: 0.704 \n",
      "One hundred steps took 7.134 seconds.\n",
      " \n",
      "running step: 1900\n",
      "rmsle of validation set at this step: 0.700 \n",
      "One hundred steps took 7.150 seconds.\n",
      " \n",
      "running step: 2000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-9b5330dab4d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mbatch_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mval_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minput_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeed_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_y\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeed_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mpred_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munwind_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munwind_true\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0mpredictions_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mtrue_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Start training...')\n",
    "\n",
    "start_time = time.time()\n",
    "learn_rate = 1e-4\n",
    "counter = 0 \n",
    "i = 1\n",
    "best_rmsle = 2 # just has to be greater than 2 really\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator_val.initializer, {feat_placeholder_val: features_input_val, label_placeholder_val: label_input_val}) \n",
    "    sess.run(iterator.initializer, {feat_placeholder: features_input, label_placeholder: label_input})\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)  \n",
    "\n",
    "    while counter <= 2001:\n",
    "        features_, label_ = sess.run(next_batch)\n",
    "        batch_len = features_.shape[0]\n",
    "        sess.run(train_step,{input_x: features_, input_y: label_, dropout_keep_prob:.5})\n",
    "        end_time = time.time()\n",
    "        if i % 100 == 0:\n",
    "            print('running step: ' + str(i))\n",
    "            predictions_val = []\n",
    "            true_val = []\n",
    "            for _ in range(int(tot_runs)):\n",
    "                feed_x, feed_y = sess.run(next_batch_val)\n",
    "                batch_len = feed_x.shape[0]\n",
    "                val_dict = {input_x: feed_x, input_y: feed_y, dropout_keep_prob: 1}\n",
    "                pred_batch, true_batch = sess.run([unwind_pred, unwind_true],val_dict)\n",
    "                predictions_val.extend(pred_batch)\n",
    "                true_val.extend(true_batch)\n",
    "            true_val = np.array(true_val)\n",
    "            predictions_val = np.array(predictions_val)\n",
    "            rmsle = np.sqrt(np.mean(np.square(np.log(true_val) - np.log(predictions_val))))   \n",
    "            print(\"rmsle of validation set at this step: %5.3f \" % rmsle)\n",
    "            if rmsle < best_rmsle:\n",
    "                best_rmsle = rmsle\n",
    "                saver.save(sess, save_path=export_dir)\n",
    "                print('resetting counter after ' + str(counter) + ' steps')\n",
    "                counter = 0\n",
    "            tot_time = end_time - start_time\n",
    "            print('One hundred steps took %5.3f seconds.' % tot_time)\n",
    "            print(' ')\n",
    "            start_time = time.time()\n",
    "            sess.run(iterator_val.initializer, {feat_placeholder_val: features_input_val, label_placeholder_val: label_input_val}) \n",
    "        counter += 1\n",
    "        i += 1 \n",
    "        if i % 500 == 0: \n",
    "            learn_rate = learn_rate/10\n",
    "\n",
    "    print('Done!')            "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
